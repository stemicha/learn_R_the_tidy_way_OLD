[
["index.html", "Advanced R Welcome Other books", " Advanced R Hadley Wickham Welcome This is the website for work-in-progress 2nd edition of “Advanced R”, a book in Chapman &amp; Hall’s R Series. The book is designed primarily for R users who want to improve their programming skills and understanding of the language. It should also be useful for programmers coming to R from other languages, as it explains some of R’s quirks and shows how some parts that seem horrible do have a positive side. This edition is a work in progress. If you’re looking for the electronic version of the 1st edition, you can find it online at http://adv-r.had.co.nz/. Other books You may also be interested in: “R for Data Science” which introduces you to R as a tool for doing data science, focussing on a consistent set of packages known as the tidyverse. “R Packages” which teaches you how to make the most of R’s fantastic package system. "],
["introduction.html", "1 Introduction 1.1 Who should read this book 1.2 What you will get out of this book 1.3 Meta-techniques 1.4 Recommended reading 1.5 Getting help 1.6 Acknowledgments 1.7 Conventions 1.8 Colophon", " 1 Introduction With more than 10 years experience programming in R, I’ve had the luxury of being able to spend a lot of time trying to figure out and understand how the language works. This book is my attempt to pass on what I’ve learned so that you can quickly become an effective R programmer. Reading it will help you avoid the mistakes I’ve made and dead ends I’ve gone down, and will teach you useful tools, techniques, and idioms that can help you to attack many types of problems. In the process, I hope to show that, despite its frustrating quirks, R is, at its heart, an elegant and beautiful language, well tailored for data analysis and statistics. If you are new to R, you might wonder what makes learning such a quirky language worthwhile. To me, some of the best features are: It’s free, open source, and available on every major platform. As a result, if you do your analysis in R, anyone can easily replicate it. A massive set of packages for statistical modelling, machine learning, visualisation, and importing and manipulating data. Whatever model or graphic you’re trying to do, chances are that someone has already tried to do it. At a minimum, you can learn from their efforts. Cutting edge tools. Researchers in statistics and machine learning will often publish an R package to accompany their articles. This means immediate access to the very latest statistical techniques and implementations. Deep-seated language support for data analysis. This includes features like missing values, data frames, and subsetting. A fantastic community. It is easy to get help from experts on the R-help mailing list, stackoverflow, or subject-specific mailing lists like R-SIG-mixed-models or ggplot2. You can also connect with other R learners via twitter, linkedin, and through many local user groups. Powerful tools for communicating your results. R packages make it easy to produce html or pdf reports, or create interactive websites. A strong foundation in functional programming. The ideas of functional programming are well suited to solving many of the challenges of data analysis. R provides a powerful and flexible toolkit which allows you to write concise yet descriptive code. An IDE tailored to the needs of interactive data analysis and statistical programming. Powerful metaprogramming facilities. R is not just a programming language, it is also an environment for interactive data analysis. Its metaprogramming capabilities allow you to write magically succinct and concise functions and provide an excellent environment for designing domain-specific languages. Designed to connect to high-performance programming languages like C, Fortran, and C++. Of course, R is not perfect. R’s biggest challenge is that most R users are not programmers. This means that: Much of the R code you’ll see in the wild is written in haste to solve a pressing problem. As a result, code is not very elegant, fast, or easy to understand. Most users do not revise their code to address these shortcomings. Compared to other programming languages, the R community tends to be more focussed on results instead of processes. Knowledge of software engineering best practices is patchy: for instance, not enough R programmers use source code control or automated testing. Metaprogramming is a double-edged sword. Too many R functions use tricks to reduce the amount of typing at the cost of making code that is hard to understand and that can fail in unexpected ways. Inconsistency is rife across contributed packages, even within base R. You are confronted with over 20 years of evolution every time you use R. Learning R can be tough because there are many special cases to remember. R is not a particularly fast programming language, and poorly written R code can be terribly slow. R is also a profligate user of memory. Personally, I think these challenges create a great opportunity for experienced programmers to have a profound positive impact on R and the R community. R users do care about writing high quality code, particularly for reproducible research, but they don’t yet have the skills to do so. I hope this book will not only help more R users to become R programmers but also encourage programmers from other languages to contribute to R. 1.1 Who should read this book This book is aimed at two complementary audiences: Intermediate R programmers who want to dive deeper into R and learn new strategies for solving diverse problems. Programmers from other languages who are learning R and want to understand why R works the way it does. To get the most out of this book, you’ll need to have written a decent amount of code in R or another programming language. You might not know all the details, but you should be familiar with how functions work in R and although you may currently struggle to use them effectively, you should be familiar with the apply family (like apply() and lapply()). 1.2 What you will get out of this book This book describes the skills I think an advanced R programmer should have: the ability to produce quality code that can be used in a wide variety of circumstances. After reading this book, you will: Be familiar with the fundamentals of R. You will understand complex data types and the best ways to perform operations on them. You will have a deep understanding of how functions work, and be able to recognise and use the four object systems in R. Understand what functional programming means, and why it is a useful tool for data analysis. You’ll be able to quickly learn how to use existing tools, and have the knowledge to create your own functional tools when needed. Appreciate the double-edged sword of metaprogramming. You’ll be able to create functions that use non-standard evaluation in a principled way, saving typing and creating elegant code to express important operations. You’ll also understand the dangers of metaprogramming and why you should be careful about its use. Have a good intuition for which operations in R are slow or use a lot of memory. You’ll know how to use profiling to pinpoint performance bottlenecks, and you’ll know enough C++ to convert slow R functions to fast C++ equivalents. Be comfortable reading and understanding the majority of R code. You’ll recognise common idioms (even if you wouldn’t use them yourself) and be able to critique others’ code. 1.3 Meta-techniques There are two meta-techniques that are tremendously helpful for improving your skills as an R programmer: reading source code and adopting a scientific mindset. Reading source code is important because it will help you write better code. A great place to start developing this skill is to look at the source code of the functions and packages you use most often. You’ll find things that are worth emulating in your own code and you’ll develop a sense of taste for what makes good R code. You will also see things that you don’t like, either because its virtues are not obvious or it offends your sensibilities. Such code is nonetheless valuable, because it helps make concrete your opinions on good and bad code. A scientific mindset is extremely helpful when learning R. If you don’t understand how something works, develop a hypothesis, design some experiments, run them, and record the results. This exercise is extremely useful since if you can’t figure something out and need to get help, you can easily show others what you tried. Also, when you learn the right answer, you’ll be mentally prepared to update your world view. When I clearly describe a problem to someone else (the art of creating a reproducible example), I often figure out the solution myself. 1.4 Recommended reading R is still a relatively young language, and the resources to help you understand it are still maturing. In my personal journey to understand R, I’ve found it particularly helpful to use resources from other programming languages. R has aspects of both functional and object-oriented (OO) programming languages. Learning how these concepts are expressed in R will help you leverage your existing knowledge of other programming languages, and will help you identify areas where you can improve. To understand why R’s object systems work the way they do, I found The Structure and Interpretation of Computer Programs (SICP) by Harold Abelson and Gerald Jay Sussman, particularly helpful. It’s a concise but deep book. After reading it, I felt for the first time that I could actually design my own object-oriented system. The book was my first introduction to the generic function style of OO common in R. It helped me understand its strengths and weaknesses. SICP also talks a lot about functional programming, and how to create simple functions which become powerful when combined. To understand the trade-offs that R has made compared to other programming languages, I found Concepts, Techniques and Models of Computer Programming by Peter van Roy and Sef Haridi extremely helpful. It helped me understand that R’s copy-on-modify semantics make it substantially easier to reason about code, and that while its current implementation is not particularly efficient, it is a solvable problem. If you want to learn to be a better programmer, there’s no place better to turn than The Pragmatic Programmer by Andrew Hunt and David Thomas. This book is language agnostic, and provides great advice for how to be a better programmer. 1.5 Getting help Currently, there are two main venues to get help when you’re stuck and can’t figure out what’s causing the problem: stackoverflow and the R-help mailing list. You can get fantastic help in both venues, but they do have their own cultures and expectations. It’s usually a good idea to spend a little time lurking, learning about community expectations, before you put up your first post. Some good general advice: Make sure you have the latest version of R and of the package (or packages) you are having problems with. It may be that your problem is the result of a recently fixed bug. Spend some time creating a reproducible example. This is often a useful process in its own right, because in the course of making the problem reproducible you often figure out what’s causing the problem. Look for related problems before posting. If someone has already asked your question and it has been answered, it’s much faster for everyone if you use the existing answer. 1.6 Acknowledgments I would like to thank the tireless contributors to R-help and, more recently, stackoverflow. There are too many to name individually, but I’d particularly like to thank Luke Tierney, John Chambers, Dirk Eddelbuettel, JJ Allaire and Brian Ripley for generously giving their time and correcting my countless misunderstandings. This book was written in the open, and chapters were advertised on twitter when complete. It is truly a community effort: many people read drafts, fixed typos, suggested improvements, and contributed content. Without those contributors, the book wouldn’t be nearly as good as it is, and I’m deeply grateful for their help. Special thanks go to Peter Li, who read the book from cover-to-cover and provided many fixes. Other outstanding contributors were Aaron Schumacher, @crtahlin, Lingbing Feng, @juancentro, and @johnbaums. Thanks go to all contributers in alphabetical order: Aaron Schumacher, Aaron Wolen, @aaronwolen, @absolutelyNoWarranty, Adam Hunt, @agrabovsky, @ajdm, Alan Dipert, Alexander Grueneberg, @alexbbrown, @alko989, @allegretto, @amarchin, @AmeliaMN, Andrew Bray, @andrewla, Andy Teucher, Anthony Damico, Anton Antonov, @aranlunzer, @arilamstein, @asnr, @avilella, @baptiste, Bart Kastermans, @blindjesse, @blmoore, @bnjmn, Brandon Greenwell, Brandon Hurr, Brett Klamer, Brian G. Barkley, Brian Knaus, @BrianDiggs, @Bryce, C. Jason Liang, @carey1024, @Carson, @cdrv, Ching Boon, @chiphogg, @ChrisMuir, Christopher Brown, @christophergandrud, Clay Ford, Colin Fay, @cornelius1729, @cplouffe, Craig Citro, @crossfitAL, @crowding, Crt Ahlin, @crtahlin, @cscheid, @csgillespie, @cusanovich, @cwarden, @cwickham, Daisuke Ichikawa, Daniel Lee, @darrkj, @Dasonk, Dave Childers, David Hajage, David LeBauer, Davor Cubranic, @dchudz, Dean Attali, dennis feehan, Dewey Dunnington, @dfeehan, Dirk Eddelbuettel, @dkahle, @dlebauer, @dlschweizer, @dmontaner, @dougmitarotonda, @dpatschke, @duncandonutz, @eaurele, @EdFineOKL, @EDiLD, Edwin Thoen, @eijoac, @eipi10, @elegrand, Ellis Valentiner, @EmilRehnberg, Eric C. Anderson, @etb, @fabian-s, Facundo Muñoz, @flammy0530, @fpepin, Francois Michonneau, Frank Farach, Frans van Dunné, @freezby, @fyears, Garrett Grolemund, @garrettgman, @gavinsimpson, @gezakiss7, @gggtest, Gökçen Eraslan, @gr650, Gregg Whitworth, @gregorp, @gsee, @gsk3, @gthb, Guy Dawson, Harley Day, @hassaad85, @helmingstay, Henrik Bengtsson, @i, Iain Dillingham, Ian Lyttle, @IanKopacka, @ijlyttle, Ilan Man, @imanuelcostigan, @initdch, @irudnyts, Jason Asher, Jason Knight, @jasondavies, @jastingo, @jcborras, Jeff Allen, @jeharmse, Jennifer (Jenny) Bryan, @jennybc, @jentjr, @Jeremiah, @JestonBlu, Jim Hester, Jim Vine, @JimInNashville, @jinlong25, JJ Allaire, @JMHay, Jochen Van de Velde, Johann Hibschman, John Blischak, john verzani, @johnbaums, @johnjosephhorton, @johnthomas12, Jon Calder, Joris Muller, @JorneBiccler, Jose Antonio Magaña Mesa, Joseph Casillas, @juancentro, Julia Gustavsen, @kdauria, Ken Williams, @kenahoo, Kenny Darrell, @kent37, Kevin Markham, Kevin Ushey, @kforner, Kirill Müller, Kirill Sevastyanenko, Krishna Sankar, Kun Ren, Laurent Gatto, @Lawrence-Liu, @ldfmrails, @lgatto, @liangcj, Lingbing Feng, Lionel Henry, @lynaghk, Maarten Kruijver, Mamoun Benghezal, @mannyishere, Marcel Ramos, Mark Rosenstein, Martin Morgan, Matt Pettis, @mattbaggott, Matthew Grogan, Matthew Sedaghatfar, Matthieu Gomez, @mattmalin, Mauro Lepore, Max Ghenis, @Michael, Michael Bishop, Michael Buckley, Michael Kane, Michael Quinn, @michaelbach, Michał Bojanowski, @mjsduncan, @Mullefa, @myqlarson, Nacho Caballero, Nick Carchedi, @nignatiadis, @nstjhp, @ogennadi, Oliver Keyes, Oliver Paisley, @otepoti, Pariksheet Nanda, Parker Abercrombie, @patperu, Patrick Miller, @pavel-vodrazka, @pdb61, @pengyu, Peter F Schulam, Peter Lindbrook, Peter Meilstrup, @philchalmers, @picasa, @piccolbo, @pierreroudier, @polmath, @pooryorick, @quantbo, R. Mark Sharp, Ramnath Vaidyanathan, @ramnathv, @Rappster, Ricardo Pietrobon, Richard Cotton, @richardreeve, @rmflight, @rmsharp, Rob Weyant, Robert Krzyzanowski, Robert M Flight, @RobertZK, @robiRagan, Romain François, @rrunner, @rubenfcasal, Rumen Zarev, @sailingwave, @sarunasmerkliopas, @sbgraves237, Scott Ritchie, @scottko, @scottl, @seaaan, Sean Anderson, Sean Carmody, Sean Wilkinson, @Sebastian, @sebastian-c, Sebastien Vigneau, @shabbychef, Shannon Rush, Simon O’Hanlon, Simon Potter, @SplashDance, @ste-fan, Stefan Widgren, @stephens999, Steve Lianoglou, Steve Walker, Steven Pav, @strongh, @stuttungur, @surmann, Sven E. Templer, @Swarchal, @swnydick, @taekyunk, Tal Galili, @talgalili, @Tazinho, @tdenes, Terence Teo, @Thomas, Thomas Lin Pedersen, @thomasherbig, @thomaszumbrunn, Tim Cole, tj mahr, @tjmahr, Tom Buckley, Tom Crockett, @ttriche, @twjacobs, @tyhenkaline, @tylerritchie, @ulrichatz, @varun729, @victorkryukov, @vijaybarve, @vzemlys, @wchi144, Welliton Souza, @wibeasley, @WilCrofter, William Doane, Winston Chang, @winterschlaefer, @wmc3, Wolfgang Huber, @wordnerd, Yoni Ben-Meshulam, @yuchouchen, @zachcp, @zackham, @zerokarmaleft, Zhongpeng Lin, @电线杆. 1.7 Conventions Throughout this book I use f() to refer to functions, g to refer to variables and function parameters, and h/ to paths. Larger code blocks intermingle input and output. Output is commented so that if you have an electronic version of the book, e.g., http://adv-r.had.co.nz, you can easily copy and paste examples into R. Output comments look like #&gt; to distinguish them from regular comments. 1.8 Colophon This book was written in Rmarkdown inside Rstudio. knitr and pandoc converted the raw Rmarkdown to html and pdf. The website was made with jekyll, styled with bootstrap, and automatically published to Amazon’s S3 by travis-ci. The complete source is available from github. Code is set in inconsolata. "],
["data-structures.html", "2 Data structures 2.1 Vectors 2.2 Attributes 2.3 Matrices and arrays 2.4 Data frames 2.5 Answers", " 2 Data structures This chapter summarises the most important data structures in base R. You’ve probably used many (if not all) of them before, but you may not have thought deeply about how they are interrelated. In this brief overview, I won’t discuss individual types in depth. Instead, I’ll show you how they fit together as a whole. If you need more details, you can find them in R’s documentation. R’s base data structures can be organised by their dimensionality (1d, 2d, or nd) and whether they’re homogeneous (all contents must be of the same type) or heterogeneous (the contents can be of different types). This gives rise to the five data types most often used in data analysis: Homogeneous Heterogeneous 1d Atomic vector List 2d Matrix Data frame nd Array Almost all other objects are built upon these foundations. In the OO field guide, you’ll see how more complicated objects are built of these simple pieces. Note that R has no 0-dimensional, or scalar types. Individual numbers or strings, which you might think would be scalars, are actually vectors of length one. Given an object, the best way to understand what data structures it’s composed of is to use str(). str() is short for structure and it gives a compact, human readable description of any R data structure. 2.0.0.0.1 Quiz Take this short quiz to determine if you need to read this chapter. If the answers quickly come to mind, you can comfortably skip this chapter. You can check your answers in answers. What are the three properties of a vector, other than its contents? What are the four common types of atomic vectors? What are the two rare types? What are attributes? How do you get them and set them? How is a list different from an atomic vector? How is a matrix different from a data frame? Can you have a list that is a matrix? Can a data frame have a column that is a matrix? 2.0.0.0.2 Outline Vectors introduces you to atomic vectors and lists, R’s 1d data structures. Attributes takes a small detour to discuss attributes, R’s flexible metadata specification. Here you’ll learn about factors, an important data structure created by setting attributes of an atomic vector. Matrices and arrays introduces matrices and arrays, data structures for storing 2d and higher dimensional data. Data frames teaches you about the data frame, the most important data structure for storing data in R. Data frames combine the behaviour of lists and matrices to make a structure ideally suited for the needs of statistical data. 2.1 Vectors The basic data structure in R is the vector. Vectors come in two flavours: atomic vectors and lists. They have three common properties: Type, typeof(), what it is. Length, length(), how many elements it contains. Attributes, attributes(), additional arbitrary metadata. They differ in the types of their elements: all elements of an atomic vector must be the same type, whereas the elements of a list can have different types. NB: is.vector() does not test if an object is a vector. Instead it returns TRUE only if the object is a vector with no attributes apart from names. Use is.atomic(x) || is.list(x) to test if an object is actually a vector. 2.1.1 Atomic vectors There are four common types of atomic vectors that I’ll discuss in detail: logical, integer, double (often called numeric), and character. There are two rare types that I will not discuss further: complex and raw. Atomic vectors are usually created with c(), short for combine: dbl_var &lt;- c(1, 2.5, 4.5) # With the L suffix, you get an integer rather than a double int_var &lt;- c(1L, 6L, 10L) # Use TRUE and FALSE (or T and F) to create logical vectors log_var &lt;- c(TRUE, FALSE, T, F) chr_var &lt;- c(&quot;these are&quot;, &quot;some strings&quot;) Atomic vectors are always flat, even if you nest c()’s: c(1, c(2, c(3, 4))) #&gt; [1] 1 2 3 4 # the same as c(1, 2, 3, 4) #&gt; [1] 1 2 3 4 Missing values are specified with NA, which is a logical vector of length 1. NA will always be coerced to the correct type if used inside c(), or you can create NAs of a specific type with NA_real_ (a double vector), NA_integer_ and NA_character_. 2.1.1.1 Types and tests Given a vector, you can determine its type with typeof(). Use “is” functions with care. is.character(), is.double(), is.integer(), is.logical() are ok. The following are suprising: is.vector() tests for vectors with no attributes apart from names is.atomic() tests for atomic vectors or NULL is.numeric() tests for the numerical-ness of a vector, not whether it’s built on top of an integer or double. 2.1.1.2 Coercion All elements of an atomic vector must be the same type, so when you attempt to combine different types they will be coerced to the most flexible type. Types from least to most flexible are: logical, integer, double, and character. For example, combining a character and an integer yields a character: str(c(&quot;a&quot;, 1)) #&gt; chr [1:2] &quot;a&quot; &quot;1&quot; When a logical vector is coerced to an integer or double, TRUE becomes 1 and FALSE becomes 0. This is very useful in conjunction with sum() and mean() x &lt;- c(FALSE, FALSE, TRUE) as.numeric(x) #&gt; [1] 0 0 1 # Total number of TRUEs sum(x) #&gt; [1] 1 # Proportion that are TRUE mean(x) #&gt; [1] 0.333 Coercion often happens automatically. Most mathematical functions (+, log, abs, etc.) will coerce to a double or integer, and most logical operations (&amp;, |, any, etc) will coerce to a logical. You will usually get a warning message if the coercion might lose information. If confusion is likely, explicitly coerce with as.character(), as.double(), as.integer(), or as.logical(). 2.1.2 Lists Lists are different from atomic vectors because their elements can be of any type, including lists. You construct lists by using list() instead of c(): x &lt;- list(1:3, &quot;a&quot;, c(TRUE, FALSE, TRUE), c(2.3, 5.9)) str(x) #&gt; List of 4 #&gt; $ : int [1:3] 1 2 3 #&gt; $ : chr &quot;a&quot; #&gt; $ : logi [1:3] TRUE FALSE TRUE #&gt; $ : num [1:2] 2.3 5.9 Lists are sometimes called recursive vectors, because a list can contain other lists. This makes them fundamentally different from atomic vectors. x &lt;- list(list(list(list()))) str(x) #&gt; List of 1 #&gt; $ :List of 1 #&gt; ..$ :List of 1 #&gt; .. ..$ : list() is.recursive(x) #&gt; [1] TRUE c() will combine several lists into one. If given a combination of atomic vectors and lists, c() will coerce the vectors to lists before combining them. Compare the results of list() and c(): x &lt;- list(list(1, 2), c(3, 4)) y &lt;- c(list(1, 2), c(3, 4)) str(x) #&gt; List of 2 #&gt; $ :List of 2 #&gt; ..$ : num 1 #&gt; ..$ : num 2 #&gt; $ : num [1:2] 3 4 str(y) #&gt; List of 4 #&gt; $ : num 1 #&gt; $ : num 2 #&gt; $ : num 3 #&gt; $ : num 4 The typeof() a list is list. You can test for a list with is.list() and coerce to a list with as.list(). You can turn a list into an atomic vector with unlist(). If the elements of a list have different types, unlist() uses the same coercion rules as c(). Lists are used to build up many of the more complicated data structures in R. For example, both data frames (described in data frames) and linear models objects (as produced by lm()) are lists: is.list(mtcars) #&gt; [1] TRUE mod &lt;- lm(mpg ~ wt, data = mtcars) is.list(mod) #&gt; [1] TRUE 2.1.3 Exercises What are the six types of atomic vector? How does a list differ from an atomic vector? What makes is.vector() and is.numeric() fundamentally different to is.list() and is.character()? Test your knowledge of vector coercion rules by predicting the output of the following uses of c(): c(1, FALSE) c(&quot;a&quot;, 1) c(list(1), &quot;a&quot;) c(TRUE, 1L) Why do you need to use unlist() to convert a list to an atomic vector? Why doesn’t as.vector() work? Why is 1 == &quot;1&quot; true? Why is -1 &lt; FALSE true? Why is &quot;one&quot; &lt; 2 false? Why is the default missing value, NA, a logical vector? What’s special about logical vectors? (Hint: think about c(FALSE, NA_character_).) 2.2 Attributes All objects can have arbitrary additional attributes, used to store metadata about the object. Attributes can be thought of as a named list (with unique names). Attributes can be accessed individually with attr() or all at once (as a list) with attributes(). y &lt;- 1:10 attr(y, &quot;my_attribute&quot;) &lt;- &quot;This is a vector&quot; attr(y, &quot;my_attribute&quot;) #&gt; [1] &quot;This is a vector&quot; str(attributes(y)) #&gt; List of 1 #&gt; $ my_attribute: chr &quot;This is a vector&quot; The structure() function returns a new object with modified attributes: structure(1:10, my_attribute = &quot;This is a vector&quot;) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 #&gt; attr(,&quot;my_attribute&quot;) #&gt; [1] &quot;This is a vector&quot; By default, most attributes are lost when modifying a vector: attributes(y[1]) #&gt; NULL attributes(sum(y)) #&gt; NULL The only attributes not lost are the three most important: Names, a character vector giving each element a name, described in names. Dimensions, used to turn vectors into matrices and arrays, described in matrices and arrays. Class, used to implement the S3 object system, described in S3. Each of these attributes has a specific accessor function to get and set values. When working with these attributes, use names(x), dim(x), and class(x), not attr(x, &quot;names&quot;), attr(x, &quot;dim&quot;), and attr(x, &quot;class&quot;). 2.2.0.1 Names You can name a vector in three ways: When creating it: x &lt;- c(a = 1, b = 2, c = 3). By modifying an existing vector in place: x &lt;- 1:3; names(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;). Or: x &lt;- 1:3; names(x)[[1]] &lt;- c(&quot;a&quot;). By creating a modified copy of a vector: x &lt;- setNames(1:3, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)). Names don’t have to be unique. However, character subsetting, described in subsetting, is the most important reason to use names and it is most useful when the names are unique. Not all elements of a vector need to have a name. If some names are missing when you create the vector, the names will be set to an empty string for those elements. If you modify the vector in place by setting some, but not all variable names, names() will return NA (more specifically, NA_character_) for them. If all names are missing, names() will return NULL. y &lt;- c(a = 1, 2, 3) names(y) #&gt; [1] &quot;a&quot; &quot;&quot; &quot;&quot; v &lt;- c(1, 2, 3) names(v) &lt;- c(&#39;a&#39;) names(v) #&gt; [1] &quot;a&quot; NA NA z &lt;- c(1, 2, 3) names(z) #&gt; NULL You can create a new vector without names using unname(x), or remove names in place with names(x) &lt;- NULL. 2.2.1 Factors One important use of attributes is to define factors. A factor is a vector that can contain only predefined values, and is used to store categorical data. Factors are built on top of integer vectors using two attributes: the class, “factor”, which makes them behave differently from regular integer vectors, and the levels, which defines the set of allowed values. x &lt;- factor(c(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;)) x #&gt; [1] a b b a #&gt; Levels: a b class(x) #&gt; [1] &quot;factor&quot; levels(x) #&gt; [1] &quot;a&quot; &quot;b&quot; # You can&#39;t use values that are not in the levels x[2] &lt;- &quot;c&quot; #&gt; Warning in `[&lt;-.factor`(`*tmp*`, 2, value = &quot;c&quot;): ungültiges Faktorniveau, #&gt; NA erzeugt x #&gt; [1] a &lt;NA&gt; b a #&gt; Levels: a b # NB: you can&#39;t combine factors c(factor(&quot;a&quot;), factor(&quot;b&quot;)) #&gt; [1] 1 1 Factors are useful when you know the possible values a variable may take, even if you don’t see all values in a given dataset. Using a factor instead of a character vector makes it obvious when some groups contain no observations: sex_char &lt;- c(&quot;m&quot;, &quot;m&quot;, &quot;m&quot;) sex_factor &lt;- factor(sex_char, levels = c(&quot;m&quot;, &quot;f&quot;)) table(sex_char) #&gt; sex_char #&gt; m #&gt; 3 table(sex_factor) #&gt; sex_factor #&gt; m f #&gt; 3 0 Sometimes when a data frame is read directly from a file, a column you’d thought would produce a numeric vector instead produces a factor. This is caused by a non-numeric value in the column, often a missing value encoded in a special way like . or -. To remedy the situation, coerce the vector from a factor to a character vector, and then from a character to a double vector. (Be sure to check for missing values after this process.) Of course, a much better plan is to discover what caused the problem in the first place and fix that; using the na.strings argument to read.csv() is often a good place to start. # Reading in &quot;text&quot; instead of from a file here: z &lt;- read.csv(text = &quot;value\\n12\\n1\\n.\\n9&quot;) typeof(z$value) #&gt; [1] &quot;integer&quot; as.double(z$value) #&gt; [1] 3 2 1 4 # Oops, that&#39;s not right: 3 2 1 4 are the levels of a factor, # not the values we read in! class(z$value) #&gt; [1] &quot;factor&quot; # We can fix it now: as.double(as.character(z$value)) #&gt; Warning: NAs durch Umwandlung erzeugt #&gt; [1] 12 1 NA 9 # Or change how we read it in: z &lt;- read.csv(text = &quot;value\\n12\\n1\\n.\\n9&quot;, na.strings=&quot;.&quot;) typeof(z$value) #&gt; [1] &quot;integer&quot; class(z$value) #&gt; [1] &quot;integer&quot; z$value #&gt; [1] 12 1 NA 9 # Perfect! :) Unfortunately, most data loading functions in R automatically convert character vectors to factors. This is suboptimal, because there’s no way for those functions to know the set of all possible levels or their optimal order. Instead, use the argument stringsAsFactors = FALSE to suppress this behaviour, and then manually convert character vectors to factors using your knowledge of the data. A global option, options(stringsAsFactors = FALSE), is available to control this behaviour, but I don’t recommend using it. Changing a global option may have unexpected consequences when combined with other code (either from packages, or code that you’re source()ing), and global options make code harder to understand because they increase the number of lines you need to read to understand how a single line of code will behave. While factors look (and often behave) like character vectors, they are actually integers. Be careful when treating them like strings. Some string methods (like gsub() and grepl()) will coerce factors to strings, while others (like nchar()) will throw an error, and still others (like c()) will use the underlying integer values. For this reason, it’s usually best to explicitly convert factors to character vectors if you need string-like behaviour. In early versions of R, there was a memory advantage to using factors instead of character vectors, but this is no longer the case. 2.2.2 Exercises An early draft used this code to illustrate structure(): structure(1:5, comment = &quot;my attribute&quot;) #&gt; [1] 1 2 3 4 5 But when you print that object you don’t see the comment attribute. Why? Is the attribute missing, or is there something else special about it? (Hint: try using help.) What happens to a factor when you modify its levels? f1 &lt;- factor(letters) levels(f1) &lt;- rev(levels(f1)) What does this code do? How do f2 and f3 differ from f1? f2 &lt;- rev(factor(letters)) f3 &lt;- factor(letters, levels = rev(letters)) 2.3 Matrices and arrays Adding a dim attribute to an atomic vector allows it to behave like a multi-dimensional array. A special case of the array is the matrix, which has two dimensions. Matrices are used commonly as part of the mathematical machinery of statistics. Arrays are much rarer, but worth being aware of. Matrices and arrays are created with matrix() and array(), or by using the assignment form of dim(): # Two scalar arguments to specify rows and columns a &lt;- matrix(1:6, ncol = 3, nrow = 2) # One vector argument to describe all dimensions b &lt;- array(1:12, c(2, 3, 2)) # You can also modify an object in place by setting dim() c &lt;- 1:6 dim(c) &lt;- c(3, 2) c #&gt; [,1] [,2] #&gt; [1,] 1 4 #&gt; [2,] 2 5 #&gt; [3,] 3 6 dim(c) &lt;- c(2, 3) c #&gt; [,1] [,2] [,3] #&gt; [1,] 1 3 5 #&gt; [2,] 2 4 6 length() and names() have high-dimensional generalisations: length() generalises to nrow() and ncol() for matrices, and dim() for arrays. names() generalises to rownames() and colnames() for matrices, and dimnames(), a list of character vectors, for arrays. length(a) #&gt; [1] 6 nrow(a) #&gt; [1] 2 ncol(a) #&gt; [1] 3 rownames(a) &lt;- c(&quot;A&quot;, &quot;B&quot;) colnames(a) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) a #&gt; a b c #&gt; A 1 3 5 #&gt; B 2 4 6 length(b) #&gt; [1] 12 dim(b) #&gt; [1] 2 3 2 dimnames(b) &lt;- list(c(&quot;one&quot;, &quot;two&quot;), c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), c(&quot;A&quot;, &quot;B&quot;)) b #&gt; , , A #&gt; #&gt; a b c #&gt; one 1 3 5 #&gt; two 2 4 6 #&gt; #&gt; , , B #&gt; #&gt; a b c #&gt; one 7 9 11 #&gt; two 8 10 12 c() generalises to cbind() and rbind() for matrices, and to abind() (provided by the abind package) for arrays. You can transpose a matrix with t(); the generalised equivalent for arrays is aperm(). You can test if an object is a matrix or array using is.matrix() and is.array(), or by looking at the length of the dim(). as.matrix() and as.array() make it easy to turn an existing vector into a matrix or array. Vectors are not the only 1-dimensional data structure. You can have matrices with a single row or single column, or arrays with a single dimension. They may print similarly, but will behave differently. The differences aren’t too important, but it’s useful to know they exist in case you get strange output from a function (tapply() is a frequent offender). As always, use str() to reveal the differences. str(1:3) # 1d vector #&gt; int [1:3] 1 2 3 str(matrix(1:3, ncol = 1)) # column vector #&gt; int [1:3, 1] 1 2 3 str(matrix(1:3, nrow = 1)) # row vector #&gt; int [1, 1:3] 1 2 3 str(array(1:3, 3)) # &quot;array&quot; vector #&gt; int [1:3(1d)] 1 2 3 While atomic vectors are most commonly turned into matrices, the dimension attribute can also be set on lists to make list-matrices or list-arrays: l &lt;- list(1:3, &quot;a&quot;, TRUE, 1.0) dim(l) &lt;- c(2, 2) l #&gt; [,1] [,2] #&gt; [1,] Integer,3 TRUE #&gt; [2,] &quot;a&quot; 1 These are relatively esoteric data structures, but can be useful if you want to arrange objects into a grid-like structure. For example, if you’re running models on a spatio-temporal grid, it might be natural to preserve the grid structure by storing the models in a 3d array. 2.3.1 Exercises What does dim() return when applied to a vector? If is.matrix(x) is TRUE, what will is.array(x) return? How would you describe the following three objects? What makes them different to 1:5? x1 &lt;- array(1:5, c(1, 1, 5)) x2 &lt;- array(1:5, c(1, 5, 1)) x3 &lt;- array(1:5, c(5, 1, 1)) 2.4 Data frames A data frame is the most common way of storing data in R, and if used systematically makes data analysis easier. Under the hood, a data frame is a list of equal-length vectors. This makes it a 2-dimensional structure, so it shares properties of both the matrix and the list. This means that a data frame has names(), colnames(), and rownames(), although names() and colnames() are the same thing. The length() of a data frame is the length of the underlying list and so is the same as ncol(); nrow() gives the number of rows. As described in subsetting, you can subset a data frame like a 1d structure (where it behaves like a list), or a 2d structure (where it behaves like a matrix). 2.4.1 Creation You create a data frame using data.frame(), which takes named vectors as input: df &lt;- data.frame(x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) str(df) #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ x: int 1 2 3 #&gt; $ y: Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 2 3 Beware data.frame()’s default behaviour which turns strings into factors. Use stringsAsFactors = FALSE to suppress this behaviour: df &lt;- data.frame( x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), stringsAsFactors = FALSE) str(df) #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ x: int 1 2 3 #&gt; $ y: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; 2.4.2 Testing and coercion Because a data.frame is an S3 class, its type reflects the underlying vector used to build it: the list. To check if an object is a data frame, use class() or test explicitly with is.data.frame(): typeof(df) #&gt; [1] &quot;list&quot; class(df) #&gt; [1] &quot;data.frame&quot; is.data.frame(df) #&gt; [1] TRUE You can coerce an object to a data frame with as.data.frame(): A vector will create a one-column data frame. A list will create one column for each element; it’s an error if they’re not all the same length. A matrix will create a data frame with the same number of columns and rows as the matrix. 2.4.3 Combining data frames You can combine data frames using cbind() and rbind(): cbind(df, data.frame(z = 3:1)) #&gt; x y z #&gt; 1 1 a 3 #&gt; 2 2 b 2 #&gt; 3 3 c 1 rbind(df, data.frame(x = 10, y = &quot;z&quot;)) #&gt; x y #&gt; 1 1 a #&gt; 2 2 b #&gt; 3 3 c #&gt; 4 10 z When combining column-wise, the number of rows must match, but row names are ignored. When combining row-wise, both the number and names of columns must match. Use plyr::rbind.fill() to combine data frames that don’t have the same columns. It’s a common mistake to try and create a data frame by cbind()ing vectors together. This doesn’t work because cbind() will create a matrix unless one of the arguments is already a data frame. Instead use data.frame() directly: bad &lt;- data.frame(cbind(a = 1:2, b = c(&quot;a&quot;, &quot;b&quot;))) str(bad) #&gt; &#39;data.frame&#39;: 2 obs. of 2 variables: #&gt; $ a: Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 2 #&gt; $ b: Factor w/ 2 levels &quot;a&quot;,&quot;b&quot;: 1 2 good &lt;- data.frame(a = 1:2, b = c(&quot;a&quot;, &quot;b&quot;), stringsAsFactors = FALSE) str(good) #&gt; &#39;data.frame&#39;: 2 obs. of 2 variables: #&gt; $ a: int 1 2 #&gt; $ b: chr &quot;a&quot; &quot;b&quot; The conversion rules for cbind() are complicated and best avoided by ensuring all inputs are of the same type. 2.4.4 Special columns Since a data frame is a list of vectors, it is possible for a data frame to have a column that is a list: df &lt;- data.frame(x = 1:3) df$y &lt;- list(1:2, 1:3, 1:4) df #&gt; x y #&gt; 1 1 1, 2 #&gt; 2 2 1, 2, 3 #&gt; 3 3 1, 2, 3, 4 However, when a list is given to data.frame(), it tries to put each item of the list into its own column, so this fails: data.frame(x = 1:3, y = list(1:2, 1:3, 1:4)) #&gt; Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE, : Argumente implizieren unterschiedliche Anzahl Zeilen: 2, 3, 4 A workaround is to use I(), which causes data.frame() to treat the list as one unit: dfl &lt;- data.frame(x = 1:3, y = I(list(1:2, 1:3, 1:4))) str(dfl) #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ x: int 1 2 3 #&gt; $ y:List of 3 #&gt; ..$ : int 1 2 #&gt; ..$ : int 1 2 3 #&gt; ..$ : int 1 2 3 4 #&gt; ..- attr(*, &quot;class&quot;)= chr &quot;AsIs&quot; dfl[2, &quot;y&quot;] #&gt; [[1]] #&gt; [1] 1 2 3 I() adds the AsIs class to its input, but this can usually be safely ignored. Similarly, it’s also possible to have a column of a data frame that’s a matrix or array, as long as the number of rows matches the data frame: dfm &lt;- data.frame(x = 1:3, y = I(matrix(1:9, nrow = 3))) str(dfm) #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ x: int 1 2 3 #&gt; $ y: &#39;AsIs&#39; int [1:3, 1:3] 1 2 3 4 5 6 7 8 9 dfm[2, &quot;y&quot;] #&gt; [,1] [,2] [,3] #&gt; [1,] 2 5 8 Use list and array columns with caution: many functions that work with data frames assume that all columns are atomic vectors. 2.4.5 Exercises What attributes does a data frame possess? What does as.matrix() do when applied to a data frame with columns of different types? Can you have a data frame with 0 rows? What about 0 columns? 2.5 Answers The three properties of a vector are type, length, and attributes. The four common types of atomic vector are logical, integer, double (sometimes called numeric), and character. The two rarer types are complex and raw. Attributes allow you to associate arbitrary additional metadata to any object. You can get and set individual attributes with attr(x, &quot;y&quot;) and attr(x, &quot;y&quot;) &lt;- value; or get and set all attributes at once with attributes(). The elements of a list can be any type (even a list); the elements of an atomic vector are all of the same type. Similarly, every element of a matrix must be the same type; in a data frame, the different columns can have different types. You can make “list-array” by assigning dimensions to a list. You can make a matrix a column of a data frame with df$x &lt;- matrix(), or using I() when creating a new data frame data.frame(x = I(matrix())). "],
["subsetting.html", "3 Subsetting 3.1 Data types 3.2 Subsetting operators 3.3 Subsetting and assignment 3.4 Applications 3.5 Answers", " 3 Subsetting R’s subsetting operators are powerful and fast. Mastery of subsetting allows you to succinctly express complex operations in a way that few other languages can match. Subsetting is hard to learn because you need to master a number of interrelated concepts: The three subsetting operators. The six types of subsetting. Important differences in behaviour for different objects (e.g., vectors, lists, factors, matrices, and data frames). The use of subsetting in conjunction with assignment. This chapter helps you master subsetting by starting with the simplest type of subsetting: subsetting an atomic vector with [. It then gradually extends your knowledge, first to more complicated data types (like arrays and lists), and then to the other subsetting operators, [[ and $. You’ll then learn how subsetting and assignment can be combined to modify parts of an object, and, finally, you’ll see a large number of useful applications. Subsetting is a natural complement to str(). str() shows you the structure of any object, and subsetting allows you to pull out the pieces that you’re interested in. 3.0.0.0.1 Quiz Take this short quiz to determine if you need to read this chapter. If the answers quickly come to mind, you can comfortably skip this chapter. Check your answers in answers. What is the result of subsetting a vector with positive integers, negative integers, a logical vector, or a character vector? What’s the difference between [, [[, and $ when applied to a list? When should you use drop = FALSE? If x is a matrix, what does x[] &lt;- 0 do? How is it different to x &lt;- 0? How can you use a named vector to relabel categorical variables? 3.0.0.0.2 Outline Data types starts by teaching you about [. You’ll start by learning the six types of data that you can use to subset atomic vectors. You’ll then learn how those six data types act when used to subset lists, matrices, data frames, and S3 objects. Subsetting operators expands your knowledge of subsetting operators to include [[ and $, focussing on the important principles of simplifying vs. preserving. In Subsetting and assignment you’ll learn the art of subassignment, combining subsetting and assignment to modify parts of an object. Applications leads you through eight important, but not obvious, applications of subsetting to solve problems that you often encounter in a data analysis. 3.1 Data types It’s easiest to learn how subsetting works for atomic vectors, and then how it generalises to higher dimensions and other more complicated objects. We’ll start with [, the most commonly used operator. Subsetting operators will cover [[ and $, the two other main subsetting operators. 3.1.1 Atomic vectors Let’s explore the different types of subsetting with a simple vector, x. x &lt;- c(2.1, 4.2, 3.3, 5.4) Note that the number after the decimal point gives the original position in the vector. There are five things that you can use to subset a vector: Positive integers return elements at the specified positions: x[c(3, 1)] ## [1] 3.3 2.1 x[order(x)] ## [1] 2.1 3.3 4.2 5.4 # Duplicated indices yield duplicated values x[c(1, 1)] ## [1] 2.1 2.1 # Real numbers are silently truncated to integers x[c(2.1, 2.9)] ## [1] 4.2 4.2 Negative integers omit elements at the specified positions: x[-c(3, 1)] ## [1] 4.2 5.4 You can’t mix positive and negative integers in a single subset: x[c(-1, 2)] ## Error in x[c(-1, 2)]: only 0&#39;s may be mixed with negative subscripts Logical vectors select elements where the corresponding logical value is TRUE. This is probably the most useful type of subsetting because you write the expression that creates the logical vector: x[c(TRUE, TRUE, FALSE, FALSE)] ## [1] 2.1 4.2 x[x &gt; 3] ## [1] 4.2 3.3 5.4 If the logical vector is shorter than the vector being subsetted, it will be recycled to be the same length. x[c(TRUE, FALSE)] ## [1] 2.1 3.3 # Equivalent to x[c(TRUE, FALSE, TRUE, FALSE)] ## [1] 2.1 3.3 A missing value in the index always yields a missing value in the output: x[c(TRUE, TRUE, NA, FALSE)] ## [1] 2.1 4.2 NA Nothing returns the original vector. This is not useful for vectors but is very useful for matrices, data frames, and arrays. It can also be useful in conjunction with assignment. x[] ## [1] 2.1 4.2 3.3 5.4 Zero returns a zero-length vector. This is not something you usually do on purpose, but it can be helpful for generating test data. x[0] ## numeric(0) If the vector is named, you can also use: Character vectors to return elements with matching names. (y &lt;- setNames(x, letters[1:4])) ## a b c d ## 2.1 4.2 3.3 5.4 y[c(&quot;d&quot;, &quot;c&quot;, &quot;a&quot;)] ## d c a ## 5.4 3.3 2.1 # Like integer indices, you can repeat indices y[c(&quot;a&quot;, &quot;a&quot;, &quot;a&quot;)] ## a a a ## 2.1 2.1 2.1 # When subsetting with [ names are always matched exactly z &lt;- c(abc = 1, def = 2) z[c(&quot;a&quot;, &quot;d&quot;)] ## &lt;NA&gt; &lt;NA&gt; ## NA NA 3.1.2 Lists Subsetting a list works in the same way as subsetting an atomic vector. Using [ will always return a list; [[ and $, as described below, let you pull out the components of the list. 3.1.3 Matrices and arrays You can subset higher-dimensional structures in three ways: With multiple vectors. With a single vector. With a matrix. The most common way of subsetting matrices (2d) and arrays (&gt;2d) is a simple generalisation of 1d subsetting: you supply a 1d index for each dimension, separated by a comma. Blank subsetting is now useful because it lets you keep all rows or all columns. a &lt;- matrix(1:9, nrow = 3) colnames(a) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) a[1:2, ] ## A B C ## [1,] 1 4 7 ## [2,] 2 5 8 a[c(TRUE, FALSE, TRUE), c(&quot;B&quot;, &quot;A&quot;)] ## B A ## [1,] 4 1 ## [2,] 6 3 a[0, -2] ## A C By default, [ will simplify the results to the lowest possible dimensionality. See simplifying vs. preserving to learn how to avoid this. Because matrices and arrays are implemented as vectors with special attributes, you can subset them with a single vector. In that case, they will behave like a vector. Arrays in R are stored in column-major order: (vals &lt;- outer(1:5, 1:5, FUN = &quot;paste&quot;, sep = &quot;,&quot;)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;1,1&quot; &quot;1,2&quot; &quot;1,3&quot; &quot;1,4&quot; &quot;1,5&quot; ## [2,] &quot;2,1&quot; &quot;2,2&quot; &quot;2,3&quot; &quot;2,4&quot; &quot;2,5&quot; ## [3,] &quot;3,1&quot; &quot;3,2&quot; &quot;3,3&quot; &quot;3,4&quot; &quot;3,5&quot; ## [4,] &quot;4,1&quot; &quot;4,2&quot; &quot;4,3&quot; &quot;4,4&quot; &quot;4,5&quot; ## [5,] &quot;5,1&quot; &quot;5,2&quot; &quot;5,3&quot; &quot;5,4&quot; &quot;5,5&quot; vals[c(4, 15)] ## [1] &quot;4,1&quot; &quot;5,3&quot; You can also subset higher-dimensional data structures with an integer matrix (or, if named, a character matrix). Each row in the matrix specifies the location of one value, where each column corresponds to a dimension in the array being subsetted. This means that you use a 2 column matrix to subset a matrix, a 3 column matrix to subset a 3d array, and so on. The result is a vector of values: vals &lt;- outer(1:5, 1:5, FUN = &quot;paste&quot;, sep = &quot;,&quot;) select &lt;- matrix(ncol = 2, byrow = TRUE, c( 1, 1, 3, 1, 2, 4 )) vals[select] ## [1] &quot;1,1&quot; &quot;3,1&quot; &quot;2,4&quot; 3.1.4 Data frames Data frames possess the characteristics of both lists and matrices: if you subset with a single vector, they behave like lists; if you subset with two vectors, they behave like matrices. df &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3]) df[df$x == 2, ] ## x y z ## 2 2 2 b df[c(1, 3), ] ## x y z ## 1 1 3 a ## 3 3 1 c # There are two ways to select columns from a data frame # Like a list: df[c(&quot;x&quot;, &quot;z&quot;)] ## x z ## 1 1 a ## 2 2 b ## 3 3 c # Like a matrix df[, c(&quot;x&quot;, &quot;z&quot;)] ## x z ## 1 1 a ## 2 2 b ## 3 3 c # There&#39;s an important difference if you select a single # column: matrix subsetting simplifies by default, list # subsetting does not. str(df[&quot;x&quot;]) ## &#39;data.frame&#39;: 3 obs. of 1 variable: ## $ x: int 1 2 3 str(df[, &quot;x&quot;]) ## int [1:3] 1 2 3 3.1.5 S3 objects S3 objects are made up of atomic vectors, arrays, and lists, so you can always pull apart an S3 object using the techniques described above and the knowledge you gain from str(). 3.1.6 S4 objects There are also two additional subsetting operators that are needed for S4 objects: @ (equivalent to $), and slot() (equivalent to [[). @ is more restrictive than $ in that it will return an error if the slot does not exist. These are described in more detail in the OO field guide. 3.1.7 Exercises Fix each of the following common data frame subsetting errors: mtcars[mtcars$cyl = 4, ] mtcars[-1:4, ] mtcars[mtcars$cyl &lt;= 5] mtcars[mtcars$cyl == 4 | 6, ] Why does x &lt;- 1:5; x[NA] yield five missing values? (Hint: why is it different from x[NA_real_]?) What does upper.tri() return? How does subsetting a matrix with it work? Do we need any additional subsetting rules to describe its behaviour? x &lt;- outer(1:5, 1:5, FUN = &quot;*&quot;) x[upper.tri(x)] Why does mtcars[1:20] return an error? How does it differ from the similar mtcars[1:20, ]? Implement your own function that extracts the diagonal entries from a matrix (it should behave like diag(x) where x is a matrix). What does df[is.na(df)] &lt;- 0 do? How does it work? 3.2 Subsetting operators There are two other subsetting operators: [[ and $. [[ is similar to [, except it can only return a single value and it allows you to pull pieces out of a list. $ is a useful shorthand for [[ combined with character subsetting. You need [[ when working with lists. This is because when [ is applied to a list it always returns a list: it never gives you the contents of the list. To get the contents, you need [[: “If list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.” — @RLangTip Because it can return only a single value, you must use [[ with either a single positive integer or a string: a &lt;- list(a = 1, b = 2) a[[1]] ## [1] 1 a[[&quot;a&quot;]] ## [1] 1 # If you do supply a vector it indexes recursively b &lt;- list(a = list(b = list(c = list(d = 1)))) b[[c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)]] ## [1] 1 # Same as b[[&quot;a&quot;]][[&quot;b&quot;]][[&quot;c&quot;]][[&quot;d&quot;]] ## [1] 1 Because data frames are lists of columns, you can use [[ to extract a column from data frames: mtcars[[1]], mtcars[[&quot;cyl&quot;]]. S3 and S4 objects can override the standard behaviour of [ and [[ so they behave differently for different types of objects. The key difference is usually how you select between simplifying or preserving behaviours, and what the default is. 3.2.1 Simplifying vs. preserving subsetting It’s important to understand the distinction between simplifying and preserving subsetting. Simplifying subsets returns the simplest possible data structure that can represent the output, and is useful interactively because it usually gives you what you want. Preserving subsetting keeps the structure of the output the same as the input, and is generally better for programming because the result will always be the same type. Omitting drop = FALSE when subsetting matrices and data frames is one of the most common sources of programming errors. (It will work for your test cases, but then someone will pass in a single column data frame and it will fail in an unexpected and unclear way.) Unfortunately, how you switch between simplifying and preserving differs for different data types, as summarised in the table below. Simplifying Preserving Vector x[[1]] x[1] List x[[1]] x[1] Factor x[1:4, drop = T] x[1:4] Array x[1, ] or x[, 1] x[1, , drop = F] or x[, 1, drop = F] Data frame x[, 1] or x[[1]] x[, 1, drop = F] or x[1] Preserving is the same for all data types: you get the same type of output as input. Simplifying behaviour varies slightly between different data types, as described below: Atomic vector: removes names. x &lt;- c(a = 1, b = 2) x[1] ## a ## 1 x[[1]] ## [1] 1 List: return the object inside the list, not a single element list. y &lt;- list(a = 1, b = 2) str(y[1]) ## List of 1 ## $ a: num 1 str(y[[1]]) ## num 1 Factor: drops any unused levels. z &lt;- factor(c(&quot;a&quot;, &quot;b&quot;)) z[1] ## [1] a ## Levels: a b z[1, drop = TRUE] ## [1] a ## Levels: a Matrix or array: if any of the dimensions has length 1, drops that dimension. a &lt;- matrix(1:4, nrow = 2) a[1, , drop = FALSE] ## [,1] [,2] ## [1,] 1 3 a[1, ] ## [1] 1 3 Data frame: if output is a single column, returns a vector instead of a data frame. df &lt;- data.frame(a = 1:2, b = 1:2) str(df[1]) ## &#39;data.frame&#39;: 2 obs. of 1 variable: ## $ a: int 1 2 str(df[[1]]) ## int [1:2] 1 2 str(df[, &quot;a&quot;, drop = FALSE]) ## &#39;data.frame&#39;: 2 obs. of 1 variable: ## $ a: int 1 2 str(df[, &quot;a&quot;]) ## int [1:2] 1 2 3.2.2 $ $ is a shorthand operator, where x$y is equivalent to x[[&quot;y&quot;, exact = FALSE]]. It’s often used to access variables in a data frame, as in mtcars$cyl or diamonds$carat. One common mistake with $ is to try and use it when you have the name of a column stored in a variable: var &lt;- &quot;cyl&quot; # Doesn&#39;t work - mtcars$var translated to mtcars[[&quot;var&quot;]] mtcars$var ## NULL # Instead use [[ mtcars[[var]] ## [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4 There’s one important difference between $ and [[. $ does partial matching: x &lt;- list(abc = 1) x$a ## [1] 1 x[[&quot;a&quot;]] ## NULL If you want to avoid this behaviour you can set the global option warnPartialMatchDollar to TRUE. Use with caution: it may affect behaviour in other code you have loaded (e.g., from a package). 3.2.3 Missing/out of bounds indices [ and [[ differ slightly in their behaviour when the index is out of bounds (OOB), for example, when you try to extract the fifth element of a length four vector, or subset a vector with NA or NULL: x &lt;- 1:4 str(x[5]) ## int NA str(x[NA_real_]) ## int NA str(x[NULL]) ## int(0) The following table summarises the results of subsetting atomic vectors and lists with [ and [[ and different types of OOB value. Operator Index Atomic List [ OOB NA list(NULL) [ NA_real_ NA list(NULL) [ NULL x[0] list() [[ OOB Error Error [[ NA_real_ Error NULL [[ NULL Error Error If the input vector is named, then the names of OOB, missing, or NULL components will be &quot;&lt;NA&gt;&quot;. 3.2.4 Exercises Given a linear model, e.g., mod &lt;- lm(mpg ~ wt, data = mtcars), extract the residual degrees of freedom. Extract the R squared from the model summary (summary(mod)) 3.3 Subsetting and assignment All subsetting operators can be combined with assignment to modify selected values of the input vector. x &lt;- 1:5 x[c(1, 2)] &lt;- 2:3 x ## [1] 2 3 3 4 5 # The length of the LHS needs to match the RHS x[-1] &lt;- 4:1 x ## [1] 2 4 3 2 1 # Duplicated indices go unchecked and may be problematic x[c(1, 1)] &lt;- 2:3 x ## [1] 3 4 3 2 1 # You can&#39;t combine integer indices with NA x[c(1, NA)] &lt;- c(1, 2) ## Error in x[c(1, NA)] &lt;- c(1, 2): NAs are not allowed in subscripted assignments # But you can combine logical indices with NA # (where they&#39;re treated as false). x[c(T, F, NA)] &lt;- 1 x ## [1] 1 4 3 1 1 # This is mostly useful when conditionally modifying vectors df &lt;- data.frame(a = c(1, 10, NA)) df$a[df$a &lt; 5] &lt;- 0 df$a ## [1] 0 10 NA Subsetting with nothing can be useful in conjunction with assignment because it will preserve the original object class and structure. Compare the following two expressions. In the first, mtcars will remain as a data frame. In the second, mtcars will become a list. mtcars[] &lt;- lapply(mtcars, as.integer) mtcars &lt;- lapply(mtcars, as.integer) With lists, you can use subsetting + assignment + NULL to remove components from a list. To add a literal NULL to a list, use [ and list(NULL): x &lt;- list(a = 1, b = 2) x[[&quot;b&quot;]] &lt;- NULL str(x) ## List of 1 ## $ a: num 1 y &lt;- list(a = 1) y[&quot;b&quot;] &lt;- list(NULL) str(y) ## List of 2 ## $ a: num 1 ## $ b: NULL 3.4 Applications The basic principles described above give rise to a wide variety of useful applications. Some of the most important are described below. Many of these basic techniques are wrapped up into more concise functions (e.g., subset(), merge(), plyr::arrange()), but it is useful to understand how they are implemented with basic subsetting. This will allow you to adapt to new situations that are not dealt with by existing functions. 3.4.1 Lookup tables (character subsetting) Character matching provides a powerful way to make lookup tables. Say you want to convert abbreviations: x &lt;- c(&quot;m&quot;, &quot;f&quot;, &quot;u&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;) lookup &lt;- c(m = &quot;Male&quot;, f = &quot;Female&quot;, u = NA) lookup[x] ## m f u f f m m ## &quot;Male&quot; &quot;Female&quot; NA &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; unname(lookup[x]) ## [1] &quot;Male&quot; &quot;Female&quot; NA &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; # Or with fewer output values c(m = &quot;Known&quot;, f = &quot;Known&quot;, u = &quot;Unknown&quot;)[x] ## m f u f f m m ## &quot;Known&quot; &quot;Known&quot; &quot;Unknown&quot; &quot;Known&quot; &quot;Known&quot; &quot;Known&quot; &quot;Known&quot; If you don’t want names in the result, use unname() to remove them. 3.4.2 Matching and merging by hand (integer subsetting) You may have a more complicated lookup table which has multiple columns of information. Suppose we have a vector of integer grades, and a table that describes their properties: grades &lt;- c(1, 2, 2, 3, 1) info &lt;- data.frame( grade = 3:1, desc = c(&quot;Excellent&quot;, &quot;Good&quot;, &quot;Poor&quot;), fail = c(F, F, T) ) We want to duplicate the info table so that we have a row for each value in grades. We can do this in two ways, either using match() and integer subsetting, or rownames() and character subsetting: grades ## [1] 1 2 2 3 1 # Using match id &lt;- match(grades, info$grade) info[id, ] ## grade desc fail ## 3 1 Poor TRUE ## 2 2 Good FALSE ## 2.1 2 Good FALSE ## 1 3 Excellent FALSE ## 3.1 1 Poor TRUE # Using rownames rownames(info) &lt;- info$grade info[as.character(grades), ] ## grade desc fail ## 1 1 Poor TRUE ## 2 2 Good FALSE ## 2.1 2 Good FALSE ## 3 3 Excellent FALSE ## 1.1 1 Poor TRUE If you have multiple columns to match on, you’ll need to first collapse them to a single column (with interaction(), paste(), or plyr::id()). You can also use merge() or plyr::join(), which do the same thing for you — read the source code to see how. 3.4.3 Random samples/bootstrap (integer subsetting) You can use integer indices to perform random sampling or bootstrapping of a vector or data frame. sample() generates a vector of indices, then subsetting to access the values: df &lt;- data.frame(x = rep(1:3, each = 2), y = 6:1, z = letters[1:6]) # Set seed for reproducibility set.seed(10) # Randomly reorder df[sample(nrow(df)), ] ## x y z ## 4 2 3 d ## 2 1 5 b ## 5 3 2 e ## 3 2 4 c ## 1 1 6 a ## 6 3 1 f # Select 3 random rows df[sample(nrow(df), 3), ] ## x y z ## 2 1 5 b ## 6 3 1 f ## 3 2 4 c # Select 6 bootstrap replicates df[sample(nrow(df), 6, rep = T), ] ## x y z ## 3 2 4 c ## 4 2 3 d ## 4.1 2 3 d ## 1 1 6 a ## 4.2 2 3 d ## 3.1 2 4 c The arguments of sample() control the number of samples to extract, and whether sampling is performed with or without replacement. 3.4.4 Ordering (integer subsetting) order() takes a vector as input and returns an integer vector describing how the subsetted vector should be ordered: x &lt;- c(&quot;b&quot;, &quot;c&quot;, &quot;a&quot;) order(x) ## [1] 3 1 2 x[order(x)] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; To break ties, you can supply additional variables to order(), and you can change from ascending to descending order using decreasing = TRUE. By default, any missing values will be put at the end of the vector; however, you can remove them with na.last = NA or put at the front with na.last = FALSE. For two or more dimensions, order() and integer subsetting makes it easy to order either the rows or columns of an object: # Randomly reorder df df2 &lt;- df[sample(nrow(df)), 3:1] df2 ## z y x ## 3 c 4 2 ## 1 a 6 1 ## 2 b 5 1 ## 4 d 3 2 ## 6 f 1 3 ## 5 e 2 3 df2[order(df2$x), ] ## z y x ## 1 a 6 1 ## 2 b 5 1 ## 3 c 4 2 ## 4 d 3 2 ## 6 f 1 3 ## 5 e 2 3 df2[, order(names(df2))] ## x y z ## 3 2 4 c ## 1 1 6 a ## 2 1 5 b ## 4 2 3 d ## 6 3 1 f ## 5 3 2 e More concise, but less flexible, functions are available for sorting vectors, sort(), and data frames, plyr::arrange(). 3.4.5 Expanding aggregated counts (integer subsetting) Sometimes you get a data frame where identical rows have been collapsed into one and a count column has been added. rep() and integer subsetting make it easy to uncollapse the data by subsetting with a repeated row index: df &lt;- data.frame(x = c(2, 4, 1), y = c(9, 11, 6), n = c(3, 5, 1)) rep(1:nrow(df), df$n) ## [1] 1 1 1 2 2 2 2 2 3 df[rep(1:nrow(df), df$n), ] ## x y n ## 1 2 9 3 ## 1.1 2 9 3 ## 1.2 2 9 3 ## 2 4 11 5 ## 2.1 4 11 5 ## 2.2 4 11 5 ## 2.3 4 11 5 ## 2.4 4 11 5 ## 3 1 6 1 3.4.6 Removing columns from data frames (character subsetting) There are two ways to remove columns from a data frame. You can set individual columns to NULL: df &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3]) df$z &lt;- NULL Or you can subset to return only the columns you want: df &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3]) df[c(&quot;x&quot;, &quot;y&quot;)] ## x y ## 1 1 3 ## 2 2 2 ## 3 3 1 If you know the columns you don’t want, use set operations to work out which colums to keep: df[setdiff(names(df), &quot;z&quot;)] ## x y ## 1 1 3 ## 2 2 2 ## 3 3 1 3.4.7 Selecting rows based on a condition (logical subsetting) Because it allows you to easily combine conditions from multiple columns, logical subsetting is probably the most commonly used technique for extracting rows out of a data frame. mtcars[mtcars$gear == 5, ] ## mpg cyl disp hp drat wt qsec vs am gear carb ## 27 26.0 4 120.3 91 4.43 2.140 16.7 0 1 5 2 ## 28 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 ## 29 15.8 8 351.0 264 4.22 3.170 14.5 0 1 5 4 ## 30 19.7 6 145.0 175 3.62 2.770 15.5 0 1 5 6 ## 31 15.0 8 301.0 335 3.54 3.570 14.6 0 1 5 8 mtcars[mtcars$gear == 5 &amp; mtcars$cyl == 4, ] ## mpg cyl disp hp drat wt qsec vs am gear carb ## 27 26.0 4 120.3 91 4.43 2.140 16.7 0 1 5 2 ## 28 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 Remember to use the vector boolean operators &amp; and |, not the short-circuiting scalar operators &amp;&amp; and || which are more useful inside if statements. Don’t forget De Morgan’s laws, which can be useful to simplify negations: !(X &amp; Y) is the same as !X | !Y !(X | Y) is the same as !X &amp; !Y For example, !(X &amp; !(Y | Z)) simplifies to !X | !!(Y|Z), and then to !X | Y | Z. subset() is a specialised shorthand function for subsetting data frames, and saves some typing because you don’t need to repeat the name of the data frame. You’ll learn how it works in non-standard evaluation. subset(mtcars, gear == 5) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 27 26.0 4 120.3 91 4.43 2.140 16.7 0 1 5 2 ## 28 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 ## 29 15.8 8 351.0 264 4.22 3.170 14.5 0 1 5 4 ## 30 19.7 6 145.0 175 3.62 2.770 15.5 0 1 5 6 ## 31 15.0 8 301.0 335 3.54 3.570 14.6 0 1 5 8 subset(mtcars, gear == 5 &amp; cyl == 4) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 27 26.0 4 120.3 91 4.43 2.140 16.7 0 1 5 2 ## 28 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 3.4.8 Boolean algebra vs. sets (logical &amp; integer subsetting) It’s useful to be aware of the natural equivalence between set operations (integer subsetting) and boolean algebra (logical subsetting). Using set operations is more effective when: You want to find the first (or last) TRUE. You have very few TRUEs and very many FALSEs; a set representation may be faster and require less storage. which() allows you to convert a boolean representation to an integer representation. There’s no reverse operation in base R but we can easily create one: x &lt;- sample(10) &lt; 4 which(x) ## [1] 3 7 10 unwhich &lt;- function(x, n) { out &lt;- rep_len(FALSE, n) out[x] &lt;- TRUE out } unwhich(which(x), 10) ## [1] FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE TRUE Let’s create two logical vectors and their integer equivalents and then explore the relationship between boolean and set operations. (x1 &lt;- 1:10 %% 2 == 0) ## [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE (x2 &lt;- which(x1)) ## [1] 2 4 6 8 10 (y1 &lt;- 1:10 %% 5 == 0) ## [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE (y2 &lt;- which(y1)) ## [1] 5 10 # X &amp; Y &lt;-&gt; intersect(x, y) x1 &amp; y1 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE intersect(x2, y2) ## [1] 10 # X | Y &lt;-&gt; union(x, y) x1 | y1 ## [1] FALSE TRUE FALSE TRUE TRUE TRUE FALSE TRUE FALSE TRUE union(x2, y2) ## [1] 2 4 6 8 10 5 # X &amp; !Y &lt;-&gt; setdiff(x, y) x1 &amp; !y1 ## [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE FALSE setdiff(x2, y2) ## [1] 2 4 6 8 # xor(X, Y) &lt;-&gt; setdiff(union(x, y), intersect(x, y)) xor(x1, y1) ## [1] FALSE TRUE FALSE TRUE TRUE TRUE FALSE TRUE FALSE FALSE setdiff(union(x2, y2), intersect(x2, y2)) ## [1] 2 4 6 8 5 When first learning subsetting, a common mistake is to use x[which(y)] instead of x[y]. Here the which() achieves nothing: it switches from logical to integer subsetting but the result will be exactly the same. In more general cases, there are two important differences. First, when the logical vector contains NA, logical subsetting replaces these values by NA while which() drops these values. Second, x[-which(y)] is not equivalent to x[!y]: if y is all FALSE, which(y) will be integer(0) and -integer(0) is still integer(0), so you’ll get no values, instead of all values. In general, avoid switching from logical to integer subsetting unless you want, for example, the first or last TRUE value. 3.4.9 Exercises How would you randomly permute the columns of a data frame? (This is an important technique in random forests.) Can you simultaneously permute the rows and columns in one step? How would you select a random sample of m rows from a data frame? What if the sample had to be contiguous (i.e., with an initial row, a final row, and every row in between)? How could you put the columns in a data frame in alphabetical order? 3.5 Answers Positive integers select elements at specific positions, negative integers drop elements; logical vectors keep elements at positions corresponding to TRUE; character vectors select elements with matching names. [ selects sub-lists. It always returns a list; if you use it with a single positive integer, it returns a list of length one. [[ selects an element within a list. $ is a convenient shorthand: x$y is equivalent to x[[&quot;y&quot;]]. Use drop = FALSE if you are subsetting a matrix, array, or data frame and you want to preserve the original dimensions. You should almost always use it when subsetting inside a function. If x is a matrix, x[] &lt;- 0 will replace every element with 0, keeping the same number of rows and columns. x &lt;- 0 completely replaces the matrix with the value 0. A named character vector can act as a simple lookup table: c(x = 1, y = 2, z = 3)[c(&quot;y&quot;, &quot;z&quot;, &quot;x&quot;)] "],
["functions.html", "4 Functions 4.1 Function components 4.2 Lexical scoping 4.3 Every operation is a function call 4.4 Function arguments 4.5 Special calls 4.6 Return values 4.7 Quiz answers", " 4 Functions Functions are a fundamental building block of R: to master many of the more advanced techniques in this book, you need a solid foundation in how functions work. You’ve probably already created many R functions, and you’re familiar with the basics of how they work. The focus of this chapter is to turn your existing, informal knowledge of functions into a rigorous understanding of what functions are and how they work. You’ll see some interesting tricks and techniques in this chapter, but most of what you’ll learn will be more important as the building blocks for more advanced techniques. The most important thing to understand about R is that functions are objects in their own right. You can work with them exactly the same way you work with any other type of object. This theme will be explored in depth in functional programming. 4.0.0.0.1 Quiz Answer the following questions to see if you can safely skip this chapter. You can find the answers at the end of the chapter in answers. What are the three components of a function? What does the following code return? x &lt;- 10 f1 &lt;- function(x) { function() { x + 10 } } f1(1)() How would you more typically write this code? `+`(1, `*`(2, 3)) How could you make this call easier to read? mean(, TRUE, x = c(1:10, NA)) Does the following function throw an error when called? Why/why not? f2 &lt;- function(a, b) { a * 10 } f2(10, stop(&quot;This is an error!&quot;)) What is an infix function? How do you write it? What’s a replacement function? How do you write it? What function do you use to ensure that a cleanup action occurs regardless of how a function terminates? 4.0.0.0.2 Outline Function components describes the three main components of a function. Lexical scoping teaches you how R finds values from names, the process of lexical scoping. Every operation is a function call shows you that everything that happens in R is a result of a function call, even if it doesn’t look like it. Function arguments discusses the three ways of supplying arguments to a function, how to call a function given a list of arguments, and the impact of lazy evaluation. Special calls describes two special types of function: infix and replacement functions. Return values discusses how and when functions return values, and how you can ensure that a function does something before it exits. 4.0.0.0.3 Prerequisites The only package you’ll need is pryr, which is used to explore what happens when modifying vectors in place. Install it with install.packages(&quot;pryr&quot;). 4.1 Function components All R functions have three parts: the body(), the code inside the function. the formals(), the list of arguments which controls how you can call the function. the environment(), the “map” of the location of the function’s variables. When you print a function in R, it shows you these three important components. If the environment isn’t displayed, it means that the function was created in the global environment. f &lt;- function(x) x^2 f #&gt; function(x) x^2 formals(f) #&gt; $x body(f) #&gt; x^2 environment(f) #&gt; &lt;environment: R_GlobalEnv&gt; The assignment forms of body(), formals(), and environment() can also be used to modify functions. Like all objects in R, functions can also possess any number of additional attributes(). One attribute used by base R is “srcref”, short for source reference, which points to the source code used to create the function. Unlike body(), this contains code comments and other formatting. You can also add attributes to a function. For example, you can set the class() and add a custom print() method. 4.1.1 Primitive functions There is one exception to the rule that functions have three components. Primitive functions, like sum(), call C code directly with .Primitive() and contain no R code. Therefore their formals(), body(), and environment() are all NULL: sum #&gt; function (..., na.rm = FALSE) .Primitive(&quot;sum&quot;) formals(sum) #&gt; NULL body(sum) #&gt; NULL environment(sum) #&gt; NULL Primitive functions are only found in the base package, and since they operate at a low level, they can be more efficient (primitive replacement functions don’t have to make copies), and can have different rules for argument matching (e.g., switch and call). This, however, comes at a cost of behaving differently from all other functions in R. Hence the R core team generally avoids creating them unless there is no other option. 4.1.2 Exercises What function allows you to tell if an object is a function? What function allows you to tell if a function is a primitive function? This code makes a list of all functions in the base package. objs &lt;- mget(ls(&quot;package:base&quot;), inherits = TRUE) funs &lt;- Filter(is.function, objs) Use it to answer the following questions: Which base function has the most arguments? How many base functions have no arguments? What’s special about those functions? How could you adapt the code to find all primitive functions? What are the three important components of a function? When does printing a function not show what environment it was created in? 4.2 Lexical scoping Scoping is the set of rules that govern how R looks up the value of a symbol. In the example below, scoping is the set of rules that R applies to go from the symbol x to its value 10: x &lt;- 10 x #&gt; [1] 10 Understanding scoping allows you to: build tools by composing functions, as described in functional programming. overrule the usual evaluation rules and do non-standard evaluation, as described in non-standard evaluation. R has two types of scoping: lexical scoping, implemented automatically at the language level, and dynamic scoping, used in select functions to save typing during interactive analysis. We discuss lexical scoping here because it is intimately tied to function creation. Dynamic scoping is described in more detail in scoping issues. Lexical scoping looks up symbol values based on how functions were nested when they were created, not how they are nested when they are called. With lexical scoping, you don’t need to know how the function is called to figure out where the value of a variable will be looked up. You just need to look at the function’s definition. The “lexical” in lexical scoping doesn’t correspond to the usual English definition (“of or relating to words or the vocabulary of a language as distinguished from its grammar and construction”) but comes from the computer science term “lexing”, which is part of the process that converts code represented as text to meaningful pieces that the programming language understands. There are four basic principles behind R’s implementation of lexical scoping: name masking functions vs. variables a fresh start dynamic lookup You probably know many of these principles already, although you might not have thought about them explicitly. Test your knowledge by mentally running through the code in each block before looking at the answers. 4.2.1 Name masking The following example illustrates the most basic principle of lexical scoping, and you should have no problem predicting the output. f &lt;- function() { x &lt;- 1 y &lt;- 2 c(x, y) } f() rm(f) If a name isn’t defined inside a function, R will look one level up. x &lt;- 2 g &lt;- function() { y &lt;- 1 c(x, y) } g() rm(x, g) The same rules apply if a function is defined inside another function: look inside the current function, then where that function was defined, and so on, all the way up to the global environment, and then on to other loaded packages. Run the following code in your head, then confirm the output by running the R code. x &lt;- 1 h &lt;- function() { y &lt;- 2 i &lt;- function() { z &lt;- 3 c(x, y, z) } i() } h() rm(x, h) The same rules apply to closures, functions created by other functions. Closures will be described in more detail in functional programming; here we’ll just look at how they interact with scoping. The following function, j(), returns a function. What do you think this function will return when we call it? j &lt;- function(x) { y &lt;- 2 function() { c(x, y) } } k &lt;- j(1) k() rm(j, k) This seems a little magical (how does R know what the value of y is after the function has been called). It works because k preserves the environment in which it was defined and because the environment includes the value of y. Environments gives some pointers on how you can dive in and figure out what values are stored in the environment associated with each function. 4.2.2 Functions vs. variables The same principles apply regardless of the type of associated value — finding functions works exactly the same way as finding variables: l &lt;- function(x) x + 1 m &lt;- function() { l &lt;- function(x) x * 2 l(10) } m() #&gt; [1] 20 rm(l, m) For functions, there is one small tweak to the rule. If you are using a name in a context where it’s obvious that you want a function (e.g., f(3)), R will ignore objects that are not functions while it is searching. In the following example n takes on a different value depending on whether R is looking for a function or a variable. n &lt;- function(x) x / 2 o &lt;- function() { n &lt;- 10 n(n) } o() #&gt; [1] 5 rm(n, o) However, using the same name for functions and other objects will make for confusing code, and is generally best avoided. 4.2.3 A fresh start What happens to the values in between invocations of a function? What will happen the first time you run this function? What will happen the second time? (If you haven’t seen exists() before: it returns TRUE if there’s a variable of that name, otherwise it returns FALSE.) j &lt;- function() { if (!exists(&quot;a&quot;)) { a &lt;- 1 } else { a &lt;- a + 1 } a } j() rm(j) You might be surprised that it returns the same value, 1, every time. This is because every time a function is called, a new environment is created to host execution. A function has no way to tell what happened the last time it was run; each invocation is completely independent. (We’ll see some ways to get around this in mutable state.) 4.2.4 Dynamic lookup Lexical scoping determines where to look for values, not when to look for them. R looks for values when the function is run, not when it’s created. This means that the output of a function can be different depending on objects outside its environment: f &lt;- function() x x &lt;- 15 f() #&gt; [1] 15 x &lt;- 20 f() #&gt; [1] 20 You generally want to avoid this behaviour because it means the function is no longer self-contained. This is a common error — if you make a spelling mistake in your code, you won’t get an error when you create the function, and you might not even get one when you run the function, depending on what variables are defined in the global environment. One way to detect this problem is the findGlobals() function from codetools. This function lists all the external dependencies of a function: f &lt;- function() x + 1 codetools::findGlobals(f) #&gt; [1] &quot;+&quot; &quot;x&quot; Another way to try and solve the problem would be to manually change the environment of the function to the emptyenv(), an environment which contains absolutely nothing: environment(f) &lt;- emptyenv() f() #&gt; Error in x + 1: konnte Funktion &quot;+&quot; nicht finden This doesn’t work because R relies on lexical scoping to find everything, even the + operator. It’s never possible to make a function completely self-contained because you must always rely on functions defined in base R or other packages. You can use this same idea to do other things that are extremely ill-advised. For example, since all of the standard operators in R are functions, you can override them with your own alternatives. If you ever are feeling particularly evil, run the following code while your friend is away from their computer: `(` &lt;- function(e1) { if (is.numeric(e1) &amp;&amp; runif(1) &lt; 0.1) { e1 + 1 } else { e1 } } replicate(50, (1 + 2)) #&gt; [1] 4 3 3 3 4 3 3 3 3 3 3 3 4 3 3 3 3 4 3 3 3 3 3 3 3 3 4 3 3 3 4 3 3 3 3 #&gt; [36] 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 rm(&quot;(&quot;) This will introduce a particularly pernicious bug: 10% of the time, 1 will be added to any numeric calculation inside parentheses. This is another good reason to regularly restart with a clean R session! 4.2.5 Exercises What does the following code return? Why? What does each of the three c’s mean? c &lt;- 10 c(c = c) What are the four principles that govern how R looks for values? What does the following function return? Make a prediction before running the code yourself. f &lt;- function(x) { f &lt;- function(x) { f &lt;- function(x) { x ^ 2 } f(x) + 1 } f(x) * 2 } f(10) 4.3 Every operation is a function call “To understand computations in R, two slogans are helpful: Everything that exists is an object. Everything that happens is a function call.&quot; — John Chambers The previous example of redefining ( works because every operation in R is a function call, whether or not it looks like one. This includes infix operators like +, control flow operators like for, if, and while, subsetting operators like [] and $, and even the curly brace {. This means that each pair of statements in the following example is exactly equivalent. Note that `, the backtick, lets you refer to functions or variables that have otherwise reserved or illegal names: x &lt;- 10; y &lt;- 5 x + y #&gt; [1] 15 `+`(x, y) #&gt; [1] 15 for (i in 1:2) print(i) #&gt; [1] 1 #&gt; [1] 2 `for`(i, 1:2, print(i)) #&gt; [1] 1 #&gt; [1] 2 if (i == 1) print(&quot;yes!&quot;) else print(&quot;no.&quot;) #&gt; [1] &quot;no.&quot; `if`(i == 1, print(&quot;yes!&quot;), print(&quot;no.&quot;)) #&gt; [1] &quot;no.&quot; x[3] #&gt; [1] NA `[`(x, 3) #&gt; [1] NA { print(1); print(2); print(3) } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 `{`(print(1), print(2), print(3)) #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 It is possible to override the definitions of these special functions, but this is almost certainly a bad idea. However, there are occasions when it might be useful: it allows you to do something that would have otherwise been impossible. For example, this feature makes it possible for the dplyr package to translate R expressions into SQL expressions. Domain specific languages uses this idea to create domain specific languages that allow you to concisely express new concepts using existing R constructs. It’s more often useful to treat special functions as ordinary functions. For example, we could use sapply() to add 3 to every element of a list by first defining a function add(), like this: add &lt;- function(x, y) x + y sapply(1:10, add, 3) #&gt; [1] 4 5 6 7 8 9 10 11 12 13 But we can also get the same effect using the built-in + function. sapply(1:5, `+`, 3) #&gt; [1] 4 5 6 7 8 sapply(1:5, &quot;+&quot;, 3) #&gt; [1] 4 5 6 7 8 Note the difference between `+` and &quot;+&quot;. The first one is the value of the object called +, and the second is a string containing the character +. The second version works because sapply can be given the name of a function instead of the function itself: if you read the source of sapply(), you’ll see the first line uses match.fun() to find functions given their names. A more useful application is to combine lapply() or sapply() with subsetting: x &lt;- list(1:3, 4:9, 10:12) sapply(x, &quot;[&quot;, 2) #&gt; [1] 2 5 11 # equivalent to sapply(x, function(x) x[2]) #&gt; [1] 2 5 11 Remembering that everything that happens in R is a function call will help you in metaprogramming. 4.4 Function arguments It’s useful to distinguish between the formal arguments and the actual arguments of a function. The formal arguments are a property of the function, whereas the actual or calling arguments can vary each time you call the function. This section discusses how calling arguments are mapped to formal arguments, how you can call a function given a list of arguments, how default arguments work, and the impact of lazy evaluation. 4.4.1 Calling functions When calling a function you can specify arguments by position, by complete name, or by partial name. Arguments are matched first by exact name (perfect matching), then by prefix matching, and finally by position. f &lt;- function(abcdef, bcde1, bcde2) { list(a = abcdef, b1 = bcde1, b2 = bcde2) } str(f(1, 2, 3)) #&gt; List of 3 #&gt; $ a : num 1 #&gt; $ b1: num 2 #&gt; $ b2: num 3 str(f(2, 3, abcdef = 1)) #&gt; List of 3 #&gt; $ a : num 1 #&gt; $ b1: num 2 #&gt; $ b2: num 3 # Can abbreviate long argument names: str(f(2, 3, a = 1)) #&gt; List of 3 #&gt; $ a : num 1 #&gt; $ b1: num 2 #&gt; $ b2: num 3 # But this doesn&#39;t work because abbreviation is ambiguous str(f(1, 3, b = 1)) #&gt; Error in f(1, 3, b = 1): Argument 3 passt auf mehrere formale Argumente Generally, you only want to use positional matching for the first one or two arguments; they will be the most commonly used, and most readers will know what they are. Avoid using positional matching for less commonly used arguments, and only use readable abbreviations with partial matching. (If you are writing code for a package that you want to publish on CRAN you can not use partial matching, and must use complete names.) Named arguments should always come after unnamed arguments. If a function uses ... (discussed in more detail below), you can only specify arguments listed after ... with their full name. These are good calls: mean(1:10) mean(1:10, trim = 0.05) This is probably overkill: mean(x = 1:10) And these are just confusing: mean(1:10, n = T) mean(1:10, , FALSE) mean(1:10, 0.05) mean(, TRUE, x = c(1:10, NA)) 4.4.2 Calling a function given a list of arguments Suppose you had a list of function arguments: args &lt;- list(1:10, na.rm = TRUE) How could you then send that list to mean()? You need do.call(): do.call(mean, args) #&gt; [1] 5.5 # Equivalent to mean(1:10, na.rm = TRUE) #&gt; [1] 5.5 4.4.3 Default and missing arguments Function arguments in R can have default values. f &lt;- function(a = 1, b = 2) { c(a, b) } f() #&gt; [1] 1 2 Since arguments in R are evaluated lazily (more on that below), the default value can be defined in terms of other arguments: g &lt;- function(a = 1, b = a * 2) { c(a, b) } g() #&gt; [1] 1 2 g(10) #&gt; [1] 10 20 Default arguments can even be defined in terms of variables created within the function. This is used frequently in base R functions, but I think it is bad practice, because you can’t understand what the default values will be without reading the complete source code. h &lt;- function(a = 1, b = d) { d &lt;- (a + 1) ^ 2 c(a, b) } h() #&gt; [1] 1 4 h(10) #&gt; [1] 10 121 You can determine if an argument was supplied or not with the missing() function. i &lt;- function(a, b) { c(missing(a), missing(b)) } i() #&gt; [1] TRUE TRUE i(a = 1) #&gt; [1] FALSE TRUE i(b = 2) #&gt; [1] TRUE FALSE i(1, 2) #&gt; [1] FALSE FALSE Sometimes you want to add a non-trivial default value, which might take several lines of code to compute. Instead of inserting that code in the function definition, you could use missing() to conditionally compute it if needed. However, this makes it hard to know which arguments are required and which are optional without carefully reading the documentation. Instead, I usually set the default value to NULL and use is.null() to check if the argument was supplied. 4.4.4 Lazy evaluation By default, R function arguments are lazy — they’re only evaluated if they’re actually used: f &lt;- function(x) { 10 } f(stop(&quot;This is an error!&quot;)) #&gt; [1] 10 If you want to ensure that an argument is evaluated you can use force(): f &lt;- function(x) { force(x) 10 } f(stop(&quot;This is an error!&quot;)) #&gt; Error in force(x): This is an error! The apply functions underwent this same change in R 3.2.0: Higher order functions such as the apply functions and Reduce() now force arguments to the functions they apply in order to eliminate undesirable interactions between lazy evaluation and variable capture in closures. So, as of R 3.2.0 (but not older versions), you can safely do: add &lt;- function(x) { function(y) x + y } adders &lt;- lapply(1:10, add) adders[[1]](10) #&gt; [1] 11 adders[[10]](10) #&gt; [1] 20 Fortunately, all good! The lesson here is that you need to keep lazy evaluation in mind when creating closures with a loop or any other construct (unless you know that these, like the apply family, force their functions’ arguments). For example, here’s a naive implementation that wants to achieve the same result as above, using a for loop instead of lapply: add &lt;- function(x) { function(y) x + y } adders &lt;- list() for (i in 1:10) { adders[[i]] &lt;- add(i) } adders[[1]](10) #&gt; [1] 20 adders[[10]](10) #&gt; [1] 20 x is lazily evaluated the first time that you call one of the adder functions. At this point, the loop is complete and the final value of x is 10. Therefore all of the adder functions will add 10 on to their input, probably not what you wanted! Manually forcing evaluation inside add() fixes the problem: add &lt;- function(x) { force(x) function(y) x + y } adders &lt;- list() for (i in 1:10) { adders[[i]] &lt;- add(i) } adders[[1]](10) #&gt; [1] 11 adders[[10]](10) #&gt; [1] 20 The add function is exactly equivalent to add &lt;- function(x) { x function(y) x + y } because the force function is defined as force &lt;- function(x) x. However, using this function clearly indicates that you’re forcing evaluation, not that you’ve accidentally typed x. Default arguments are evaluated inside the function. This means that if the expression depends on the current environment the results will differ depending on whether you use the default value or explicitly provide one. f &lt;- function(x = ls()) { a &lt;- 1 x } # ls() evaluated inside f: f() #&gt; [1] &quot;a&quot; &quot;x&quot; # ls() evaluated in global environment: f(ls()) #&gt; [1] &quot;add&quot; &quot;adders&quot; &quot;args&quot; &quot;begin_sidebar&quot; #&gt; [5] &quot;doc_type&quot; &quot;end_sidebar&quot; &quot;f&quot; &quot;funs&quot; #&gt; [9] &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;objs&quot; #&gt; [13] &quot;x&quot; &quot;y&quot; More technically, an unevaluated argument is called a promise, or (less commonly) a thunk. A promise is made up of two parts: The expression which gives rise to the delayed computation. (It can be accessed with substitute(). See non-standard evaluation for more details.) The environment where the expression was created and where it should be evaluated. The first time a promise is accessed the expression is evaluated in the environment where it was created. This value is cached, so that subsequent access to the evaluated promise does not recompute the value (but the original expression is still associated with the value, so substitute() can continue to access it). You can find more information about a promise using pryr::promise_info(). This uses some C++ code to extract information about the promise without evaluating it, which is impossible to do in pure R code. Laziness is useful in if statements — the second statement below will be evaluated only if the first is true. If it wasn’t, the statement would return an error because NULL &gt; 0 is a logical vector of length 0 and not a valid input to if. x &lt;- NULL if (!is.null(x) &amp;&amp; x &gt; 0) { } We could implement “&amp;&amp;” ourselves: `&amp;&amp;` &lt;- function(x, y) { if (!x) return(FALSE) if (!y) return(FALSE) TRUE } a &lt;- NULL !is.null(a) &amp;&amp; a &gt; 0 #&gt; [1] FALSE This function would not work without lazy evaluation because both x and y would always be evaluated, testing a &gt; 0 even when a was NULL. Sometimes you can also use laziness to eliminate an if statement altogether. For example, instead of: if (is.null(a)) stop(&quot;a is null&quot;) #&gt; Error in eval(expr, envir, enclos): a is null You could write: !is.null(a) || stop(&quot;a is null&quot;) #&gt; Error in eval(expr, envir, enclos): a is null 4.4.5 ... There is a special argument called ... . This argument will match any arguments not otherwise matched, and can be easily passed on to other functions. This is useful if you want to collect arguments to call another function, but you don’t want to prespecify their possible names. ... is often used in conjunction with S3 generic functions to allow individual methods to be more flexible. One relatively sophisticated user of ... is the base plot() function. plot() is a generic method with arguments x, y and ... . To understand what ... does for a given function we need to read the help: “Arguments to be passed to methods, such as graphical parameters”. Most simple invocations of plot() end up calling plot.default() which has many more arguments, but also has ... . Again, reading the documentation reveals that ... accepts “other graphical parameters”, which are listed in the help for par(). This allows us to write code like: plot(1:5, col = &quot;red&quot;) plot(1:5, cex = 5, pch = 20) This illustrates both the advantages and disadvantages of ...: it makes plot() very flexible, but to understand how to use it, we have to carefully read the documentation. Additionally, if we read the source code for plot.default, we can discover undocumented features. It’s possible to pass along other arguments to Axis() and box(): plot(1:5, bty = &quot;u&quot;) plot(1:5, labels = FALSE) To capture ... in a form that is easier to work with, you can use list(...). (See capturing unevaluated dots for other ways to capture ... without evaluating the arguments.) f &lt;- function(...) { names(list(...)) } f(a = 1, b = 2) #&gt; [1] &quot;a&quot; &quot;b&quot; Using ... comes at a price — any misspelled arguments will not raise an error, and any arguments after ... must be fully named. This makes it easy for typos to go unnoticed: sum(1, 2, NA, na.mr = TRUE) #&gt; [1] NA It’s often better to be explicit rather than implicit, so you might instead ask users to supply a list of additional arguments. That’s certainly easier if you’re trying to use ... with multiple additional functions. 4.4.6 Exercises Clarify the following list of odd function calls: x &lt;- sample(replace = TRUE, 20, x = c(1:10, NA)) y &lt;- runif(min = 0, max = 1, 20) cor(m = &quot;k&quot;, y = y, u = &quot;p&quot;, x = x) What does this function return? Why? Which principle does it illustrate? f1 &lt;- function(x = {y &lt;- 1; 2}, y = 0) { x + y } f1() What does this function return? Why? Which principle does it illustrate? f2 &lt;- function(x = z) { z &lt;- 100 x } f2() 4.5 Special calls R supports two additional syntaxes for calling special types of functions: infix and replacement functions. 4.5.1 Infix functions Most functions in R are “prefix” operators: the name of the function comes before the arguments. You can also create infix functions where the function name comes in between its arguments, like + or -. All user-created infix functions must start and end with %. R comes with the following infix functions predefined: %%, %*%, %/%, %in%, %o%, %x%. (The complete list of built-in infix operators that don’t need % is: :, ::, :::, $, @, ^, *, /, +, -, &gt;, &gt;=, &lt;, &lt;=, ==, !=, !, &amp;, &amp;&amp;, |, ||, ~, &lt;-, &lt;&lt;-) For example, we could create a new operator that pastes together strings: `%+%` &lt;- function(a, b) paste0(a, b) &quot;new&quot; %+% &quot; string&quot; #&gt; [1] &quot;new string&quot; Note that when creating the function, you have to put the name in backticks because it’s a special name. This is just a syntactic sugar for an ordinary function call; as far as R is concerned there is no difference between these two expressions: &quot;new&quot; %+% &quot; string&quot; #&gt; [1] &quot;new string&quot; `%+%`(&quot;new&quot;, &quot; string&quot;) #&gt; [1] &quot;new string&quot; Or indeed between 1 + 5 #&gt; [1] 6 `+`(1, 5) #&gt; [1] 6 The names of infix functions are more flexible than regular R functions: they can contain any sequence of characters (except “%”, of course). You will need to escape any special characters in the string used to define the function, but not when you call it: `% %` &lt;- function(a, b) paste(a, b) `%&#39;%` &lt;- function(a, b) paste(a, b) `%/\\\\%` &lt;- function(a, b) paste(a, b) &quot;a&quot; % % &quot;b&quot; #&gt; [1] &quot;a b&quot; &quot;a&quot; %&#39;% &quot;b&quot; #&gt; [1] &quot;a b&quot; &quot;a&quot; %/\\% &quot;b&quot; #&gt; [1] &quot;a b&quot; R’s default precedence rules mean that infix operators are composed from left to right: `%-%` &lt;- function(a, b) paste0(&quot;(&quot;, a, &quot; %-% &quot;, b, &quot;)&quot;) &quot;a&quot; %-% &quot;b&quot; %-% &quot;c&quot; #&gt; [1] &quot;((a %-% b) %-% c)&quot; There’s one infix function that I use very often. It’s inspired by Ruby’s || logical or operator, although it works a little differently in R because Ruby has a more flexible definition of what evaluates to TRUE in an if statement. It’s useful as a way of providing a default value in case the output of another function is NULL: `%||%` &lt;- function(a, b) if (!is.null(a)) a else b function_that_might_return_null() %||% default value 4.5.2 Replacement functions Replacement functions act like they modify their arguments in place, and have the special name xxx&lt;-. They typically have two arguments (x and value), although they can have more, and they must return the modified object. For example, the following function allows you to modify the second element of a vector: `second&lt;-` &lt;- function(x, value) { x[2] &lt;- value x } x &lt;- 1:10 second(x) &lt;- 5L x #&gt; [1] 1 5 3 4 5 6 7 8 9 10 When R evaluates the assignment second(x) &lt;- 5, it notices that the left hand side of the &lt;- is not a simple name, so it looks for a function named second&lt;- to do the replacement. I say they “act” like they modify their arguments in place, because they actually create a modified copy. We can see that by using pryr::address() to find the memory address of the underlying object. library(pryr) x &lt;- 1:10 address(x) #&gt; [1] &quot;0x7f7f4a053a20&quot; second(x) &lt;- 6L address(x) #&gt; [1] &quot;0x7f7f4a0cbfb8&quot; Built-in functions that are implemented using .Primitive() will modify in place: x &lt;- 1:10 address(x) #&gt; [1] &quot;0x103945110&quot; x[2] &lt;- 7L address(x) #&gt; [1] &quot;0x103945110&quot; It’s important to be aware of this behaviour since it has important performance implications. If you want to supply additional arguments, they go in between x and value: `modify&lt;-` &lt;- function(x, position, value) { x[position] &lt;- value x } modify(x, 1) &lt;- 10 x #&gt; [1] 10 7 3 4 5 6 7 8 9 10 When you call modify(x, 1) &lt;- 10, behind the scenes R turns it into: x &lt;- `modify&lt;-`(x, 1, 10) This means you can’t do things like: modify(get(&quot;x&quot;), 1) &lt;- 10 because that gets turned into the invalid code: get(&quot;x&quot;) &lt;- `modify&lt;-`(get(&quot;x&quot;), 1, 10) It’s often useful to combine replacement and subsetting: x &lt;- c(a = 1, b = 2, c = 3) names(x) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; names(x)[2] &lt;- &quot;two&quot; names(x) #&gt; [1] &quot;a&quot; &quot;two&quot; &quot;c&quot; This works because the expression names(x)[2] &lt;- &quot;two&quot; is evaluated as if you had written: `*tmp*` &lt;- names(x) `*tmp*`[2] &lt;- &quot;two&quot; names(x) &lt;- `*tmp*` (Yes, it really does create a local variable named *tmp*, which is removed afterwards.) 4.5.3 Exercises Create a list of all the replacement functions found in the base package. Which ones are primitive functions? What are valid names for user-created infix functions? Create an infix xor() operator. Create infix versions of the set functions intersect(), union(), and setdiff(). Create a replacement function that modifies a random location in a vector. 4.6 Return values The last expression evaluated in a function becomes the return value, the result of invoking the function. f &lt;- function(x) { if (x &lt; 10) { 0 } else { 10 } } f(5) #&gt; [1] 0 f(15) #&gt; [1] 10 Generally, I think it’s good style to reserve the use of an explicit return() for when you are returning early, such as for an error, or a simple case of the function. This style of programming can also reduce the level of indentation, and generally make functions easier to understand because you can reason about them locally. f &lt;- function(x, y) { if (!x) return(y) # complicated processing here } Functions can return only a single object. But this is not a limitation because you can return a list containing any number of objects. The functions that are the easiest to understand and reason about are pure functions: functions that always map the same input to the same output and have no other impact on the workspace. In other words, pure functions have no side effects: they don’t affect the state of the world in any way apart from the value they return. R protects you from one type of side effect: most R objects have copy-on-modify semantics. So modifying a function argument does not change the original value: f &lt;- function(x) { x$a &lt;- 2 x } x &lt;- list(a = 1) f(x) #&gt; $a #&gt; [1] 2 x$a #&gt; [1] 1 (There are two important exceptions to the copy-on-modify rule: environments and reference classes. These can be modified in place, so extra care is needed when working with them.) This is notably different to languages like Java where you can modify the inputs of a function. This copy-on-modify behaviour has important performance consequences which are discussed in depth in profiling. (Note that the performance consequences are a result of R’s implementation of copy-on-modify semantics; they are not true in general. Clojure is a new language that makes extensive use of copy-on-modify semantics with limited performance consequences.) Most base R functions are pure, with a few notable exceptions: library() which loads a package, and hence modifies the search path. setwd(), Sys.setenv(), Sys.setlocale() which change the working directory, environment variables, and the locale, respectively. plot() and friends which produce graphical output. write(), write.csv(), saveRDS(), etc. which save output to disk. options() and par() which modify global settings. S4 related functions which modify global tables of classes and methods. Random number generators which produce different numbers each time you run them. It’s generally a good idea to minimise the use of side effects, and where possible, to minimise the footprint of side effects by separating pure from impure functions. Pure functions are easier to test (because all you need to worry about are the input values and the output), and are less likely to work differently on different versions of R or on different platforms. For example, this is one of the motivating principles of ggplot2: most operations work on an object that represents a plot, and only the final print or plot call has the side effect of actually drawing the plot. Functions can return invisible values, which are not printed out by default when you call the function. f1 &lt;- function() 1 f2 &lt;- function() invisible(1) f1() #&gt; [1] 1 f2() f1() == 1 #&gt; [1] TRUE f2() == 1 #&gt; [1] TRUE You can force an invisible value to be displayed by wrapping it in parentheses: (f2()) #&gt; [1] 1 The most common function that returns invisibly is &lt;-: a &lt;- 2 (a &lt;- 2) #&gt; [1] 2 This is what makes it possible to assign one value to multiple variables: a &lt;- b &lt;- c &lt;- d &lt;- 2 because this is parsed as: (a &lt;- (b &lt;- (c &lt;- (d &lt;- 2)))) #&gt; [1] 2 4.6.1 On exit As well as returning a value, functions can set up other triggers to occur when the function is finished using on.exit(). This is often used as a way to guarantee that changes to the global state are restored when the function exits. The code in on.exit() is run regardless of how the function exits, whether with an explicit (early) return, an error, or simply reaching the end of the function body. in_dir &lt;- function(dir, code) { old &lt;- setwd(dir) on.exit(setwd(old)) force(code) } getwd() #&gt; [1] &quot;/Users/stephanmichalik/Desktop/RStudio_Rconf/adv-r-book&quot; in_dir(&quot;~&quot;, getwd()) #&gt; [1] &quot;/Users/stephanmichalik&quot; The basic pattern is simple: We first set the directory to a new location, capturing the current location from the output of setwd(). We then use on.exit() to ensure that the working directory is returned to the previous value regardless of how the function exits. Finally, we explicitly force evaluation of the code. (We don’t actually need force() here, but it makes it clear to readers what we’re doing.) Caution: If you’re using multiple on.exit() calls within a function, make sure to set add = TRUE. Unfortunately, the default in on.exit() is add = FALSE, so that every time you run it, it overwrites existing exit expressions. Because of the way on.exit() is implemented, it’s not possible to create a variant with add = TRUE, so you must be careful when using it. 4.6.2 Exercises How does the chdir parameter of source() compare to in_dir()? Why might you prefer one approach to the other? What function undoes the action of library()? How do you save and restore the values of options() and par()? Write a function that opens a graphics device, runs the supplied code, and closes the graphics device (always, regardless of whether or not the plotting code worked). We can use on.exit() to implement a simple version of capture.output(). capture.output2 &lt;- function(code) { temp &lt;- tempfile() on.exit(file.remove(temp), add = TRUE) sink(temp) on.exit(sink(), add = TRUE) force(code) readLines(temp) } capture.output2(cat(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, sep = &quot;\\n&quot;)) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; Compare capture.output() to capture.output2(). How do the functions differ? What features have I removed to make the key ideas easier to see? How have I rewritten the key ideas to be easier to understand? 4.7 Quiz answers The three components of a function are its body, arguments, and environment. f1(1)() returns 11. You’d normally write it in infix style: 1 + (2 * 3). Rewriting the call to mean(c(1:10, NA), na.rm = TRUE) is easier to understand. No, it does not throw an error because the second argument is never used so it’s never evaluated. See infix and replacement functions. You use on.exit(); see on exit for details. "],
["memory.html", "5 Memory 5.1 Object size 5.2 Memory usage and garbage collection 5.3 Memory profiling with lineprof 5.4 Modification in place", " 5 Memory A solid understanding of R’s memory management will help you predict how much memory you’ll need for a given task and help you to make the most of the memory you have. It can even help you write faster code because accidental copies are a major cause of slow code. The goal of this chapter is to help you understand the basics of memory management in R, moving from individual objects to functions to larger blocks of code. Along the way, you’ll learn about some common myths, such as that you need to call gc() to free up memory, or that for loops are always slow. 5.0.0.0.1 Outline Object size shows you how to use object_size() to see how much memory an object occupies, and uses that as a launching point to improve your understanding of how R objects are stored in memory. Memory usage and garbage collection introduces you to the mem_used() and mem_change() functions that will help you understand how R allocates and frees memory. Memory profiling with lineprof shows you how to use the lineprof package to understand how memory is allocated and released in larger code blocks. Modification in place introduces you to the address() and refs() functions so that you can understand when R modifies in place and when R modifies a copy. Understanding when objects are copied is very important for writing efficient R code. 5.0.0.0.2 Prerequisites In this chapter, we’ll use tools from the pryr and lineprof packages to understand memory usage, and a sample dataset from ggplot2. If you don’t already have them, run this code to get the packages you need: install.packages(&quot;ggplot2&quot;) install.packages(&quot;pryr&quot;) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;hadley/lineprof&quot;) 5.0.0.0.3 Sources The details of R’s memory management are not documented in a single place. Most of the information in this chapter was gleaned from a close reading of the documentation (particularly ?Memory and ?gc), the memory profiling section of R-exts, and the SEXPs section of R-ints. The rest I figured out by reading the C source code, performing small experiments, and asking questions on R-devel. Any mistakes are entirely mine. 5.1 Object size To understand memory usage in R, we will start with pryr::object_size(). This function tells you how many bytes of memory an object occupies: library(pryr) object_size(1:10) #&gt; 88 B object_size(mean) #&gt; 1.13 kB object_size(mtcars) #&gt; 6.74 kB (This function is better than the built-in object.size() because it accounts for shared elements within an object and includes the size of environments.) Something interesting occurs if we use object_size() to systematically explore the size of an integer vector. The code below computes and plots the memory usage of integer vectors ranging in length from 0 to 50 elements. You might expect that the size of an empty vector would be zero and that memory usage would grow proportionately with length. Neither of those things are true! sizes &lt;- sapply(0:50, function(n) object_size(seq_len(n))) plot(0:50, sizes, xlab = &quot;Length&quot;, ylab = &quot;Size (bytes)&quot;, type = &quot;s&quot;) This isn’t just an artefact of integer vectors. Every length 0 vector occupies 40 bytes of memory: object_size(numeric()) #&gt; 40 B object_size(logical()) #&gt; 40 B object_size(raw()) #&gt; 40 B object_size(list()) #&gt; 40 B Those 40 bytes are used to store four components possessed by every object in R: Object metadata (4 bytes). These metadata store the base type (e.g. integer) and information used for debugging and memory management. Two pointers: one to the next object in memory and one to the previous object (2 * 8 bytes). This doubly-linked list makes it easy for internal R code to loop through every object in memory. A pointer to the attributes (8 bytes). All vectors have three additional components: The length of the vector (4 bytes). By using only 4 bytes, you might expect that R could only support vectors up to \\(2 ^ {4 \\times 8 - 1}\\) (\\(2 ^ {31}\\), about two billion) elements. But in R 3.0.0 and later, you can actually have vectors up to \\(2 ^ {52}\\) elements. Read R-internals to see how support for long vectors was added without having to change the size of this field. The “true” length of the vector (4 bytes). This is basically never used, except when the object is the hash table used for an environment. In that case, the true length represents the allocated space, and the length represents the space currently used. The data (?? bytes). An empty vector has 0 bytes of data. Numeric vectors occupy 8 bytes for every element, integer vectors 4, and complex vectors 16. If you’re keeping count you’ll notice that this only adds up to 36 bytes. The remaining 4 bytes are used for padding so that each component starts on an 8 byte (= 64-bit) boundary. Most cpu architectures require pointers to be aligned in this way, and even if they don’t require it, accessing non-aligned pointers tends to be rather slow. (If you’re interested, you can read more about it in C structure packing.) This explains the intercept on the graph. But why does the memory size grow irregularly? To understand why, you need to know a little bit about how R requests memory from the operating system. Requesting memory (with malloc()) is a relatively expensive operation. Having to request memory every time a small vector is created would slow R down considerably. Instead, R asks for a big block of memory and then manages that block itself. This block is called the small vector pool and is used for vectors less than 128 bytes long. For efficiency and simplicity, it only allocates vectors that are 8, 16, 32, 48, 64, or 128 bytes long. If we adjust our previous plot to remove the 40 bytes of overhead, we can see that those values correspond to the jumps in memory use. plot(0:50, sizes - 40, xlab = &quot;Length&quot;, ylab = &quot;Bytes excluding overhead&quot;, type = &quot;n&quot;) abline(h = 0, col = &quot;grey80&quot;) abline(h = c(8, 16, 32, 48, 64, 128), col = &quot;grey80&quot;) abline(a = 0, b = 4, col = &quot;grey90&quot;, lwd = 4) lines(sizes - 40, type = &quot;s&quot;) Beyond 128 bytes, it no longer makes sense for R to manage vectors. After all, allocating big chunks of memory is something that operating systems are very good at. Beyond 128 bytes, R will ask for memory in multiples of 8 bytes. This ensures good alignment. A subtlety of the size of an object is that components can be shared across multiple objects. For example, look at the following code: x &lt;- 1:1e6 object_size(x) #&gt; 4 MB y &lt;- list(x, x, x) object_size(y) #&gt; 4 MB y isn’t three times as big as x because R is smart enough to not copy x three times; instead it just points to the existing x. It’s misleading to look at the sizes of x and y individually. If you want to know how much space they take up together, you have to supply them to the same object_size() call: object_size(x, y) #&gt; 4 MB In this case, x and y together take up the same amount of space as y alone. This is not always the case. If there are no shared components, as in the following example, then you can add up the sizes of individual components to find out the total size: x1 &lt;- 1:1e6 y1 &lt;- list(1:1e6, 1:1e6, 1:1e6) object_size(x1) #&gt; 4 MB object_size(y1) #&gt; 12 MB object_size(x1, y1) #&gt; 16 MB object_size(x1) + object_size(y1) == object_size(x1, y1) #&gt; [1] TRUE The same issue also comes up with strings, because R has a global string pool. This means that each unique string is only stored in one place, and therefore character vectors take up less memory than you might expect: object_size(&quot;banana&quot;) #&gt; 96 B object_size(rep(&quot;banana&quot;, 10)) #&gt; 216 B 5.1.1 Exercises Repeat the analysis above for numeric, logical, and complex vectors. If a data frame has one million rows, and three variables (two numeric, and one integer), how much space will it take up? Work it out from theory, then verify your work by creating a data frame and measuring its size. Compare the sizes of the elements in the following two lists. Each contains basically the same data, but one contains vectors of small strings while the other contains a single long string. vec &lt;- lapply(0:50, function(i) c(&quot;ba&quot;, rep(&quot;na&quot;, i))) str &lt;- lapply(vec, paste0, collapse = &quot;&quot;) Which takes up more memory: a factor (x) or the equivalent character vector (as.character(x))? Why? Explain the difference in size between 1:5 and list(1:5). 5.2 Memory usage and garbage collection While object_size() tells you the size of a single object, pryr::mem_used() tells you the total size of all objects in memory: library(pryr) mem_used() #&gt; 57.8 MB This number won’t agree with the amount of memory reported by your operating system for a number of reasons: It only includes objects created by R, not the R interpreter itself. Both R and the operating system are lazy: they won’t reclaim memory until it’s actually needed. R might be holding on to memory because the OS hasn’t yet asked for it back. R counts the memory occupied by objects but there may be gaps due to deleted objects. This problem is known as memory fragmentation. mem_change() builds on top of mem_used() to tell you how memory changes during code execution. Positive numbers represent an increase in the memory used by R, and negative numbers represent a decrease. # Need about 4 mb to store 1 million integers mem_change(x &lt;- 1:1e6) #&gt; 4 MB # We get that memory back when we delete it mem_change(rm(x)) #&gt; -4 MB Even operations that don’t do anything use up a little memory. This is because R is tracking the history of everything you do. You can ignore anything smaller than a couple kB. mem_change(NULL) #&gt; -592 B mem_change(NULL) #&gt; 656 B In some languages, you have to explicitly delete unused objects for their memory to be returned. R uses an alternative approach: garbage collection (or GC for short). GC automatically releases memory when an object is no longer used. It does this by tracking how many names point to each object, and when there are no names pointing to an object, it deletes that object. # Create a big object mem_change(x &lt;- 1:1e6) #&gt; 4 MB # Also point to 1:1e6 from y mem_change(y &lt;- x) #&gt; 712 B # Remove x, no memory freed because y is still pointing to it mem_change(rm(x)) #&gt; 600 B # Now nothing points to it and the memory can be freed mem_change(rm(y)) #&gt; -4 MB Despite what you might have read elsewhere, there’s never any need to call gc() yourself. R will automatically run garbage collection whenever it needs more space; if you want to see when that is, call gcinfo(TRUE). The only reason you might want to call gc() is to ask R to return memory to the operating system. However, even that might not have any effect: older versions of Windows had no way for a program to return memory to the OS. GC takes care of releasing objects that are no longer used. However, you do need to be aware of possible memory leaks. A memory leak occurs when you keep pointing to an object without realising it. In R, the two main causes of memory leaks are formulas and closures because they both capture the enclosing environment. The following code illustrates the problem. In f1(), 1:1e6 is only referenced inside the function, so when the function completes the memory is returned and the net memory change is 0. f2() and f3() both return objects that capture environments, so that x is not freed when the function completes. f1 &lt;- function() { x &lt;- 1:1e6 10 } mem_change(x &lt;- f1()) #&gt; -688 B object_size(x) #&gt; 48 B f2 &lt;- function() { x &lt;- 1:1e6 a ~ b } mem_change(y &lt;- f2()) #&gt; 4 MB object_size(y) #&gt; 4 MB f3 &lt;- function() { x &lt;- 1:1e6 function() 10 } mem_change(z &lt;- f3()) #&gt; 4 MB object_size(z) #&gt; 4.01 MB 5.3 Memory profiling with lineprof mem_change() captures the net change in memory when running a block of code. Sometimes, however, we may want to measure incremental change. One way to do this is to use memory profiling to capture usage every few milliseconds. This functionality is provided by utils::Rprof() but it doesn’t provide a very useful display of the results. Instead we’ll use the lineprof package. It is powered by Rprof(), but displays the results in a more informative manner. To demonstrate lineprof, we’re going to explore a bare-bones implementation of read.delim() with only three arguments: read_delim &lt;- function(file, header = TRUE, sep = &quot;,&quot;) { # Determine number of fields by reading first line first &lt;- scan(file, what = character(1), nlines = 1, sep = sep, quiet = TRUE) p &lt;- length(first) # Load all fields as character vectors all &lt;- scan(file, what = as.list(rep(&quot;character&quot;, p)), sep = sep, skip = if (header) 1 else 0, quiet = TRUE) # Convert from strings to appropriate types (never to factors) all[] &lt;- lapply(all, type.convert, as.is = TRUE) # Set column names if (header) { names(all) &lt;- first } else { names(all) &lt;- paste0(&quot;V&quot;, seq_along(all)) } # Convert list into data frame as.data.frame(all) } We’ll also create a sample csv file: library(ggplot2) write.csv(diamonds, &quot;diamonds.csv&quot;, row.names = FALSE) Using lineprof is straightforward. source() the code, apply lineprof() to an expression, then use shine() to view the results. Note that you must use source() to load the code. This is because lineprof uses srcrefs to match up the code and run times. The needed srcrefs are only created when you load code from disk. library(lineprof) source(&quot;code/read-delim.R&quot;) prof &lt;- lineprof(read_delim(&quot;diamonds.csv&quot;)) shine(prof) shine() will also open a new web page (or if you’re using RStudio, a new pane) that shows your source code annotated with information about memory usage. shine() starts a shiny app which will “block” your R session. To exit, press escape or ctrl + break. Next to the source code, four columns provide details about the performance of the code: t, the time (in seconds) spent on that line of code (explained in measuring performance). a, the memory (in megabytes) allocated by that line of code. r, the memory (in megabytes) released by that line of code. While memory allocation is deterministic, memory release is stochastic: it depends on when the GC was run. This means that memory release only tells you that the memory released was no longer needed before this line. d, the number of vector duplications that occurred. A vector duplication occurs when R copies a vector as a result of its copy on modify semantics. You can hover over any of the bars to get the exact numbers. In this example, looking at the allocations tells us most of the story: scan() allocates about 2.5 MB of memory, which is very close to the 2.8 MB of space that the file occupies on disk. You wouldn’t expect the two numbers to be identical because R doesn’t need to store the commas and because the global string pool will save some memory. Converting the columns allocates another 0.6 MB of memory. You’d also expect this step to free some memory because we’ve converted string columns into integer and numeric columns (which occupy less space), but we can’t see those releases because GC hasn’t been triggered yet. Finally, calling as.data.frame() on a list allocates about 1.6 megabytes of memory and performs over 600 duplications. This is because as.data.frame() isn’t terribly efficient and ends up copying the input multiple times. We’ll discuss duplication more in the next section. There are two downsides to profiling: read_delim() only takes around half a second, but profiling can, at best, capture memory usage every 1 ms. This means we’ll only get about 500 samples. Since GC is lazy, we can never tell exactly when memory is no longer needed. You can work around both problems by using torture = TRUE, which forces R to run GC after every allocation (see gctorture() for more details). This helps with both problems because memory is freed as soon as possible, and R runs 10–100x slower. This effectively makes the resolution of the timer greater, so that you can see smaller allocations and exactly when memory is no longer needed. 5.3.1 Exercises When the input is a list, we can make a more efficient as.data.frame() by using special knowledge. A data frame is a list with class data.frame and row.names attribute. row.names is either a character vector or vector of sequential integers, stored in a special format created by .set_row_names(). This leads to an alternative as.data.frame(): to_df &lt;- function(x) { class(x) &lt;- &quot;data.frame&quot; attr(x, &quot;row.names&quot;) &lt;- .set_row_names(length(x[[1]])) x } What impact does this function have on read_delim()? What are the downsides of this function? Line profile the following function with torture = TRUE. What is surprising? Read the source code of rm() to figure out what’s going on. f &lt;- function(n = 1e5) { x &lt;- rep(1, n) rm(x) } 5.4 Modification in place What happens to x in the following code? x &lt;- 1:10 x[5] &lt;- 10L x #&gt; [1] 1 2 3 4 10 6 7 8 9 10 There are two possibilities: R modifies x in place. R makes a copy of x to a new location, modifies the copy, and then uses the name x to point to the new location. It turns out that R can do either depending on the circumstances. In the example above, it will modify in place. But if another variable also points to x, then R will copy it to a new location. To explore what’s going on in greater detail, we use two tools from the pryr package. Given the name of a variable, address() will tell us the variable’s location in memory and refs() will tell us how many names point to that location. library(pryr) x &lt;- 1:10 c(address(x), refs(x)) # [1] &quot;0x103100060&quot; &quot;1&quot; y &lt;- x c(address(y), refs(y)) # [1] &quot;0x103100060&quot; &quot;2&quot; (Note that if you’re using RStudio, refs() will always return 2: the environment browser makes a reference to every object you create on the command line.) refs() is only an estimate. It can only distinguish between one and more than one reference (future versions of R might do better). This means that refs() returns 2 in both of the following cases: x &lt;- 1:5 y &lt;- x rm(y) # Should really be 1, because we&#39;ve deleted y refs(x) #&gt; [1] 2 x &lt;- 1:5 y &lt;- x z &lt;- x # Should really be 3 refs(x) #&gt; [1] 2 When refs(x) is 1, modification will occur in place. When refs(x) is 2, R will make a copy (this ensures that other pointers to the object remain unaffected). Note that in the following example, y keeps pointing to the same location while x changes. x &lt;- 1:10 y &lt;- x c(address(x), address(y)) #&gt; [1] &quot;0x7fd9bf09d7b8&quot; &quot;0x7fd9bf09d7b8&quot; x[5] &lt;- 6L c(address(x), address(y)) #&gt; [1] &quot;0x7fd9bc80be20&quot; &quot;0x7fd9bf09d7b8&quot; Another useful function is tracemem(). It prints a message every time the traced object is copied: x &lt;- 1:10 # Prints the current memory location of the object tracemem(x) # [1] &quot;&lt;0x7feeaaa1c6b8&gt;&quot; x[5] &lt;- 6L y &lt;- x # Prints where it has moved from and to x[5] &lt;- 6L # tracemem[0x7feeaaa1c6b8 -&gt; 0x7feeaaa1c768]: For interactive use, tracemem() is slightly more useful than refs(), but because it just prints a message, it’s harder to program with. I don’t use it in this book because it interacts poorly with knitr, the tool I use to interleave text and code. Non-primitive functions that touch the object always increment the ref count. Primitive functions usually don’t. (The reasons are a little complicated, but see the R-devel thread confused about NAMED.) # Touching the object forces an increment f &lt;- function(x) x {x &lt;- 1:10; f(x); refs(x)} #&gt; [1] 2 # Sum is primitive, so no increment {x &lt;- 1:10; sum(x); refs(x)} #&gt; [1] 1 # f() and g() never evaluate x, so refs don&#39;t increment f &lt;- function(x) 10 g &lt;- function(x) substitute(x) {x &lt;- 1:10; f(x); refs(x)} #&gt; [1] 1 {x &lt;- 1:10; g(x); refs(x)} #&gt; [1] 1 Generally, provided that the object is not referred to elsewhere, any primitive replacement function will modify in place. This includes [[&lt;-, [&lt;-, @&lt;-, $&lt;-, attr&lt;-, attributes&lt;-, class&lt;-, dim&lt;-, dimnames&lt;-, names&lt;-, and levels&lt;-. To be precise, all non-primitive functions increment refs, but a primitive function may be written in such a way that it doesn’t. The rules are sufficiently complicated that there’s little point in trying to memorise them. Instead, you should approach the problem practically by using refs() and address() to figure out when objects are being copied. While determining that copies are being made is not hard, preventing such behaviour is. If you find yourself resorting to exotic tricks to avoid copies, it may be time to rewrite your function in C++, as described in Rcpp. 5.4.1 Loops For loops in R have a reputation for being slow. Often that slowness is because you’re modifying a copy instead of modifying in place. Consider the following code. It subtracts the median from each column of a large data frame: x &lt;- data.frame(matrix(runif(100 * 1e4), ncol = 100)) medians &lt;- vapply(x, median, numeric(1)) for(i in seq_along(medians)) { x[, i] &lt;- x[, i] - medians[i] } You may be surprised to realise that every iteration of the loop copies the data frame. We can see that more clearly by using address() and refs() for a small sample of the loop: for(i in 1:5) { x[, i] &lt;- x[, i] - medians[i] print(c(address(x), refs(x))) } #&gt; [1] &quot;0x7fd9ba8d5da0&quot; &quot;2&quot; #&gt; [1] &quot;0x7fd9ba8d6790&quot; &quot;2&quot; #&gt; [1] &quot;0x7fd9ba8d7180&quot; &quot;2&quot; #&gt; [1] &quot;0x7fd9ba8d7b70&quot; &quot;2&quot; #&gt; [1] &quot;0x7fd9ba8d8560&quot; &quot;2&quot; For each iteration, x is moved to a new location so refs(x) is always 2. This occurs because [&lt;-.data.frame is not a primitive function, so it always increments the refs. We can make the function substantially more efficient by using a list instead of a data frame. Modifying a list uses primitive functions, so the refs are not incremented and all modifications occur in place: y &lt;- as.list(x) for(i in 1:5) { y[[i]] &lt;- y[[i]] - medians[i] print(c(address(y), refs(y))) } #&gt; [1] &quot;0x7fd9b7e24760&quot; &quot;1&quot; #&gt; [1] &quot;0x7fd9b7e24760&quot; &quot;1&quot; #&gt; [1] &quot;0x7fd9b7e24760&quot; &quot;1&quot; #&gt; [1] &quot;0x7fd9b7e24760&quot; &quot;1&quot; #&gt; [1] &quot;0x7fd9b7e24760&quot; &quot;1&quot; This behaviour was substantially more problematic prior to R 3.1.0, because every copy of the data frame was a deep copy. This made the motivating example take around 5 s, compared to 0.01 s today. 5.4.2 Exercises The code below makes one duplication. Where does it occur and why? (Hint: look at refs(y).) y &lt;- as.list(x) for(i in seq_along(medians)) { y[[i]] &lt;- y[[i]] - medians[i] } The implementation of as.data.frame() in the previous section has one big downside. What is it and how could you avoid it? "],
["environments.html", "6 Environments 6.1 Environment basics 6.2 Recursing over environments 6.3 Function environments 6.4 Binding names to values 6.5 Explicit environments 6.6 Quiz answers", " 6 Environments The environment is the data structure that powers scoping. This chapter dives deep into environments, describing their structure in depth, and using them to improve your understanding of the four scoping rules described in lexical scoping. Environments can also be useful data structures in their own right because they have reference semantics. When you modify a binding in an environment, the environment is not copied; it’s modified in place. Reference semantics are not often needed, but can be extremely useful. 6.0.0.0.1 Quiz If you can answer the following questions correctly, you already know the most important topics in this chapter. You can find the answers at the end of the chapter in answers. List at least three ways that an environment is different to a list. What is the parent of the global environment? What is the only environment that doesn’t have a parent? What is the enclosing environment of a function? Why is it important? How do you determine the environment from which a function was called? How are &lt;- and &lt;&lt;- different? 6.0.0.0.2 Outline Environment basics introduces you to the basic properties of an environment and shows you how to create your own. Recursing over environments provides a function template for computing with environments, illustrating the idea with a useful function. Function environments revises R’s scoping rules in more depth, showing how they correspond to four types of environment associated with each function. Binding names to values describes the rules that names must follow (and how to bend them), and shows some variations on binding a name to a value. Explicit environments discusses three problems where environments are useful data structures in their own right, independent of the role they play in scoping. 6.0.0.0.3 Prerequisites This chapter uses many functions from the pryr package to pry open R and look inside at the messy details. You can install pryr by running install.packages(&quot;pryr&quot;) 6.1 Environment basics The job of an environment is to associate, or bind, a set of names to a set of values. You can think of an environment as a bag of names: Each name points to an object stored elsewhere in memory: e &lt;- new.env() e$a &lt;- FALSE e$b &lt;- &quot;a&quot; e$c &lt;- 2.3 e$d &lt;- 1:3 The objects don’t live in the environment so multiple names can point to the same object: e$a &lt;- e$d Confusingly, they can also point to different objects that have the same value: e$a &lt;- 1:3 If an object has no names pointing to it, it gets automatically deleted by the garbage collector. This process is described in more detail in gc. Every environment has a parent, another environment. In diagrams, I’ll represent the pointer to parent with a small black circle. The parent is used to implement lexical scoping: if a name is not found in an environment, then R will look in its parent (and so on). Only one environment doesn’t have a parent: the empty environment. We use the metaphor of a family to refer to environments. The grandparent of an environment is the parent’s parent, and the ancestors include all parent environments up to the empty environment. It’s rare to talk about the children of an environment because there are no back links: given an environment we have no way to find its children. Generally, an environment is similar to a list, with four important exceptions: Every name in an environment is unique. The names in an environment are not ordered (i.e., it doesn’t make sense to ask what the first element of an environment is). An environment has a parent. Environments have reference semantics. More technically, an environment is made up of two components, the frame, which contains the name-object bindings (and behaves much like a named list), and the parent environment. Unfortunately “frame” is used inconsistently in R. For example, parent.frame() doesn’t give you the parent frame of an environment. Instead, it gives you the calling environment. This is discussed in more detail in calling environments. There are four special environments: The globalenv(), or global environment, is the interactive workspace. This is the environment in which you normally work. The parent of the global environment is the last package that you attached with library() or require(). The baseenv(), or base environment, is the environment of the base package. Its parent is the empty environment. The emptyenv(), or empty environment, is the ultimate ancestor of all environments, and the only environment without a parent. The environment() is the current environment. search() lists all parents of the global environment. This is called the search path because objects in these environments can be found from the top-level interactive workspace. It contains one environment for each attached package and any other objects that you’ve attach()ed. It also contains a special environment called Autoloads which is used to save memory by only loading package objects (like big datasets) when needed. You can access any environment on the search list using as.environment(). search() #&gt; [1] &quot;.GlobalEnv&quot; &quot;package:stats&quot; &quot;package:graphics&quot; #&gt; [4] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; #&gt; [7] &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; as.environment(&quot;package:stats&quot;) #&gt; &lt;environment: package:stats&gt; globalenv(), baseenv(), the environments on the search path, and emptyenv() are connected as shown below. Each time you load a new package with library() it is inserted between the global environment and the package that was previously at the top of the search path. To create an environment manually, use new.env(). You can list the bindings in the environment’s frame with ls() and see its parent with parent.env(). e &lt;- new.env() # the default parent provided by new.env() is the environment from # which it is called - in this case that&#39;s the global environment. parent.env(e) #&gt; &lt;environment: R_GlobalEnv&gt; ls(e) #&gt; character(0) The easiest way to modify the bindings in an environment is to treat it like a list: e$a &lt;- 1 e$b &lt;- 2 ls(e) #&gt; [1] &quot;a&quot; &quot;b&quot; e$a #&gt; [1] 1 By default, ls() only shows names that don’t begin with .. Use all.names = TRUE to show all bindings in an environment: e$.a &lt;- 2 ls(e) #&gt; [1] &quot;a&quot; &quot;b&quot; ls(e, all.names = TRUE) #&gt; [1] &quot;.a&quot; &quot;a&quot; &quot;b&quot; Another useful way to view an environment is ls.str(). It is more useful than str() because it shows each object in the environment. Like ls(), it also has an all.names argument. str(e) #&gt; &lt;environment: 0x7fc820d15408&gt; ls.str(e) #&gt; a : num 1 #&gt; b : num 2 Given a name, you can extract the value to which it is bound with $, [[, or get(): $ and [[ look only in one environment and return NULL if there is no binding associated with the name. get() uses the regular scoping rules and throws an error if the binding is not found. e$c &lt;- 3 e$c #&gt; [1] 3 e[[&quot;c&quot;]] #&gt; [1] 3 get(&quot;c&quot;, envir = e) #&gt; [1] 3 Deleting objects from environments works a little differently from lists. With a list you can remove an entry by setting it to NULL. In environments, that will create a new binding to NULL. Instead, use rm() to remove the binding. e &lt;- new.env() e$a &lt;- 1 e$a &lt;- NULL ls(e) #&gt; [1] &quot;a&quot; rm(&quot;a&quot;, envir = e) ls(e) #&gt; character(0) You can determine if a binding exists in an environment with exists(). Like get(), its default behaviour is to follow the regular scoping rules and look in parent environments. If you don’t want this behavior, use inherits = FALSE: x &lt;- 10 exists(&quot;x&quot;, envir = e) #&gt; [1] TRUE exists(&quot;x&quot;, envir = e, inherits = FALSE) #&gt; [1] FALSE To compare environments, you must use identical() not ==: identical(globalenv(), environment()) #&gt; [1] TRUE globalenv() == environment() #&gt; Error in globalenv() == environment(): Vergleich (1) ist nur für atomare und Listentypen möglich 6.1.1 Exercises List three ways in which an environment differs from a list. If you don’t supply an explicit environment, where do ls() and rm() look? Where does &lt;- make bindings? Using parent.env() and a loop (or a recursive function), verify that the ancestors of globalenv() include baseenv() and emptyenv(). Use the same basic idea to implement your own version of search(). 6.2 Recursing over environments Environments form a tree, so it’s often convenient to write a recursive function. This section shows you how by applying your new knowledge of environments to understand the helpful pryr::where(). Given a name, where() finds the environment where that name is defined, using R’s regular scoping rules: library(pryr) x &lt;- 5 where(&quot;x&quot;) #&gt; &lt;environment: R_GlobalEnv&gt; where(&quot;mean&quot;) #&gt; &lt;environment: base&gt; The definition of where() is straightforward. It has two arguments: the name to look for (as a string), and the environment in which to start the search. (We’ll learn later why parent.frame() is a good default in calling environments.) where &lt;- function(name, env = parent.frame()) { if (identical(env, emptyenv())) { # Base case stop(&quot;Can&#39;t find &quot;, name, call. = FALSE) } else if (exists(name, envir = env, inherits = FALSE)) { # Success case env } else { # Recursive case where(name, parent.env(env)) } } There are three cases: The base case: we’ve reached the empty environment and haven’t found the binding. We can’t go any further, so we throw an error. The successful case: the name exists in this environment, so we return the environment. The recursive case: the name was not found in this environment, so try the parent. It’s easier to see what’s going on with an example. Imagine you have two environments as in the following diagram: If you’re looking for a, where() will find it in the first environment. If you’re looking for b, it’s not in the first environment, so where() will look in its parent and find it there. If you’re looking for c, it’s not in the first environment, or the second environment, so where() reaches the empty environment and throws an error. It’s natural to work with environments recursively, so where() provides a useful template. Removing the specifics of where() shows the structure more clearly: f &lt;- function(..., env = parent.frame()) { if (identical(env, emptyenv())) { # base case } else if (success) { # success case } else { # recursive case f(..., env = parent.env(env)) } } Iteration vs. recursion It’s possible to use a loop instead of recursion. This might run slightly faster (because we eliminate some function calls), but I think it’s harder to understand. I include it because you might find it easier to see what’s happening if you’re less familiar with recursive functions. is_empty &lt;- function(x) identical(x, emptyenv()) f2 &lt;- function(..., env = parent.frame()) { while (!is_empty(env)) { if (success) { # success case } # inspect parent env &lt;- parent.env(env) } # base case } 6.2.1 Exercises Modify where() to find all environments that contain a binding for name. Write your own version of get() using a function written in the style of where(). Write a function called fget() that finds only function objects. It should have two arguments, name and env, and should obey the regular scoping rules for functions: if there’s an object with a matching name that’s not a function, look in the parent. For an added challenge, also add an inherits argument which controls whether the function recurses up the parents or only looks in one environment. Write your own version of exists(inherits = FALSE) (Hint: use ls().) Write a recursive version that behaves like exists(inherits = TRUE). 6.3 Function environments Most environments are not created by you with new.env() but are created as a consequence of using functions. This section discusses the four types of environments associated with a function: enclosing, binding, execution, and calling. The enclosing environment is the environment where the function was created. Every function has one and only one enclosing environment. For the three other types of environment, there may be 0, 1, or many environments associated with each function: Binding a function to a name with &lt;- defines a binding environment. Calling a function creates an ephemeral execution environment that stores variables created during execution. Every execution environment is associated with a calling environment, which tells you where the function was called. The following sections will explain why each of these environments is important, how to access them, and how you might use them. 6.3.1 The enclosing environment When a function is created, it gains a reference to the environment where it was made. This is the enclosing environment and is used for lexical scoping. You can determine the enclosing environment of a function by calling environment() with a function as its first argument: y &lt;- 1 f &lt;- function(x) x + y environment(f) #&gt; &lt;environment: R_GlobalEnv&gt; In diagrams, I’ll depict functions as rounded rectangles. The enclosing environment of a function is given by a small black circle: 6.3.2 Binding environments The previous diagram is too simple because functions don’t have names. Instead, the name of a function is defined by a binding. The binding environments of a function are all the environments which have a binding to it. The following diagram better reflects this relationship because the enclosing environment contains a binding from f to the function: In this case the enclosing and binding environments are the same. They will be different if you assign a function into a different environment: e &lt;- new.env() e$g &lt;- function() 1 The enclosing environment belongs to the function, and never changes, even if the function is moved to a different environment. The enclosing environment determines how the function finds values; the binding environments determine how we find the function. The distinction between the binding environment and the enclosing environment is important for package namespaces. Package namespaces keep packages independent. For example, if package A uses the base mean() function, what happens if package B creates its own mean() function? Namespaces ensure that package A continues to use the base mean() function, and that package A is not affected by package B (unless explicitly asked for). Namespaces are implemented using environments, taking advantage of the fact that functions don’t have to live in their enclosing environments. For example, take the base function sd(). Its enclosing and binding environments are different: ## enclosing = where sd() finds values, such as mean() or var() environment(sd) #&gt; &lt;environment: namespace:stats&gt; ## binding = where we can find sd() where(&quot;sd&quot;) #&gt; &lt;environment: package:stats&gt; The definition of sd() uses var(), but if we make our own version of var() it doesn’t affect sd(): x &lt;- 1:10 sd(x) #&gt; [1] 3.03 var &lt;- function(x, na.rm = TRUE) 100 sd(x) #&gt; [1] 3.03 This works because every package has two environments associated with it: the package environment and the namespace environment. The package environment contains every publicly accessible function, and is placed on the search path. The namespace environment contains all functions (including internal functions), and its parent environment is a special imports environment that contains bindings to all the functions that the package needs. Every exported function in a package is bound into the package environment, but enclosed by the namespace environment. This complicated relationship is illustrated by the following diagram: When we type var into the console, it’s found first in the global environment. When sd() looks for var() it finds it first in its namespace environment so never looks in the globalenv(). 6.3.3 Execution environments What will the following function return the first time it’s run? What about the second? g &lt;- function(x) { if (!exists(&quot;a&quot;, inherits = FALSE)) { message(&quot;Defining a&quot;) a &lt;- 1 } else { a &lt;- a + 1 } a } g(10) g(10) This function returns the same value every time it is called because of the fresh start principle, described in a fresh start. Each time a function is called, a new environment is created to host execution. The parent of the execution environment is the enclosing environment of the function. Once the function has completed, this environment is thrown away. Let’s depict that graphically with a simpler function. I draw execution environments around the function they belong to with a dotted border. h &lt;- function(x) { a &lt;- 2 x + a } y &lt;- h(1) When you create a function inside another function, the enclosing environment of the child function is the execution environment of the parent, and the execution environment is no longer ephemeral. The following example illustrates that idea with a function factory, plus(). We use that factory to create a function called plus_one(). The enclosing environment of plus_one() is the execution environment of plus() where x is bound to the value 1. plus &lt;- function(x) { function(y) x + y } plus_one &lt;- plus(1) identical(parent.env(environment(plus_one)), environment(plus)) #&gt; [1] TRUE You’ll learn more about function factories in functional programming. 6.3.4 Calling environments Look at the following code. What do you expect i() to return when the code is run? h &lt;- function() { x &lt;- 10 function() { x } } i &lt;- h() x &lt;- 20 i() The top-level x (bound to 20) is a red herring: using the regular scoping rules, h() looks first where it is defined and finds that the value associated with x is 10. However, it’s still meaningful to ask what value x is associated with in the environment where i() is called: x is 10 in the environment where h() is defined, but it is 20 in the environment where h() is called. We can access this environment using the unfortunately named parent.frame(). This function returns the environment where the function was called. We can also use this function to look up the value of names in that environment: f2 &lt;- function() { x &lt;- 10 function() { def &lt;- get(&quot;x&quot;, environment()) cll &lt;- get(&quot;x&quot;, parent.frame()) list(defined = def, called = cll) } } g2 &lt;- f2() x &lt;- 20 str(g2()) #&gt; List of 2 #&gt; $ defined: num 10 #&gt; $ called : num 20 In more complicated scenarios, there’s not just one parent call, but a sequence of calls which lead all the way back to the initiating function, called from the top-level. The following code generates a call stack three levels deep. The open-ended arrows represent the calling environment of each execution environment. x &lt;- 0 y &lt;- 10 f &lt;- function() { x &lt;- 1 g() } g &lt;- function() { x &lt;- 2 h() } h &lt;- function() { x &lt;- 3 x + y } f() #&gt; [1] 13 Note that each execution environment has two parents: a calling environment and an enclosing environment. R’s regular scoping rules only use the enclosing parent; parent.frame() allows you to access the calling parent. Looking up variables in the calling environment rather than in the enclosing environment is called dynamic scoping. Few languages implement dynamic scoping (Emacs Lisp is a notable exception.) This is because dynamic scoping makes it much harder to reason about how a function operates: not only do you need to know how it was defined, you also need to know in what context it was called. Dynamic scoping is primarily useful for developing functions that aid interactive data analysis. It is one of the topics discussed in non-standard evaluation. 6.3.5 Exercises List the four environments associated with a function. What does each one do? Why is the distinction between enclosing and binding environments particularly important? Draw a diagram that shows the enclosing environments of this function: f1 &lt;- function(x1) { f2 &lt;- function(x2) { f3 &lt;- function(x3) { x1 + x2 + x3 } f3(3) } f2(2) } f1(1) Expand your previous diagram to show function bindings. Expand it again to show the execution and calling environments. Write an enhanced version of str() that provides more information about functions. Show where the function was found and what environment it was defined in. 6.4 Binding names to values Assignment is the act of binding (or rebinding) a name to a value in an environment. It is the counterpart to scoping, the set of rules that determines how to find the value associated with a name. Compared to most languages, R has extremely flexible tools for binding names to values. In fact, you can not only bind values to names, but you can also bind expressions (promises) or even functions, so that every time you access the value associated with a name, you get something different! You’ve probably used regular assignment in R thousands of times. Regular assignment creates a binding between a name and an object in the current environment. Names usually consist of letters, digits, . and _, and can’t begin with _. If you try to use a name that doesn’t follow these rules, you get an error: _abc &lt;- 1 # Error: unexpected input in &quot;_&quot; Reserved words (like TRUE, NULL, if, and function) follow the rules but are reserved by R for other purposes: if &lt;- 10 #&gt; Error: unexpected assignment in &quot;if &lt;-&quot; A complete list of reserved words can be found in ?Reserved. It’s possible to override the usual rules and use a name with any sequence of characters by surrounding the name with backticks: `a + b` &lt;- 3 `:)` &lt;- &quot;smile&quot; ` ` &lt;- &quot;spaces&quot; ls() # [1] &quot; &quot; &quot;:)&quot; &quot;a + b&quot; `:)` # [1] &quot;smile&quot; Quotes You can also create non-syntactic bindings using single and double quotes instead of backticks, but I don’t recommend it. The ability to use strings on the left hand side of the assignment arrow is a historical artefact, used before R supported backticks. The regular assignment arrow, &lt;-, always creates a variable in the current environment. The deep assignment arrow, &lt;&lt;-, never creates a variable in the current environment, but instead modifies an existing variable found by walking up the parent environments. x &lt;- 0 f &lt;- function() { x &lt;&lt;- 1 } f() x #&gt; [1] 1 If &lt;&lt;- doesn’t find an existing variable, it will create one in the global environment. This is usually undesirable, because global variables introduce non-obvious dependencies between functions. &lt;&lt;- is most often used in conjunction with a closure, as described in Closures. There are two other special types of binding, delayed and active: Rather than assigning the result of an expression immediately, a delayed binding creates and stores a promise to evaluate the expression when needed. We can create delayed bindings with the special assignment operator %&lt;d-%, provided by the pryr package. library(pryr) system.time(b %&lt;d-% {Sys.sleep(1); 1}) #&gt; User System verstrichen #&gt; 0 0 0 system.time(b) #&gt; User System verstrichen #&gt; 0 0 1 %&lt;d-% is a wrapper around the base delayedAssign() function, which you may need to use directly if you need more control. Delayed bindings are used to implement autoload(), which makes R behave as if the package data is in memory, even though it’s only loaded from disk when you ask for it. Active are not bound to a constant object. Instead, they’re re-computed every time they’re accessed: x %&lt;a-% runif(1) x #&gt; [1] 0.0808 x #&gt; [1] 0.834 rm(x) %&lt;a-% is a wrapper for the base function makeActiveBinding(). You may want to use this function directly if you want more control. Active bindings are used to implement reference class fields. 6.4.1 Exercises What does this function do? How does it differ from &lt;&lt;- and why might you prefer it? rebind &lt;- function(name, value, env = parent.frame()) { if (identical(env, emptyenv())) { stop(&quot;Can&#39;t find &quot;, name, call. = FALSE) } else if (exists(name, envir = env, inherits = FALSE)) { assign(name, value, envir = env) } else { rebind(name, value, parent.env(env)) } } rebind(&quot;a&quot;, 10) #&gt; Error: Can&#39;t find a a &lt;- 5 rebind(&quot;a&quot;, 10) a #&gt; [1] 10 Create a version of assign() that will only bind new names, never re-bind old names. Some programming languages only do this, and are known as single assignment languages. Write an assignment function that can do active, delayed, and locked bindings. What might you call it? What arguments should it take? Can you guess which sort of assignment it should do based on the input? 6.5 Explicit environments As well as powering scoping, environments are also useful data structures in their own right because they have reference semantics. Unlike most objects in R, when you modify an environment, it does not make a copy. For example, look at this modify() function. modify &lt;- function(x) { x$a &lt;- 2 invisible() } If you apply it to a list, the original list is not changed because modifying a list actually creates and modifies a copy. x_l &lt;- list() x_l$a &lt;- 1 modify(x_l) x_l$a #&gt; [1] 1 However, if you apply it to an environment, the original environment is modified: x_e &lt;- new.env() x_e$a &lt;- 1 modify(x_e) x_e$a #&gt; [1] 2 Just as you can use a list to pass data between functions, you can also use an environment. When creating your own environment, note that you should set its parent environment to be the empty environment. This ensures you don’t accidentally inherit objects from somewhere else: x &lt;- 1 e1 &lt;- new.env() get(&quot;x&quot;, envir = e1) #&gt; [1] 1 e2 &lt;- new.env(parent = emptyenv()) get(&quot;x&quot;, envir = e2) #&gt; Error in get(&quot;x&quot;, envir = e2): Objekt &#39;x&#39; nicht gefunden Environments are data structures useful for solving three common problems: Avoiding copies of large data. Managing state within a package. Efficiently looking up values from names. These are described in turn below. 6.5.1 Avoiding copies Since environments have reference semantics, you’ll never accidentally create a copy. This makes it a useful vessel for large objects. It’s a common technique for bioconductor packages which often have to manage large genomic objects. Changes to R 3.1.0 have made this use substantially less important because modifying a list no longer makes a deep copy. Previously, modifying a single element of a list would cause every element to be copied, an expensive operation if some elements are large. Now, modifying a list efficiently reuses existing vectors, saving much time. 6.5.2 Package state Explicit environments are useful in packages because they allow you to maintain state across function calls. Normally, objects in a package are locked, so you can’t modify them directly. Instead, you can do something like this: my_env &lt;- new.env(parent = emptyenv()) my_env$a &lt;- 1 get_a &lt;- function() { my_env$a } set_a &lt;- function(value) { old &lt;- my_env$a my_env$a &lt;- value invisible(old) } Returning the old value from setter functions is a good pattern because it makes it easier to reset the previous value in conjunction with on.exit() (see more in on exit). 6.5.3 As a hashmap A hashmap is a data structure that takes constant, O(1), time to find an object based on its name. Environments provide this behaviour by default, so can be used to simulate a hashmap. See the CRAN package hash for a complete development of this idea. 6.6 Quiz answers There are four ways: every object in an environment must have a name; order doesn’t matter; environments have parents; environments have reference semantics. The parent of the global environment is the last package that you loaded. The only environment that doesn’t have a parent is the empty environment. The enclosing environment of a function is the environment where it was created. It determines where a function looks for variables. Use parent.frame(). &lt;- always creates a binding in the current environment; &lt;&lt;- rebinds an existing name in a parent of the current environment. "],
["debugging.html", "7 Debugging, condition handling, and defensive programming 7.1 Debugging techniques 7.2 Debugging tools 7.3 Condition handling 7.4 Defensive programming 7.5 Quiz answers", " 7 Debugging, condition handling, and defensive programming What happens when something goes wrong with your R code? What do you do? What tools do you have to address the problem? This chapter will teach you how to fix unanticipated problems (debugging), show you how functions can communicate problems and how you can take action based on those communications (condition handling), and teach you how to avoid common problems before they occur (defensive programming). Debugging is the art and science of fixing unexpected problems in your code. In this section you’ll learn the tools and techniques that help you get to the root cause of an error. You’ll learn general strategies for debugging, useful R functions like traceback() and browser(), and interactive tools in RStudio. Not all problems are unexpected. When writing a function, you can often anticipate potential problems (like a non-existent file or the wrong type of input). Communicating these problems to the user is the job of conditions: errors, warnings, and messages. Fatal errors are raised by stop() and force all execution to terminate. Errors are used when there is no way for a function to continue. Warnings are generated by warning() and are used to display potential problems, such as when some elements of a vectorised input are invalid, like log(-1:2). Messages are generated by message() and are used to give informative output in a way that can easily be suppressed by the user (?suppressMessages()). I often use messages to let the user know what value the function has chosen for an important missing argument. Conditions are usually displayed prominently, in a bold font or coloured red depending on your R interface. You can tell them apart because errors always start with “Error” and warnings with “Warning message”. Function authors can also communicate with their users with print() or cat(), but I think that’s a bad idea because it’s hard to capture and selectively ignore this sort of output. Printed output is not a condition, so you can’t use any of the useful condition handling tools you’ll learn about below. Condition handling tools, like withCallingHandlers(), tryCatch(), and try() allow you to take specific actions when a condition occurs. For example, if you’re fitting many models, you might want to continue fitting the others even if one fails to converge. R offers an exceptionally powerful condition handling system based on ideas from Common Lisp, but it’s currently not very well documented or often used. This chapter will introduce you to the most important basics, but if you want to learn more, I recommend the following two sources: A prototype of a condition system for R by Robert Gentleman and Luke Tierney. This describes an early version of R’s condition system. While the implementation has changed somewhat since this document was written, it provides a good overview of how the pieces fit together, and some motivation for its design. Beyond Exception Handling: Conditions and Restarts by Peter Seibel. This describes exception handling in Lisp, which happens to be very similar to R’s approach. It provides useful motivation and more sophisticated examples. I have provided an R translation of the chapter at http://adv-r.had.co.nz/beyond-exception-handling.html. The chapter concludes with a discussion of “defensive” programming: ways to avoid common errors before they occur. In the short run you’ll spend more time writing code, but in the long run you’ll save time because error messages will be more informative and will let you narrow in on the root cause more quickly. The basic principle of defensive programming is to “fail fast”, to raise an error as soon as something goes wrong. In R, this takes three particular forms: checking that inputs are correct, avoiding non-standard evaluation, and avoiding functions that can return different types of output. 7.0.0.0.1 Quiz Want to skip this chapter? Go for it, if you can answer the questions below. Find the answers at the end of the chapter in answers. How can you find out where an error occurred? What does browser() do? List the five useful single-key commands that you can use inside of a browser() environment. What function do you use to ignore errors in block of code? Why might you want to create an error with a custom S3 class? 7.0.0.0.2 Outline Debugging techniques outlines a general approach for finding and resolving bugs. Debugging tools introduces you to the R functions and RStudio features that help you locate exactly where an error occurred. Condition handling shows you how you can catch conditions (errors, warnings, and messages) in your own code. This allows you to create code that’s both more robust and more informative in the presence of errors. Defensive programming introduces you to some important techniques for defensive programming, techniques that help prevent bugs from occurring in the first place. 7.1 Debugging techniques “Finding your bug is a process of confirming the many things that you believe are true — until you find one which is not true.” —Norm Matloff Debugging code is challenging. Many bugs are subtle and hard to find. Indeed, if a bug was obvious, you probably would’ve been able to avoid it in the first place. While it’s true that with a good technique, you can productively debug a problem with just print(), there are times when additional help would be welcome. In this section, we’ll discuss some useful tools, which R and RStudio provide, and outline a general procedure for debugging. While the procedure below is by no means foolproof, it will hopefully help you to organise your thoughts when debugging. There are four steps: Realise that you have a bug If you’re reading this chapter, you’ve probably already completed this step. It is a surprisingly important one: you can’t fix a bug until you know it exists. This is one reason why automated test suites are important when producing high-quality code. Unfortunately, automated testing is outside the scope of this book, but you can read more about it at http://r-pkgs.had.co.nz/tests.html. Make it repeatable Once you’ve determined you have a bug, you need to be able to reproduce it on command. Without this, it becomes extremely difficult to isolate its cause and to confirm that you’ve successfully fixed it. Generally, you will start with a big block of code that you know causes the error and then slowly whittle it down to get to the smallest possible snippet that still causes the error. Binary search is particularly useful for this. To do a binary search, you repeatedly remove half of the code until you find the bug. This is fast because, with each step, you reduce the amount of code to look through by half. If it takes a long time to generate the bug, it’s also worthwhile to figure out how to generate it faster. The quicker you can do this, the quicker you can figure out the cause. As you work on creating a minimal example, you’ll also discover similar inputs that don’t trigger the bug. Make note of them: they will be helpful when diagnosing the cause of the bug. If you’re using automated testing, this is also a good time to create an automated test case. If your existing test coverage is low, take the opportunity to add some nearby tests to ensure that existing good behaviour is preserved. This reduces the chances of creating a new bug. Figure out where it is If you’re lucky, one of the tools in the following section will help you to quickly identify the line of code that’s causing the bug. Usually, however, you’ll have to think a bit more about the problem. It’s a great idea to adopt the scientific method. Generate hypotheses, design experiments to test them, and record your results. This may seem like a lot of work, but a systematic approach will end up saving you time. I often waste a lot of time relying on my intuition to solve a bug (“oh, it must be an off-by-one error, so I’ll just subtract 1 here”), when I would have been better off taking a systematic approach. Fix it and test it Once you’ve found the bug, you need to figure out how to fix it and to check that the fix actually worked. Again, it’s very useful to have automated tests in place. Not only does this help to ensure that you’ve actually fixed the bug, it also helps to ensure you haven’t introduced any new bugs in the process. In the absence of automated tests, make sure to carefully record the correct output, and check against the inputs that previously failed. 7.2 Debugging tools To implement a strategy of debugging, you’ll need tools. In this section, you’ll learn about the tools provided by R and the RStudio IDE. RStudio’s integrated debugging support makes life easier by exposing existing R tools in a user friendly way. I’ll show you both the R and RStudio ways so that you can work with whatever environment you use. You may also want to refer to the official RStudio debugging documentation which always reflects the tools in the latest version of RStudio. There are three key debugging tools: RStudio’s error inspector and traceback() which list the sequence of calls that lead to the error. RStudio’s “Rerun with Debug” tool and options(error = browser) which open an interactive session where the error occurred. RStudio’s breakpoints and browser() which open an interactive session at an arbitrary location in the code. I’ll explain each tool in more detail below. You shouldn’t need to use these tools when writing new functions. If you find yourself using them frequently with new code, you may want to reconsider your approach. Instead of trying to write one big function all at once, work interactively on small pieces. If you start small, you can quickly identify why something doesn’t work. But if you start large, you may end up struggling to identify the source of the problem. 7.2.1 Determining the sequence of calls The first tool is the call stack, the sequence of calls that lead up to an error. Here’s a simple example: you can see that f() calls g() calls h() calls i() which adds together a number and a string creating a error: f &lt;- function(a) g(a) g &lt;- function(b) h(b) h &lt;- function(c) i(c) i &lt;- function(d) &quot;a&quot; + d f(10) When we run this code in RStudio we see: Two options appear to the right of the error message: “Show Traceback” and “Rerun with Debug”. If you click “Show traceback” you see: If you’re not using RStudio, you can use traceback() to get the same information: traceback() # 4: i(c) at exceptions-example.R#3 # 3: h(b) at exceptions-example.R#2 # 2: g(a) at exceptions-example.R#1 # 1: f(10) Read the call stack from bottom to top: the initial call is f(), which calls g(), then h(), then i(), which triggers the error. If you’re calling code that you source()d into R, the traceback will also display the location of the function, in the form filename.r#linenumber. These are clickable in RStudio, and will take you to the corresponding line of code in the editor. Sometimes this is enough information to let you track down the error and fix it. However, it’s usually not. traceback() shows you where the error occurred, but not why. The next useful tool is the interactive debugger, which allows you to pause execution of a function and interactively explore its state. 7.2.2 Browsing on error The easiest way to enter the interactive debugger is through RStudio’s “Rerun with Debug” tool. This reruns the command that created the error, pausing execution where the error occurred. You’re now in an interactive state inside the function, and you can interact with any object defined there. You’ll see the corresponding code in the editor (with the statement that will be run next highlighted), objects in the current environment in the “Environment” pane, the call stack in a “Traceback” pane, and you can run arbitrary R code in the console. As well as any regular R function, there are a few special commands you can use in debug mode. You can access them either with the RStudio toolbar () or with the keyboard: Next, n: executes the next step in the function. Be careful if you have a variable named n; to print it you’ll need to do print(n). Step into, or s: works like next, but if the next step is a function, it will step into that function so you can work through each line. Finish, or f: finishes execution of the current loop or function. Continue, c: leaves interactive debugging and continues regular execution of the function. This is useful if you’ve fixed the bad state and want to check that the function proceeds correctly. Stop, Q: stops debugging, terminates the function, and returns to the global workspace. Use this once you’ve figured out where the problem is, and you’re ready to fix it and reload the code. There are two other slightly less useful commands that aren’t available in the toolbar: Enter: repeats the previous command. I find this too easy to activate accidentally, so I turn it off using options(browserNLdisabled = TRUE). where: prints stack trace of active calls (the interactive equivalent of traceback). To enter this style of debugging outside of RStudio, you can use the error option which specifies a function to run when an error occurs. The function most similar to RStudio’s debug is browser(): this will start an interactive console in the environment where the error occurred. Use options(error = browser) to turn it on, re-run the previous command, then use options(error = NULL) to return to the default error behaviour. You could automate this with the browseOnce() function as defined below: browseOnce &lt;- function() { old &lt;- getOption(&quot;error&quot;) function() { options(error = old) browser() } } options(error = browseOnce()) f &lt;- function() stop(&quot;!&quot;) # Enters browser f() # Runs normally f() (You’ll learn more about functions that return functions in Functional programming.) There are two other useful functions that you can use with the error option: recover is a step up from browser, as it allows you to enter the environment of any of the calls in the call stack. This is useful because often the root cause of the error is a number of calls back. dump.frames is an equivalent to recover for non-interactive code. It creates a last.dump.rda file in the current working directory. Then, in a later interactive R session, you load that file, and use debugger() to enter an interactive debugger with the same interface as recover(). This allows interactive debugging of batch code. # In batch R process ---- dump_and_quit &lt;- function() { # Save debugging info to file last.dump.rda dump.frames(to.file = TRUE) # Quit R with error status q(status = 1) } options(error = dump_and_quit) # In a later interactive session ---- load(&quot;last.dump.rda&quot;) debugger() To reset error behaviour to the default, use options(error = NULL). Then errors will print a message and abort function execution. 7.2.3 Browsing arbitrary code As well as entering an interactive console on error, you can enter it at an arbitrary code location by using either an RStudio breakpoint or browser(). You can set a breakpoint in RStudio by clicking to the left of the line number, or pressing Shift + F9. Equivalently, add browser() where you want execution to pause. Breakpoints behave similarly to browser() but they are easier to set (one click instead of nine key presses), and you don’t run the risk of accidentally including a browser() statement in your source code. There are two small downsides to breakpoints: There are a few unusual situations in which breakpoints will not work: read breakpoint troubleshooting for more details. RStudio currently does not support conditional breakpoints, whereas you can always put browser() inside an if statement. As well as adding browser() yourself, there are two other functions that will add it to code: debug() inserts a browser statement in the first line of the specified function. undebug() removes it. Alternatively, you can use debugonce() to browse only on the next run. utils::setBreakpoint() works similarly, but instead of taking a function name, it takes a file name and line number and finds the appropriate function for you. These two functions are both special cases of trace(), which inserts arbitrary code at any position in an existing function. trace() is occasionally useful when you’re debugging code that you don’t have the source for. To remove tracing from a function, use untrace(). You can only perform one trace per function, but that one trace can call multiple functions. 7.2.4 The call stack: traceback(), where, and recover() Unfortunately, the call stacks printed by traceback(), browser() + where, and recover() are not consistent. The following table shows how the call stacks from a simple nested set of calls are displayed by the three tools. traceback() where recover() 4: stop(&quot;Error&quot;) where 1: stop(&quot;Error&quot;) 1: f() 3: h(x) where 2: h(x) 2: g(x) 2: g(x) where 3: g(x) 3: h(x) 1: f() where 4: f() Note that numbering is different between traceback() and where, and that recover() displays calls in the opposite order, and omits the call to stop(). RStudio displays calls in the same order as traceback() but omits the numbers. 7.2.5 Other types of failure There are other ways for a function to fail apart from throwing an error or returning an incorrect result. A function may generate an unexpected warning. The easiest way to track down warnings is to convert them into errors with options(warn = 2) and use the regular debugging tools. When you do this you’ll see some extra calls in the call stack, like doWithOneRestart(), withOneRestart(), withRestarts(), and .signalSimpleWarning(). Ignore these: they are internal functions used to turn warnings into errors. A function may generate an unexpected message. There’s no built-in tool to help solve this problem, but it’s possible to create one: message2error &lt;- function(code) { withCallingHandlers(code, message = function(e) stop(e)) } f &lt;- function() g() g &lt;- function() message(&quot;Hi!&quot;) g() # Hi! message2error(g()) # Error in message(&quot;Hi!&quot;): Hi! traceback() # 10: stop(e) at #2 # 9: (function (e) stop(e))(list(message = &quot;Hi!\\n&quot;, # call = message(&quot;Hi!&quot;))) # 8: signalCondition(cond) # 7: doWithOneRestart(return(expr), restart) # 6: withOneRestart(expr, restarts[[1L]]) # 5: withRestarts() # 4: message(&quot;Hi!&quot;) at #1 # 3: g() # 2: withCallingHandlers(code, message = function(e) stop(e)) # at #2 # 1: message2error(g()) As with warnings, you’ll need to ignore some of the calls on the traceback (i.e., the first two and the last six). A function might never return. This is particularly hard to debug automatically, but sometimes terminating the function and looking at the call stack is informative. Otherwise, use the basic debugging strategies described above. The worst scenario is that your code might crash R completely, leaving you with no way to interactively debug your code. This indicates a bug in the underlying C code. This is hard to debug. Sometimes an interactive debugger, like gdb, can be useful, but describing how to use it is beyond the scope of this book. If the crash is caused by base R code, post a reproducible example to R-help. If it’s in a package, contact the package maintainer. If it’s your own C or C++ code, you’ll need to use numerous print() statements to narrow down the location of the bug, and then you’ll need to use many more print statements to figure out which data structure doesn’t have the properties that you expect. 7.3 Condition handling Unexpected errors require interactive debugging to figure out what went wrong. Some errors, however, are expected, and you want to handle them automatically. In R, expected errors crop up most frequently when you’re fitting many models to different datasets, such as bootstrap replicates. Sometimes the model might fail to fit and throw an error, but you don’t want to stop everything. Instead, you want to fit as many models as possible and then perform diagnostics after the fact. In R, there are three tools for handling conditions (including errors) programmatically: try() gives you the ability to continue execution even when an error occurs. tryCatch() lets you specify handler functions that control what happens when a condition is signalled. withCallingHandlers() is a variant of tryCatch() that establishes local handlers, whereas tryCatch() registers exiting handlers. Local handlers are called in the same context as where the condition is signalled, without interrupting the execution of the function. When an exiting handler from tryCatch() is called, the execution of the function is interrupted and the handler is called. withCallingHandlers() is rarely needed, but is useful to be aware of. The following sections describe these tools in more detail. 7.3.1 Ignore errors with try try() allows execution to continue even after an error has occurred. For example, normally if you run a function that throws an error, it terminates immediately and doesn’t return a value: f1 &lt;- function(x) { log(x) 10 } f1(&quot;x&quot;) #&gt; Error in log(x): Nicht-numerisches Argument für mathematische Funktion However, if you wrap the statement that creates the error in try(), the error message will be printed but execution will continue: f2 &lt;- function(x) { try(log(x)) 10 } f2(&quot;a&quot;) #&gt; Error in log(x) : non-numeric argument to mathematical function #&gt; [1] 10 You can suppress the message with try(..., silent = TRUE). To pass larger blocks of code to try(), wrap them in {}: try({ a &lt;- 1 b &lt;- &quot;x&quot; a + b }) You can also capture the output of the try() function. If successful, it will be the last result evaluated in the block (just like a function). If unsuccessful it will be an (invisible) object of class “try-error”: success &lt;- try(1 + 2) failure &lt;- try(&quot;a&quot; + &quot;b&quot;) class(success) #&gt; [1] &quot;numeric&quot; class(failure) #&gt; [1] &quot;try-error&quot; try() is particularly useful when you’re applying a function to multiple elements in a list: elements &lt;- list(1:10, c(-1, 10), c(TRUE, FALSE), letters) results &lt;- lapply(elements, log) #&gt; Warning in FUN(X[[i]], ...): NaNs wurden erzeugt #&gt; Error in FUN(X[[i]], ...): Nicht-numerisches Argument für mathematische Funktion results &lt;- lapply(elements, function(x) try(log(x))) #&gt; Warning in log(x): NaNs wurden erzeugt There isn’t a built-in function to test for the try-error class, so we’ll define one. Then you can easily find the locations of errors with sapply() (as discussed in Functionals), and extract the successes or look at the inputs that lead to failures. is.error &lt;- function(x) inherits(x, &quot;try-error&quot;) succeeded &lt;- !vapply(results, is.error, logical(1)) # look at successful results str(results[succeeded]) #&gt; List of 3 #&gt; $ : num [1:10] 0 0.693 1.099 1.386 1.609 ... #&gt; $ : num [1:2] NaN 2.3 #&gt; $ : num [1:2] 0 -Inf # look at inputs that failed str(elements[!succeeded]) #&gt; List of 1 #&gt; $ : chr [1:26] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ... Another useful try() idiom is using a default value if an expression fails. Simply assign the default value outside the try block, and then run the risky code: default &lt;- NULL try(default &lt;- read.csv(&quot;possibly-bad-input.csv&quot;), silent = TRUE) There is also plyr::failwith(), which makes this strategy even easier to implement. See Function Operators for more details. 7.3.2 Handle conditions with tryCatch() tryCatch() is a general tool for handling conditions: in addition to errors, you can take different actions for warnings, messages, and interrupts. You’ve seen errors (made by stop()), warnings (warning()) and messages (message()) before, but interrupts are new. They can’t be generated directly by the programmer, but are raised when the user attempts to terminate execution by pressing Ctrl + Break, Escape, or Ctrl + C (depending on the platform). With tryCatch() you map conditions to handlers, named functions that are called with the condition as an input. If a condition is signalled, tryCatch() will call the first handler whose name matches one of the classes of the condition. The only useful built-in names are error, warning, message, interrupt, and the catch-all condition. A handler function can do anything, but typically it will either return a value or create a more informative error message. For example, the show_condition() function below sets up handlers that return the type of condition signalled: show_condition &lt;- function(code) { tryCatch(code, error = function(c) &quot;error&quot;, warning = function(c) &quot;warning&quot;, message = function(c) &quot;message&quot; ) } show_condition(stop(&quot;!&quot;)) #&gt; [1] &quot;error&quot; show_condition(warning(&quot;?!&quot;)) #&gt; [1] &quot;warning&quot; show_condition(message(&quot;?&quot;)) #&gt; [1] &quot;message&quot; # If no condition is captured, tryCatch returns the # value of the input show_condition(10) #&gt; [1] 10 You can use tryCatch() to implement try(). A simple implementation is shown below. base::try() is more complicated in order to make the error message look more like what you’d see if tryCatch() wasn’t used. Note the use of conditionMessage() to extract the message associated with the original error. try2 &lt;- function(code, silent = FALSE) { tryCatch(code, error = function(c) { msg &lt;- conditionMessage(c) if (!silent) message(c) invisible(structure(msg, class = &quot;try-error&quot;)) }) } try2(1) #&gt; [1] 1 try2(stop(&quot;Hi&quot;)) try2(stop(&quot;Hi&quot;), silent = TRUE) As well as returning default values when a condition is signalled, handlers can be used to make more informative error messages. For example, by modifying the message stored in the error condition object, the following function wraps read.csv() to add the file name to any errors: read.csv2 &lt;- function(file, ...) { tryCatch(read.csv(file, ...), error = function(c) { c$message &lt;- paste0(c$message, &quot; (in &quot;, file, &quot;)&quot;) stop(c) }) } read.csv(&quot;code/dummy.csv&quot;) #&gt; Error in file(file, &quot;rt&quot;): kann Verbindung nicht öffnen read.csv2(&quot;code/dummy.csv&quot;) #&gt; Error in file(file, &quot;rt&quot;): kann Verbindung nicht öffnen (in code/dummy.csv) Catching interrupts can be useful if you want to take special action when the user tries to abort running code. But be careful, it’s easy to create a loop that you can never escape (unless you kill R)! # Don&#39;t let the user interrupt the code i &lt;- 1 while(i &lt; 3) { tryCatch({ Sys.sleep(0.5) message(&quot;Try to escape&quot;) }, interrupt = function(x) { message(&quot;Try again!&quot;) i &lt;&lt;- i + 1 }) } tryCatch() has one other argument: finally. It specifies a block of code (not a function) to run regardless of whether the initial expression succeeds or fails. This can be useful for clean up (e.g., deleting files, closing connections). This is functionally equivalent to using on.exit() but it can wrap smaller chunks of code than an entire function. 7.3.3 withCallingHandlers() An alternative to tryCatch() is withCallingHandlers(). The difference between the two is that the former establishes exiting handlers while the latter registers local handlers. Here are the main differences between the two kind of handlers: The handlers in withCallingHandlers() are called in the context of the call that generated the condition whereas the handlers in tryCatch() are called in the context of tryCatch(). This is shown here with sys.calls(), which is the run-time equivalent of traceback() — it lists all calls leading to the current function. f &lt;- function() g() g &lt;- function() h() h &lt;- function() stop(&quot;!&quot;) tryCatch(f(), error = function(e) print(sys.calls())) # [[1]] tryCatch(f(), error = function(e) print(sys.calls())) # [[2]] tryCatchList(expr, classes, parentenv, handlers) # [[3]] tryCatchOne(expr, names, parentenv, handlers[[1L]]) # [[4]] value[[3L]](cond) withCallingHandlers(f(), error = function(e) print(sys.calls())) # [[1]] withCallingHandlers(f(), # error = function(e) print(sys.calls())) # [[2]] f() # [[3]] g() # [[4]] h() # [[5]] stop(&quot;!&quot;) # [[6]] .handleSimpleError( # function (e) print(sys.calls()), &quot;!&quot;, quote(h())) # [[7]] h(simpleError(msg, call)) This also affects the order in which on.exit() is called. A related difference is that with tryCatch(), the flow of execution is interrupted when a handler is called, while with withCallingHandlers(), execution continues normally when the handler returns. This includes the signalling function which continues its course after having called the handler (e.g., stop() will continue stopping the program and message() or warning() will continue signalling a message/warning). This is why it is often better to handle a message with withCallingHandlers() rather than tryCatch(), since the latter will stop the program: message_handler &lt;- function(c) cat(&quot;Caught a message!\\n&quot;) tryCatch(message = message_handler, { message(&quot;Someone there?&quot;) message(&quot;Why, yes!&quot;) }) #&gt; Caught a message! withCallingHandlers(message = message_handler, { message(&quot;Someone there?&quot;) message(&quot;Why, yes!&quot;) }) #&gt; Caught a message! #&gt; Someone there? #&gt; Caught a message! #&gt; Why, yes! The return value of a handler is returned by tryCatch(), whereas it is ignored with withCallingHandlers(): f &lt;- function() message(&quot;!&quot;) tryCatch(f(), message = function(m) 1) #&gt; [1] 1 withCallingHandlers(f(), message = function(m) 1) #&gt; ! These subtle differences are rarely useful, except when you’re trying to capture exactly what went wrong and pass it on to another function. For most purposes, you should never need to use withCallingHandlers(). 7.3.4 Custom signal classes One of the challenges of error handling in R is that most functions just call stop() with a string. That means if you want to figure out if a particular error occurred, you have to look at the text of the error message. This is error prone, not only because the text of the error might change over time, but also because many error messages are translated, so the message might be completely different to what you expect. R has a little known and little used feature to solve this problem. Conditions are S3 classes, so you can define your own classes if you want to distinguish different types of error. Each condition signalling function, stop(), warning(), and message(), can be given either a list of strings, or a custom S3 condition object. Custom condition objects are not used very often, but are very useful because they make it possible for the user to respond to different errors in different ways. For example, “expected” errors (like a model failing to converge for some input datasets) can be silently ignored, while unexpected errors (like no disk space available) can be propagated to the user. R doesn’t come with a built-in constructor function for conditions, but we can easily add one. Conditions must contain message and call components, and may contain other useful components. When creating a new condition, it should always inherit from condition and should in most cases inherit from one of error, warning, or message. condition &lt;- function(subclass, message, call = sys.call(-1), ...) { structure( class = c(subclass, &quot;condition&quot;), list(message = message, call = call), ... ) } is.condition &lt;- function(x) inherits(x, &quot;condition&quot;) You can signal an arbitrary condition with signalCondition(), but nothing will happen unless you’ve instantiated a custom signal handler (with tryCatch() or withCallingHandlers()). Instead, pass this condition to stop(), warning(), or message() as appropriate to trigger the usual handling. R won’t complain if the class of your condition doesn’t match the function, but in real code you should pass a condition that inherits from the appropriate class: &quot;error&quot; for stop(), &quot;warning&quot; for warning(), and &quot;message&quot; for message(). e &lt;- condition(c(&quot;my_error&quot;, &quot;error&quot;), &quot;This is an error&quot;) signalCondition(e) # NULL stop(e) # Error: This is an error w &lt;- condition(c(&quot;my_warning&quot;, &quot;warning&quot;), &quot;This is a warning&quot;) warning(w) # Warning message: This is a warning m &lt;- condition(c(&quot;my_message&quot;, &quot;message&quot;), &quot;This is a message&quot;) message(m) # This is a message You can then use tryCatch() to take different actions for different types of errors. In this example we make a convenient custom_stop() function that allows us to signal error conditions with arbitrary classes. In a real application, it would be better to have individual S3 constructor functions that you could document, describing the error classes in more detail. custom_stop &lt;- function(subclass, message, call = sys.call(-1), ...) { c &lt;- condition(c(subclass, &quot;error&quot;), message, call = call, ...) stop(c) } my_log &lt;- function(x) { if (!is.numeric(x)) custom_stop(&quot;invalid_class&quot;, &quot;my_log() needs numeric input&quot;) if (any(x &lt; 0)) custom_stop(&quot;invalid_value&quot;, &quot;my_log() needs positive inputs&quot;) log(x) } tryCatch( my_log(&quot;a&quot;), invalid_class = function(c) &quot;class&quot;, invalid_value = function(c) &quot;value&quot; ) #&gt; [1] &quot;class&quot; Note that when using tryCatch() with multiple handlers and custom classes, the first handler to match any class in the signal’s class hierarchy is called, not the best match. For this reason, you need to make sure to put the most specific handlers first: tryCatch(custom_stop(&quot;my_error&quot;, &quot;!&quot;), error = function(c) &quot;error&quot;, my_error = function(c) &quot;my_error&quot; ) #&gt; [1] &quot;error&quot; tryCatch(custom_stop(&quot;my_error&quot;, &quot;!&quot;), my_error = function(c) &quot;my_error&quot;, error = function(c) &quot;error&quot; ) #&gt; [1] &quot;my_error&quot; 7.3.5 Exercises Compare the following two implementations of message2error(). What is the main advantage of withCallingHandlers() in this scenario? (Hint: look carefully at the traceback.) message2error &lt;- function(code) { withCallingHandlers(code, message = function(e) stop(e)) } message2error &lt;- function(code) { tryCatch(code, message = function(e) stop(e)) } 7.4 Defensive programming Defensive programming is the art of making code fail in a well-defined manner even when something unexpected occurs. A key principle of defensive programming is to “fail fast”: as soon as something wrong is discovered, signal an error. This is more work for the author of the function (you!), but it makes debugging easier for users because they get errors earlier rather than later, after unexpected input has passed through several functions. In R, the “fail fast” principle is implemented in three ways: Be strict about what you accept. For example, if your function is not vectorised in its inputs, but uses functions that are, make sure to check that the inputs are scalars. You can use stopifnot(), the assertthat package, or simple if statements and stop(). Avoid functions that use non-standard evaluation, like subset, transform, and with. These functions save time when used interactively, but because they make assumptions to reduce typing, when they fail, they often fail with uninformative error messages. You can learn more about non-standard evaluation in non-standard evaluation. Avoid functions that return different types of output depending on their input. The two biggest offenders are [ and sapply(). Whenever subsetting a data frame in a function, you should always use drop = FALSE, otherwise you will accidentally convert 1-column data frames into vectors. Similarly, never use sapply() inside a function: always use the stricter vapply() which will throw an error if the inputs are incorrect types and return the correct type of output even for zero-length inputs. There is a tension between interactive analysis and programming. When you’re working interactively, you want R to do what you mean. If it guesses wrong, you want to discover that right away so you can fix it. When you’re programming, you want functions that signal errors if anything is even slightly wrong or underspecified. Keep this tension in mind when writing functions. If you’re writing functions to facilitate interactive data analysis, feel free to guess what the analyst wants and recover from minor misspecifications automatically. If you’re writing functions for programming, be strict. Never try to guess what the caller wants. 7.4.1 Exercises The goal of the col_means() function defined below is to compute the means of all numeric columns in a data frame. col_means &lt;- function(df) { numeric &lt;- sapply(df, is.numeric) numeric_cols &lt;- df[, numeric] data.frame(lapply(numeric_cols, mean)) } However, the function is not robust to unusual inputs. Look at the following results, decide which ones are incorrect, and modify col_means() to be more robust. (Hint: there are two function calls in col_means() that are particularly prone to problems.) col_means(mtcars) col_means(mtcars[, 0]) col_means(mtcars[0, ]) col_means(mtcars[, &quot;mpg&quot;, drop = F]) col_means(1:10) col_means(as.matrix(mtcars)) col_means(as.list(mtcars)) mtcars2 &lt;- mtcars mtcars2[-1] &lt;- lapply(mtcars2[-1], as.character) col_means(mtcars2) The following function “lags” a vector, returning a version of x that is n values behind the original. Improve the function so that it (1) returns a useful error message if n is not a vector, and (2) has reasonable behaviour when n is 0 or longer than x. lag &lt;- function(x, n = 1L) { xlen &lt;- length(x) c(rep(NA, n), x[seq_len(xlen - n)]) } 7.5 Quiz answers The most useful tool to determine where a error occurred is traceback(). Or use RStudio, which displays it automatically where an error occurs. browser() pauses execution at the specified line and allows you to enter an interactive environment. In that environment, there are five useful commands: n, execute the next command; s, step into the next function; f, finish the current loop or function; c, continue execution normally; Q, stop the function and return to the console. You could use try() or tryCatch(). Because you can then capture specific types of error with tryCatch(), rather than relying on the comparison of error strings, which is risky, especially when messages are translated. "],
["functional-programming.html", "8 Functional programming 8.1 Motivation 8.2 Anonymous functions 8.3 Closures 8.4 Lists of functions 8.5 Case study: numerical integration", " 8 Functional programming R, at its heart, is a functional programming (FP) language. This means that it provides many tools for the creation and manipulation of functions. In particular, R has what’s known as first class functions. You can do anything with functions that you can do with vectors: you can assign them to variables, store them in lists, pass them as arguments to other functions, create them inside functions, and even return them as the result of a function. The chapter starts by showing a motivating example, removing redundancy and duplication in code used to clean and summarise data. Then you’ll learn about the three building blocks of functional programming: anonymous functions, closures (functions written by functions), and lists of functions. These pieces are twined together in the conclusion which shows how to build a suite of tools for numerical integration, starting from very simple primitives. This is a recurring theme in FP: start with small, easy-to-understand building blocks, combine them into more complex structures, and apply them with confidence. The discussion of functional programming continues in the following two chapters: functionals explores functions that take functions as arguments and return vectors as output, and function operators explores functions that take functions as input and return them as output. 8.0.0.0.1 Outline Motivation motivates functional programming using a common problem: cleaning and summarising data before serious analysis. Anonymous functions shows you a side of functions that you might not have known about: you can use functions without giving them a name. Closures introduces the closure, a function written by another function. A closure can access its own arguments, and variables defined in its parent. Lists of functions shows how to put functions in a list, and explains why you might care. Numerical integration concludes the chapter with a case study that uses anonymous functions, closures and lists of functions to build a flexible toolkit for numerical integration. 8.0.0.0.2 Prequisites You should be familiar with the basic rules of lexical scoping, as described in lexical scoping. Make sure you’ve installed the pryr package with install.packages(&quot;pryr&quot;) 8.1 Motivation Imagine you’ve loaded a data file, like the one below, that uses \\(-99\\) to represent missing values. You want to replace all the \\(-99\\)s with NAs. # Generate a sample dataset set.seed(1014) df &lt;- data.frame(replicate(6, sample(c(1:10, -99), 6, rep = TRUE))) names(df) &lt;- letters[1:6] df #&gt; a b c d e f #&gt; 1 1 6 1 5 -99 1 #&gt; 2 10 4 4 -99 9 3 #&gt; 3 7 9 5 4 1 4 #&gt; 4 2 9 3 8 6 8 #&gt; 5 1 10 5 9 8 6 #&gt; 6 6 2 1 3 8 5 When you first started writing R code, you might have solved the problem with copy-and-paste: df$a[df$a == -99] &lt;- NA df$b[df$b == -99] &lt;- NA df$c[df$c == -98] &lt;- NA df$d[df$d == -99] &lt;- NA df$e[df$e == -99] &lt;- NA df$f[df$g == -99] &lt;- NA One problem with copy-and-paste is that it’s easy to make mistakes. Can you spot the two in the block above? These mistakes are inconsistencies that arose because we didn’t have an authoritative description of the desired action (replace \\(-99\\) with NA). Duplicating an action makes bugs more likely and makes it harder to change code. For example, if the code for a missing value changes from \\(-99\\) to 9999, you’d need to make the change in multiple places. To prevent bugs and to make more flexible code, adopt the “do not repeat yourself”, or DRY, principle. Popularised by the “pragmatic programmers”, Dave Thomas and Andy Hunt, this principle states: “every piece of knowledge must have a single, unambiguous, authoritative representation within a system”. FP tools are valuable because they provide tools to reduce duplication. We can start applying FP ideas by writing a function that fixes the missing values in a single vector: fix_missing &lt;- function(x) { x[x == -99] &lt;- NA x } df$a &lt;- fix_missing(df$a) df$b &lt;- fix_missing(df$b) df$c &lt;- fix_missing(df$c) df$d &lt;- fix_missing(df$d) df$e &lt;- fix_missing(df$e) df$f &lt;- fix_missing(df$e) This reduces the scope of possible mistakes, but it doesn’t eliminate them: you can no longer accidentally type -98 instead of -99, but you can still mess up the name of variable. The next step is to remove this possible source of error by combining two functions. One function, fix_missing(), knows how to fix a single vector; the other, lapply(), knows how to do something to each column in a data frame. lapply() takes three inputs: x, a list; f, a function; and ..., other arguments to pass to f(). It applies the function to each element of the list and returns a new list. lapply(x, f, ...) is equivalent to the following for loop: out &lt;- vector(&quot;list&quot;, length(x)) for (i in seq_along(x)) { out[[i]] &lt;- f(x[[i]], ...) } The real lapply() is rather more complicated since it’s implemented in C for efficiency, but the essence of the algorithm is the same. lapply() is called a functional, because it takes a function as an argument. Functionals are an important part of functional programming. You’ll learn more about them in functionals. We can apply lapply() to this problem because data frames are lists. We just need a neat little trick to make sure we get back a data frame, not a list. Instead of assigning the results of lapply() to df, we’ll assign them to df[]. R’s usual rules ensure that we get a data frame, not a list. (If this comes as a surprise, you might want to read subsetting and assignment.) Putting these pieces together gives us: fix_missing &lt;- function(x) { x[x == -99] &lt;- NA x } df[] &lt;- lapply(df, fix_missing) This code has five advantages over copy and paste: It’s more compact. If the code for a missing value changes, it only needs to be updated in one place. It works for any number of columns. There is no way to accidentally miss a column. There is no way to accidentally treat one column differently than another. It is easy to generalise this technique to a subset of columns: df[1:5] &lt;- lapply(df[1:5], fix_missing) The key idea is function composition. Take two simple functions, one which does something to every column and one which fixes missing values, and combines them to fix missing values in every column. Writing simple functions that can be understood in isolation and then composed is a powerful technique. What if different columns used different codes for missing values? You might be tempted to copy-and-paste: fix_missing_99 &lt;- function(x) { x[x == -99] &lt;- NA x } fix_missing_999 &lt;- function(x) { x[x == -999] &lt;- NA x } fix_missing_9999 &lt;- function(x) { x[x == -999] &lt;- NA x } As before, it’s easy to create bugs. Instead we could use closures, functions that make and return functions. Closures allow us to make functions based on a template: missing_fixer &lt;- function(na_value) { function(x) { x[x == na_value] &lt;- NA x } } fix_missing_99 &lt;- missing_fixer(-99) fix_missing_999 &lt;- missing_fixer(-999) fix_missing_99(c(-99, -999)) #&gt; [1] NA -999 fix_missing_999(c(-99, -999)) #&gt; [1] -99 NA Extra argument In this case, you could argue that we should just add another argument: fix_missing &lt;- function(x, na_value) { x[x == na_value] &lt;- NA x } That’s a reasonable solution here, but it doesn’t always work well in every situation. We’ll see more compelling uses for closures in MLE. Now consider a related problem. Once you’ve cleaned up your data, you might want to compute the same set of numerical summaries for each variable. You could write code like this: mean(df$a) median(df$a) sd(df$a) mad(df$a) IQR(df$a) mean(df$b) median(df$b) sd(df$b) mad(df$b) IQR(df$b) But again, you’d be better off identifying and removing duplicate items. Take a minute or two to think about how you might tackle this problem before reading on. One approach would be to write a summary function and then apply it to each column: summary &lt;- function(x) { c(mean(x), median(x), sd(x), mad(x), IQR(x)) } lapply(df, summary) That’s a great start, but there’s still some duplication. It’s easier to see if we make the summary function more realistic: summary &lt;- function(x) { c(mean(x, na.rm = TRUE), median(x, na.rm = TRUE), sd(x, na.rm = TRUE), mad(x, na.rm = TRUE), IQR(x, na.rm = TRUE)) } All five functions are called with the same arguments (x and na.rm) repeated five times. As always, duplication makes our code fragile: it’s easier to introduce bugs and harder to adapt to changing requirements. To remove this source of duplication, you can take advantage of another functional programming technique: storing functions in lists. summary &lt;- function(x) { funs &lt;- c(mean, median, sd, mad, IQR) lapply(funs, function(f) f(x, na.rm = TRUE)) } This chapter discusses these techniques in more detail. But before you can start learning them, you need to learn the simplest FP tool, the anonymous function. 8.2 Anonymous functions In R, functions are objects in their own right. They aren’t automatically bound to a name. Unlike many languages (e.g., C, C++, Python, and Ruby), R doesn’t have a special syntax for creating a named function: when you create a function, you use the regular assignment operator to give it a name. If you choose not to give the function a name, you get an anonymous function. You use an anonymous function when it’s not worth the effort to give it a name: lapply(mtcars, function(x) length(unique(x))) Filter(function(x) !is.numeric(x), mtcars) integrate(function(x) sin(x) ^ 2, 0, pi) Like all functions in R, anonymous functions have formals(), a body(), and a parent environment(): formals(function(x = 4) g(x) + h(x)) #&gt; $x #&gt; [1] 4 body(function(x = 4) g(x) + h(x)) #&gt; g(x) + h(x) environment(function(x = 4) g(x) + h(x)) #&gt; &lt;environment: R_GlobalEnv&gt; You can call an anonymous function without giving it a name, but the code is a little tricky to read because you must use parentheses in two different ways: first, to call a function, and second to make it clear that you want to call the anonymous function itself, as opposed to calling a (possibly invalid) function inside the anonymous function: # This does not call the anonymous function. # (Note that &quot;3&quot; is not a valid function.) function(x) 3() #&gt; function(x) 3() # With appropriate parenthesis, the function is called: (function(x) 3)() #&gt; [1] 3 # So this anonymous function syntax (function(x) x + 3)(10) #&gt; [1] 13 # behaves exactly the same as f &lt;- function(x) x + 3 f(10) #&gt; [1] 13 You can call anonymous functions with named arguments, but doing so is a good sign that your function needs a name. One of the most common uses for anonymous functions is to create closures, functions made by other functions. Closures are described in the next section. 8.2.1 Exercises Given a function, like &quot;mean&quot;, match.fun() lets you find a function. Given a function, can you find its name? Why doesn’t that make sense in R? Use lapply() and an anonymous function to find the coefficient of variation (the standard deviation divided by the mean) for all columns in the mtcars dataset. Use integrate() and an anonymous function to find the area under the curve for the following functions. Use Wolfram Alpha to check your answers. y = x ^ 2 - x, x in [0, 10] y = sin(x) + cos(x), x in [-\\(\\pi\\), \\(\\pi\\)] y = exp(x) / x, x in [10, 20] A good rule of thumb is that an anonymous function should fit on one line and shouldn’t need to use {}. Review your code. Where could you have used an anonymous function instead of a named function? Where should you have used a named function instead of an anonymous function? 8.3 Closures “An object is data with functions. A closure is a function with data.” — John D. Cook One use of anonymous functions is to create small functions that are not worth naming. Another important use is to create closures, functions written by functions. Closures get their name because they enclose the environment of the parent function and can access all its variables. This is useful because it allows us to have two levels of parameters: a parent level that controls operation and a child level that does the work. The following example uses this idea to generate a family of power functions in which a parent function (power()) creates two child functions (square() and cube()). power &lt;- function(exponent) { function(x) { x ^ exponent } } square &lt;- power(2) square(2) #&gt; [1] 4 square(4) #&gt; [1] 16 cube &lt;- power(3) cube(2) #&gt; [1] 8 cube(4) #&gt; [1] 64 When you print a closure, you don’t see anything terribly useful: square #&gt; function(x) { #&gt; x ^ exponent #&gt; } #&gt; &lt;environment: 0x7fbdeb392e68&gt; cube #&gt; function(x) { #&gt; x ^ exponent #&gt; } #&gt; &lt;bytecode: 0x7fbdead86948&gt; #&gt; &lt;environment: 0x7fbdeaf81ea8&gt; That’s because the function itself doesn’t change. The difference is the enclosing environment, environment(square). One way to see the contents of the environment is to convert it to a list: as.list(environment(square)) #&gt; $exponent #&gt; [1] 2 as.list(environment(cube)) #&gt; $exponent #&gt; [1] 3 Another way to see what’s going on is to use pryr::unenclose(). This function replaces variables defined in the enclosing environment with their values: library(pryr) unenclose(square) #&gt; function (x) #&gt; { #&gt; x^2 #&gt; } unenclose(cube) #&gt; function (x) #&gt; { #&gt; x^3 #&gt; } The parent environment of a closure is the execution environment of the function that created it, as shown by this code: power &lt;- function(exponent) { print(environment()) function(x) x ^ exponent } zero &lt;- power(0) #&gt; &lt;environment: 0x7fbdead42f50&gt; environment(zero) #&gt; &lt;environment: 0x7fbdead42f50&gt; The execution environment normally disappears after the function returns a value. However, functions capture their enclosing environments. This means when function a returns function b, function b captures and stores the execution environment of function a, and it doesn’t disappear. (This has important consequences for memory use, see memory usage for details.) In R, almost every function is a closure. All functions remember the environment in which they were created, typically either the global environment, if it’s a function that you’ve written, or a package environment, if it’s a function that someone else has written. The only exception is primitive functions, which call C code directly and don’t have an associated environment. Closures are useful for making function factories, and are one way to manage mutable state in R. 8.3.1 Function factories A function factory is a factory for making new functions. We’ve already seen two examples of function factories, missing_fixer() and power(). You call it with arguments that describe the desired actions, and it returns a function that will do the work for you. For missing_fixer() and power(), there’s not much benefit in using a function factory instead of a single function with multiple arguments. Function factories are most useful when: The different levels are more complex, with multiple arguments and complicated bodies. Some work only needs to be done once, when the function is generated. Function factories are particularly well suited to maximum likelihood problems, and you’ll see a more compelling use of them in mathematical functionals. 8.3.2 Mutable state Having variables at two levels allows you to maintain state across function invocations. This is possible because while the execution environment is refreshed every time, the enclosing environment is constant. The key to managing variables at different levels is the double arrow assignment operator (&lt;&lt;-). Unlike the usual single arrow assignment (&lt;-) that always assigns in the current environment, the double arrow operator will keep looking up the chain of parent environments until it finds a matching name. (Binding names to values has more details on how it works.) Together, a static parent environment and &lt;&lt;- make it possible to maintain state across function calls. The following example shows a counter that records how many times a function has been called. Each time new_counter is run, it creates an environment, initialises the counter i in this environment, and then creates a new function. new_counter &lt;- function() { i &lt;- 0 function() { i &lt;&lt;- i + 1 i } } The new function is a closure, and its enclosing environment is the environment created when new_counter() is run. Ordinarily, function execution environments are temporary, but a closure maintains access to the environment in which it was created. In the example below, closures counter_one() and counter_two() each get their own enclosing environments when run, so they can maintain different counts. counter_one &lt;- new_counter() counter_two &lt;- new_counter() counter_one() #&gt; [1] 1 counter_one() #&gt; [1] 2 counter_two() #&gt; [1] 1 The counters get around the “fresh start” limitation by not modifying variables in their local environment. Since the changes are made in the unchanging parent (or enclosing) environment, they are preserved across function calls. What happens if you don’t use a closure? What happens if you use &lt;- instead of &lt;&lt;-? Make predictions about what will happen if you replace new_counter() with the variants below, then run the code and check your predictions. i &lt;- 0 new_counter2 &lt;- function() { i &lt;&lt;- i + 1 i } new_counter3 &lt;- function() { i &lt;- 0 function() { i &lt;- i + 1 i } } Modifying values in a parent environment is an important technique because it is one way to generate “mutable state” in R. Mutable state is normally hard because every time it looks like you’re modifying an object, you’re actually creating and then modifying a copy. However, if you do need mutable objects and your code is not very simple, it’s usually better to use reference classes, as described in RC. The power of closures is tightly coupled with the more advanced ideas in functionals and function operators. You’ll see many more closures in those two chapters. The following section discusses the third technique of functional programming in R: the ability to store functions in a list. 8.3.3 Exercises Why are functions created by other functions called closures? What does the following statistical function do? What would be a better name for it? (The existing name is a bit of a hint.) bc &lt;- function(lambda) { if (lambda == 0) { function(x) log(x) } else { function(x) (x ^ lambda - 1) / lambda } } What does approxfun() do? What does it return? What does ecdf() do? What does it return? Create a function that creates functions that compute the ith central moment of a numeric vector. You can test it by running the following code: m1 &lt;- moment(1) m2 &lt;- moment(2) x &lt;- runif(100) stopifnot(all.equal(m1(x), 0)) stopifnot(all.equal(m2(x), var(x) * 99 / 100)) Create a function pick() that takes an index, i, as an argument and returns a function with an argument x that subsets x with i. lapply(mtcars, pick(5)) # should do the same as this lapply(mtcars, function(x) x[[5]]) 8.4 Lists of functions In R, functions can be stored in lists. This makes it easier to work with groups of related functions, in the same way a data frame makes it easier to work with groups of related vectors. We’ll start with a simple benchmarking example. Imagine you are comparing the performance of multiple ways of computing the arithmetic mean. You could do this by storing each approach (function) in a list: compute_mean &lt;- list( base = function(x) mean(x), sum = function(x) sum(x) / length(x), manual = function(x) { total &lt;- 0 n &lt;- length(x) for (i in seq_along(x)) { total &lt;- total + x[i] / n } total } ) Calling a function from a list is straightforward. You extract it then call it: x &lt;- runif(1e5) system.time(compute_mean$base(x)) #&gt; User System verstrichen #&gt; 0.001 0.000 0.000 system.time(compute_mean[[2]](x)) #&gt; User System verstrichen #&gt; 0.001 0.000 0.000 system.time(compute_mean[[&quot;manual&quot;]](x)) #&gt; User System verstrichen #&gt; 0.009 0.000 0.009 To call each function (e.g., to check that they all return the same results), use lapply(). We’ll need either an anonymous function or a new named function, since there isn’t a built-in function to handle this situation. lapply(compute_mean, function(f) f(x)) #&gt; $base #&gt; [1] 0.499 #&gt; #&gt; $sum #&gt; [1] 0.499 #&gt; #&gt; $manual #&gt; [1] 0.499 call_fun &lt;- function(f, ...) f(...) lapply(compute_mean, call_fun, x) #&gt; $base #&gt; [1] 0.499 #&gt; #&gt; $sum #&gt; [1] 0.499 #&gt; #&gt; $manual #&gt; [1] 0.499 To time each function, we can combine lapply() and system.time(): lapply(compute_mean, function(f) system.time(f(x))) #&gt; $base #&gt; User System verstrichen #&gt; 0.000 0.000 0.001 #&gt; #&gt; $sum #&gt; User System verstrichen #&gt; 0 0 0 #&gt; #&gt; $manual #&gt; User System verstrichen #&gt; 0.005 0.000 0.006 Another use for a list of functions is to summarise an object in multiple ways. To do that, we could store each summary function in a list, and then run them all with lapply(): x &lt;- 1:10 funs &lt;- list( sum = sum, mean = mean, median = median ) lapply(funs, function(f) f(x)) #&gt; $sum #&gt; [1] 55 #&gt; #&gt; $mean #&gt; [1] 5.5 #&gt; #&gt; $median #&gt; [1] 5.5 What if we wanted our summary functions to automatically remove missing values? One approach would be to make a list of anonymous functions that call our summary functions with the appropriate arguments: funs2 &lt;- list( sum = function(x, ...) sum(x, ..., na.rm = TRUE), mean = function(x, ...) mean(x, ..., na.rm = TRUE), median = function(x, ...) median(x, ..., na.rm = TRUE) ) lapply(funs2, function(f) f(x)) #&gt; $sum #&gt; [1] 55 #&gt; #&gt; $mean #&gt; [1] 5.5 #&gt; #&gt; $median #&gt; [1] 5.5 This, however, leads to a lot of duplication. Apart from a different function name, each function is almost identical. A better approach would be to modify our lapply() call to include the extra argument: lapply(funs, function(f) f(x, na.rm = TRUE)) 8.4.1 Moving lists of functions to the global environment From time to time you may create a list of functions that you want to be available without having to use a special syntax. For example, imagine you want to create HTML code by mapping each tag to an R function. The following example uses a function factory to create functions for the tags &lt;p&gt; (paragraph), &lt;b&gt; (bold), and &lt;i&gt; (italics). simple_tag &lt;- function(tag) { force(tag) function(...) { paste0(&quot;&lt;&quot;, tag, &quot;&gt;&quot;, paste0(...), &quot;&lt;/&quot;, tag, &quot;&gt;&quot;) } } tags &lt;- c(&quot;p&quot;, &quot;b&quot;, &quot;i&quot;) html &lt;- lapply(setNames(tags, tags), simple_tag) I’ve put the functions in a list because I don’t want them to be available all the time. The risk of a conflict between an existing R function and an HTML tag is high. But keeping them in a list makes code more verbose: html$p(&quot;This is &quot;, html$b(&quot;bold&quot;), &quot; text.&quot;) #&gt; [1] &quot;&lt;p&gt;This is &lt;b&gt;bold&lt;/b&gt; text.&lt;/p&gt;&quot; Depending on how long we want the effect to last, you have three options to eliminate the use of html$: For a very temporary effect, you can use with(): with(html, p(&quot;This is &quot;, b(&quot;bold&quot;), &quot; text.&quot;)) #&gt; [1] &quot;&lt;p&gt;This is &lt;b&gt;bold&lt;/b&gt; text.&lt;/p&gt;&quot; For a longer effect, you can attach() the functions to the search path, then detach() when you’re done: attach(html) p(&quot;This is &quot;, b(&quot;bold&quot;), &quot; text.&quot;) #&gt; [1] &quot;&lt;p&gt;This is &lt;b&gt;bold&lt;/b&gt; text.&lt;/p&gt;&quot; detach(html) Finally, you could copy the functions to the global environment with list2env(). You can undo this by deleting the functions after you’re done. list2env(html, environment()) #&gt; &lt;environment: R_GlobalEnv&gt; p(&quot;This is &quot;, b(&quot;bold&quot;), &quot; text.&quot;) #&gt; [1] &quot;&lt;p&gt;This is &lt;b&gt;bold&lt;/b&gt; text.&lt;/p&gt;&quot; rm(list = names(html), envir = environment()) I recommend the first option, using with(), because it makes it very clear when code is being executed in a special context and what that context is. 8.4.2 Exercises Implement a summary function that works like base::summary(), but uses a list of functions. Modify the function so it returns a closure, making it possible to use it as a function factory. Which of the following commands is equivalent to with(x, f(z))? x$f(x$z). f(x$z). x$f(z). f(z). It depends. 8.5 Case study: numerical integration To conclude this chapter, I’ll develop a simple numerical integration tool using first-class functions. Each step in the development of the tool is driven by a desire to reduce duplication and to make the approach more general. The idea behind numerical integration is simple: find the area under a curve by approximating the curve with simpler components. The two simplest approaches are the midpoint and trapezoid rules. The midpoint rule approximates a curve with a rectangle. The trapezoid rule uses a trapezoid. Each takes the function we want to integrate, f, and a range of values, from a to b, to integrate over. For this example, I’ll try to integrate sin x from 0 to \\(\\pi\\). This is a good choice for testing because it has a simple answer: 2. midpoint &lt;- function(f, a, b) { (b - a) * f((a + b) / 2) } trapezoid &lt;- function(f, a, b) { (b - a) / 2 * (f(a) + f(b)) } midpoint(sin, 0, pi) #&gt; [1] 3.14 trapezoid(sin, 0, pi) #&gt; [1] 1.92e-16 Neither of these functions gives a very good approximation. To make them more accurate using the idea that underlies calculus: we’ll break up the range into smaller pieces and integrate each piece using one of the simple rules. This is called composite integration. I’ll implement it using two new functions: midpoint_composite &lt;- function(f, a, b, n = 10) { points &lt;- seq(a, b, length = n + 1) h &lt;- (b - a) / n area &lt;- 0 for (i in seq_len(n)) { area &lt;- area + h * f((points[i] + points[i + 1]) / 2) } area } trapezoid_composite &lt;- function(f, a, b, n = 10) { points &lt;- seq(a, b, length = n + 1) h &lt;- (b - a) / n area &lt;- 0 for (i in seq_len(n)) { area &lt;- area + h / 2 * (f(points[i]) + f(points[i + 1])) } area } midpoint_composite(sin, 0, pi, n = 10) #&gt; [1] 2.01 midpoint_composite(sin, 0, pi, n = 100) #&gt; [1] 2 trapezoid_composite(sin, 0, pi, n = 10) #&gt; [1] 1.98 trapezoid_composite(sin, 0, pi, n = 100) #&gt; [1] 2 You’ll notice that there’s a lot of duplication between midpoint_composite() and trapezoid_composite(). Apart from the internal rule used to integrate over a range, they are basically the same. From these specific functions you can extract a more general composite integration function: composite &lt;- function(f, a, b, n = 10, rule) { points &lt;- seq(a, b, length = n + 1) area &lt;- 0 for (i in seq_len(n)) { area &lt;- area + rule(f, points[i], points[i + 1]) } area } composite(sin, 0, pi, n = 10, rule = midpoint) #&gt; [1] 2.01 composite(sin, 0, pi, n = 10, rule = trapezoid) #&gt; [1] 1.98 This function takes two functions as arguments: the function to integrate and the integration rule. We can now add even better rules for integrating over smaller ranges: simpson &lt;- function(f, a, b) { (b - a) / 6 * (f(a) + 4 * f((a + b) / 2) + f(b)) } boole &lt;- function(f, a, b) { pos &lt;- function(i) a + i * (b - a) / 4 fi &lt;- function(i) f(pos(i)) (b - a) / 90 * (7 * fi(0) + 32 * fi(1) + 12 * fi(2) + 32 * fi(3) + 7 * fi(4)) } composite(sin, 0, pi, n = 10, rule = simpson) #&gt; [1] 2 composite(sin, 0, pi, n = 10, rule = boole) #&gt; [1] 2 It turns out that the midpoint, trapezoid, Simpson, and Boole rules are all examples of a more general family called Newton-Cotes rules. (They are polynomials of increasing complexity.) We can use this common structure to write a function that can generate any general Newton-Cotes rule: newton_cotes &lt;- function(coef, open = FALSE) { n &lt;- length(coef) + open function(f, a, b) { pos &lt;- function(i) a + i * (b - a) / n points &lt;- pos(seq.int(0, length(coef) - 1)) (b - a) / sum(coef) * sum(f(points) * coef) } } boole &lt;- newton_cotes(c(7, 32, 12, 32, 7)) milne &lt;- newton_cotes(c(2, -1, 2), open = TRUE) composite(sin, 0, pi, n = 10, rule = milne) #&gt; [1] 1.99 Mathematically, the next step in improving numerical integration is to move from a grid of evenly spaced points to a grid where the points are closer together near the end of the range, such as Gaussian quadrature. That’s beyond the scope of this case study, but you could implement it with similar techniques. 8.5.1 Exercises Instead of creating individual functions (e.g., midpoint(), trapezoid(), simpson(), etc.), we could store them in a list. If we did that, how would that change the code? Can you create the list of functions from a list of coefficients for the Newton-Cotes formulae? The trade-off between integration rules is that more complex rules are slower to compute, but need fewer pieces. For sin() in the range [0, \\(\\pi\\)], determine the number of pieces needed so that each rule will be equally accurate. Illustrate your results with a graph. How do they change for different functions? sin(1 / x^2) is particularly challenging. "],
["functionals.html", "9 Functionals 9.1 My first functional: lapply() 9.2 For loop functionals: friends of lapply() 9.3 Manipulating matrices and data frames 9.4 Manipulating lists 9.5 Mathematical functionals 9.6 Loops that should be left as is 9.7 A family of functions", " 9 Functionals “To become significantly more reliable, code must become more transparent. In particular, nested conditions and loops must be viewed with great suspicion. Complicated control flows confuse programmers. Messy code often hides bugs.” — Bjarne Stroustrup A higher-order function is a function that takes a function as an input or returns a function as output. We’ve already seen one type of higher order function: closures, functions returned by another function. The complement to a closure is a functional, a function that takes a function as an input and returns a vector as output. Here’s a simple functional: it calls the function provided as input with 1000 random uniform numbers. randomise &lt;- function(f) f(runif(1e3)) randomise(mean) #&gt; [1] 0.506 randomise(mean) #&gt; [1] 0.501 randomise(sum) #&gt; [1] 489 The chances are that you’ve already used a functional: the three most frequently used are lapply(), apply(), and tapply(). All three take a function as input (among other things) and return a vector as output. A common use of functionals is as an alternative to for loops. For loops have a bad rap in R. They have a reputation for being slow (although that reputation is only partly true, see modification in place for more details). But the real downside of for loops is that they’re not very expressive. A for loop conveys that it’s iterating over something, but doesn’t clearly convey a high level goal. Instead of using a for loop, it’s better to use a functional. Each functional is tailored for a specific task, so when you recognise the functional you know immediately why it’s being used. Functionals play other roles as well as replacements for for-loops. They are useful for encapsulating common data manipulation tasks like split-apply-combine, for thinking “functionally”, and for working with mathematical functions. Functionals reduce bugs in your code by better communicating intent. Functionals implemented in base R are well tested (i.e., bug-free) and efficient, because they’re used by so many people. Many are written in C, and use special tricks to enhance performance. That said, using functionals will not always produce the fastest code. Instead, it helps you clearly communicate and build tools that solve a wide range of problems. It’s a mistake to focus on speed until you know it’ll be a problem. Once you have clear, correct code you can make it fast using the techniques you’ll learn in improving the speed of your code. 9.0.0.0.1 Outline My first functional: lapply() introduces your first functional: lapply(). For loop functionals shows you variants of lapply() that produce different outputs, take different inputs, and distribute computation in different ways. Data structure functionals discusses functionals that work with more complex data structures like matrices and arrays. Functional programming teaches you about the powerful Reduce() and Filter() functions which are useful for working with lists. Mathematical functionals discusses functionals that you might be familiar with from mathematics, like root finding, integration, and optimisation. Loops that shouldn’t be converted to functions provides some important caveats about when you shouldn’t attempt to convert a loop into a functional. A family of functions finishes off the chapter by showing you how functionals can take a simple building block and use it to create a set of powerful and consistent tools. 9.0.0.0.2 Prerequisites You’ll use closures frequently used in conjunction with functionals. If you need a refresher, review closures. 9.1 My first functional: lapply() The simplest functional is lapply(), which you may already be familiar with. lapply() takes a function, applies it to each element in a list, and returns the results in the form of a list. lapply() is the building block for many other functionals, so it’s important to understand how it works. Here’s a pictorial representation: lapply() is written in C for performance, but we can create a simple R implementation that does the same thing: lapply2 &lt;- function(x, f, ...) { out &lt;- vector(&quot;list&quot;, length(x)) for (i in seq_along(x)) { out[[i]] &lt;- f(x[[i]], ...) } out } From this code, you can see that lapply() is a wrapper for a common for loop pattern: create a container for output, apply f() to each component of a list, and fill the container with the results. All other for loop functionals are variations on this theme: they simply use different types of input or output. lapply() makes it easier to work with lists by eliminating much of the boilerplate associated with looping. This allows you to focus on the function that you’re applying: # Create some random data l &lt;- replicate(20, runif(sample(1:10, 1)), simplify = FALSE) # With a for loop out &lt;- vector(&quot;list&quot;, length(l)) for (i in seq_along(l)) { out[[i]] &lt;- length(l[[i]]) } unlist(out) #&gt; [1] 3 1 1 2 2 10 5 9 7 2 4 10 8 2 9 7 3 2 2 8 # With lapply unlist(lapply(l, length)) #&gt; [1] 3 1 1 2 2 10 5 9 7 2 4 10 8 2 9 7 3 2 2 8 (I’m using unlist() to convert the output from a list to a vector to make it more compact. We’ll see other ways of making the output a vector shortly.) Since data frames are also lists, lapply() is also useful when you want to do something to each column of a data frame: # What class is each column? unlist(lapply(mtcars, class)) #&gt; mpg cyl disp hp drat wt qsec #&gt; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; #&gt; vs am gear carb #&gt; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; # Divide each column by the mean mtcars[] &lt;- lapply(mtcars, function(x) x / mean(x)) The pieces of x are always supplied as the first argument to f. If you want to vary a different argument, you can use an anonymous function. The following example varies the amount of trimming applied when computing the mean of a fixed x. trims &lt;- c(0, 0.1, 0.2, 0.5) x &lt;- rcauchy(1000) unlist(lapply(trims, function(trim) mean(x, trim = trim))) #&gt; [1] 0.2879 0.0790 0.0535 0.0502 9.1.1 Looping patterns It’s useful to remember that there are three basic ways to loop over a vector: loop over the elements: for (x in xs) loop over the numeric indices: for (i in seq_along(xs)) loop over the names: for (nm in names(xs)) The first form is usually not a good choice for a for loop because it leads to inefficient ways of saving output. With this form it’s very natural to save the output by extending a data structure, like in this example: xs &lt;- runif(1e3) res &lt;- c() for (x in xs) { # This is slow! res &lt;- c(res, sqrt(x)) } This is slow because each time you extend the vector, R has to copy all of the existing elements. Avoid copies discusses this problem in more depth. Instead, it’s much better to create the space you’ll need for the output and then fill it in. This is easiest with the second form: res &lt;- numeric(length(xs)) for (i in seq_along(xs)) { res[i] &lt;- sqrt(xs[i]) } Just as there are three basic ways to use a for loop, there are three basic ways to use lapply(): lapply(xs, function(x) {}) lapply(seq_along(xs), function(i) {}) lapply(names(xs), function(nm) {}) Typically you’d use the first form because lapply() takes care of saving the output for you. However, if you need to know the position or name of the element you’re working with, you should use the second or third form. Both give you an element’s position (i, nm) and value (xs[[i]], xs[[nm]]). If you’re struggling to solve a problem using one form, you might find it easier with another. 9.1.2 Exercises Why are the following two invocations of lapply() equivalent? trims &lt;- c(0, 0.1, 0.2, 0.5) x &lt;- rcauchy(100) lapply(trims, function(trim) mean(x, trim = trim)) lapply(trims, mean, x = x) The function below scales a vector so it falls in the range [0, 1]. How would you apply it to every column of a data frame? How would you apply it to every numeric column in a data frame? scale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list: formulas &lt;- list( mpg ~ disp, mpg ~ I(1 / disp), mpg ~ disp + wt, mpg ~ I(1 / disp) + wt ) Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply(). Can you do it without an anonymous function? bootstraps &lt;- lapply(1:10, function(i) { rows &lt;- sample(1:nrow(mtcars), rep = TRUE) mtcars[rows, ] }) For each model in the previous two exercises, extract \\(R^2\\) using the function below. rsq &lt;- function(mod) summary(mod)$r.squared 9.2 For loop functionals: friends of lapply() The key to using functionals in place of for loops is recognising that common looping patterns are already implemented in existing base functionals. Once you’ve mastered these existing functionals, the next step is to start writing your own: if you discover you’re duplicating the same looping pattern in many places, you should extract it out into its own function. The following sections build on lapply() and discuss: sapply() and vapply(), variants of lapply() that produce vectors, matrices, and arrays as output, instead of lists. Map() and mapply() which iterate over multiple input data structures in parallel. mclapply() and mcMap(), parallel versions of lapply() and Map(). Writing a new function, rollapply(), to solve a new problem. 9.2.1 Vector output: sapply and vapply sapply() and vapply() are very similar to lapply() except they simplify their output to produce an atomic vector. While sapply() guesses, vapply() takes an additional argument specifying the output type. sapply() is great for interactive use because it saves typing, but if you use it inside your functions you’ll get weird errors if you supply the wrong type of input. vapply() is more verbose, but gives more informative error messages and never fails silently. It is better suited for use inside other functions. The following example illustrates these differences. When given a data frame, sapply() and vapply() return the same results. When given an empty list, sapply() returns another empty list instead of the more correct zero-length logical vector. sapply(mtcars, is.numeric) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE vapply(mtcars, is.numeric, logical(1)) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE sapply(list(), is.numeric) #&gt; list() vapply(list(), is.numeric, logical(1)) #&gt; logical(0) If the function returns results of different types or lengths, sapply() will silently return a list, while vapply() will throw an error. sapply() is fine for interactive use because you’ll normally notice if something goes wrong, but it’s dangerous when writing functions. The following example illustrates a possible problem when extracting the class of columns in a data frame: if you falsely assume that class only has one value and use sapply(), you won’t find out about the problem until some future function is given a list instead of a character vector. df &lt;- data.frame(x = 1:10, y = letters[1:10]) sapply(df, class) #&gt; x y #&gt; &quot;integer&quot; &quot;factor&quot; vapply(df, class, character(1)) #&gt; x y #&gt; &quot;integer&quot; &quot;factor&quot; df2 &lt;- data.frame(x = 1:10, y = Sys.time() + 1:10) sapply(df2, class) #&gt; $x #&gt; [1] &quot;integer&quot; #&gt; #&gt; $y #&gt; [1] &quot;POSIXct&quot; &quot;POSIXt&quot; vapply(df2, class, character(1)) #&gt; Error in vapply(df2, class, character(1)): Werte müssen die Länge 1 haben, #&gt; Ergebnis von FUN(X[[2]]) hat aber Länge 2 sapply() is a thin wrapper around lapply() that transforms a list into a vector in the final step. vapply() is an implementation of lapply() that assigns results to a vector (or matrix) of appropriate type instead of as a list. The following code shows a pure R implementation of the essence of sapply() and vapply() (the real functions have better error handling and preserve names, among other things). sapply2 &lt;- function(x, f, ...) { res &lt;- lapply2(x, f, ...) simplify2array(res) } vapply2 &lt;- function(x, f, f.value, ...) { out &lt;- matrix(rep(f.value, length(x)), nrow = length(f.value)) for (i in seq_along(x)) { res &lt;- f(x[[i]], ...) stopifnot( length(res) == length(f.value), typeof(res) == typeof(f.value) ) out[ ,i] &lt;- res } out } vapply() and sapply() have different outputs from lapply(). The following section discusses Map(), which has different inputs. 9.2.2 Multiple inputs: Map (and mapply) With lapply(), only one argument to the function varies; the others are fixed. This makes it poorly suited for some problems. For example, how would you find a weighted mean when you have two lists, one of observations and the other of weights? # Generate some sample data xs &lt;- replicate(5, runif(10), simplify = FALSE) ws &lt;- replicate(5, rpois(10, 5) + 1, simplify = FALSE) It’s easy to use lapply() to compute the unweighted means: unlist(lapply(xs, mean)) #&gt; [1] 0.678 0.445 0.427 0.469 0.560 But how could we supply the weights to weighted.mean()? lapply(x, means, w) won’t work because the additional arguments to lapply() are passed to every call. We could change looping forms: unlist(lapply(seq_along(xs), function(i) { weighted.mean(xs[[i]], ws[[i]]) })) #&gt; [1] 0.695 0.464 0.403 0.501 0.521 This works, but it’s a little clumsy. A cleaner alternative is to use Map, a variant of lapply(), where all arguments can vary. This lets us write: unlist(Map(weighted.mean, xs, ws)) #&gt; [1] 0.695 0.464 0.403 0.501 0.521 Note that the order of arguments is a little different: function is the first argument for Map() and the second for lapply(). This is equivalent to: stopifnot(length(xs) == length(ws)) out &lt;- vector(&quot;list&quot;, length(xs)) for (i in seq_along(xs)) { out[[i]] &lt;- weighted.mean(xs[[i]], ws[[i]]) } There’s a natural equivalence between Map() and lapply() because you can always convert a Map() to an lapply() that iterates over indices. But using Map() is more concise, and more clearly indicates what you’re trying to do. Map is useful whenever you have two (or more) lists (or data frames) that you need to process in parallel. For example, another way of standardising columns is to first compute the means and then divide by them. We could do this with lapply(), but if we do it in two steps, we can more easily check the results at each step, which is particularly important if the first step is more complicated. mtmeans &lt;- lapply(mtcars, mean) mtmeans[] &lt;- Map(`/`, mtcars, mtmeans) # In this case, equivalent to mtcars[] &lt;- lapply(mtcars, function(x) x / mean(x)) If some of the arguments should be fixed and constant, use an anonymous function: Map(function(x, w) weighted.mean(x, w, na.rm = TRUE), xs, ws) We’ll see a more compact way to express the same idea in the next chapter. mapply You may be more familiar with mapply() than Map(). I prefer Map() because: It’s equivalent to mapply with simplify = FALSE, which is almost always what you want. Instead of using an anonymous function to provide constant inputs, mapply has the MoreArgs argument that takes a list of extra arguments that will be supplied, as is, to each call. This breaks R’s usual lazy evaluation semantics, and is inconsistent with other functions. In brief, mapply() adds more complication for little gain. 9.2.3 Rolling computations What if you need a for loop replacement that doesn’t exist in base R? You can often create your own by recognising common looping structures and implementing your own wrapper. For example, you might be interested in smoothing your data using a rolling (or running) mean function: rollmean &lt;- function(x, n) { out &lt;- rep(NA, length(x)) offset &lt;- trunc(n / 2) for (i in (offset + 1):(length(x) - n + offset + 1)) { out[i] &lt;- mean(x[(i - offset):(i + offset - 1)]) } out } x &lt;- seq(1, 3, length = 1e2) + runif(1e2) plot(x) lines(rollmean(x, 5), col = &quot;blue&quot;, lwd = 2) lines(rollmean(x, 10), col = &quot;red&quot;, lwd = 2) But if the noise was more variable (i.e., it has a longer tail), you might worry that your rolling mean was too sensitive to outliers. Instead, you might want to compute a rolling median. x &lt;- seq(1, 3, length = 1e2) + rt(1e2, df = 2) / 3 plot(x) lines(rollmean(x, 5), col = &quot;red&quot;, lwd = 2) To change rollmean() to rollmedian(), all you need to do is replace mean with median inside the loop. But instead of copying and pasting to create a new function, we could extract the idea of computing a rolling summary into its own function: rollapply &lt;- function(x, n, f, ...) { out &lt;- rep(NA, length(x)) offset &lt;- trunc(n / 2) for (i in (offset + 1):(length(x) - n + offset + 1)) { out[i] &lt;- f(x[(i - offset):(i + offset - 1)], ...) } out } plot(x) lines(rollapply(x, 5, median), col = &quot;red&quot;, lwd = 2) You might notice that the internal loop looks pretty similar to a vapply() loop, so we could rewrite the function as: rollapply &lt;- function(x, n, f, ...) { offset &lt;- trunc(n / 2) locs &lt;- (offset + 1):(length(x) - n + offset + 1) num &lt;- vapply( locs, function(i) f(x[(i - offset):(i + offset)], ...), numeric(1) ) c(rep(NA, offset), num) } This is effectively the same as the implementation in zoo::rollapply(), which provides many more features and much more error checking. 9.2.4 Parallelisation One interesting thing about the implementation of lapply() is that because each iteration is isolated from all others, the order in which they are computed doesn’t matter. For example, lapply3() scrambles the order of computation, but the results are always the same: lapply3 &lt;- function(x, f, ...) { out &lt;- vector(&quot;list&quot;, length(x)) for (i in sample(seq_along(x))) { out[[i]] &lt;- f(x[[i]], ...) } out } unlist(lapply(1:10, sqrt)) #&gt; [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 unlist(lapply3(1:10, sqrt)) #&gt; [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 This has a very important consequence: since we can compute each element in any order, it’s easy to dispatch the tasks to different cores, and compute them in parallel. This is what parallel::mclapply() (and parallel::mcMap()) does. (These functions are not available in Windows, but you can use the similar parLapply() with a bit more work. See parallelise for more details.) library(parallel) unlist(mclapply(1:10, sqrt, mc.cores = 4)) #&gt; [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 In this case, mclapply() is actually slower than lapply(). This is because the cost of the individual computations is low, and additional work is needed to send the computation to the different cores and to collect the results. If we take a more realistic example, generating bootstrap replicates of a linear model for example, the advantages are clearer: boot_df &lt;- function(x) x[sample(nrow(x), rep = T), ] rsquared &lt;- function(mod) summary(mod)$r.square boot_lm &lt;- function(i) { rsquared(lm(mpg ~ wt + disp, data = boot_df(mtcars))) } system.time(lapply(1:500, boot_lm)) #&gt; User System verstrichen #&gt; 0.567 0.008 0.580 system.time(mclapply(1:500, boot_lm, mc.cores = 2)) #&gt; User System verstrichen #&gt; 0.003 0.007 0.326 While increasing the number of cores will not always lead to linear improvement, switching from lapply() or Map() to its parallelised forms can dramatically improve computational performance. 9.2.5 Exercises Use vapply() to: Compute the standard deviation of every column in a numeric data frame. Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you’ll need to use vapply() twice.) Why is using sapply() to get the class() of each element in a data frame dangerous? The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial. trials &lt;- replicate( 100, t.test(rpois(10, 10), rpois(7, 10)), simplify = FALSE ) Extra challenge: get rid of the anonymous function by using [[ directly. What does replicate() do? What sort of for loop does it eliminate? Why do its arguments differ from lapply() and friends? Implement a version of lapply() that supplies FUN with both the name and the value of each component. Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take? Implement mcsapply(), a multicore version of sapply(). Can you implement mcvapply(), a parallel version of vapply()? Why or why not? 9.3 Manipulating matrices and data frames Functionals can also be used to eliminate loops in common data manipulation tasks. In this section, we’ll give a brief overview of the available options, hint at how they can help you, and point you in the right direction to learn more. We’ll cover three categories of data structure functionals: apply(), sweep(), and outer() work with matrices. tapply() summarises a vector by groups defined by another vector. the plyr package, which generalises tapply() to make it easy to work with data frames, lists, or arrays as inputs, and data frames, lists, or arrays as outputs. 9.3.1 Matrix and array operations So far, all the functionals we’ve seen work with 1d input structures. The three functionals in this section provide useful tools for working with higher-dimensional data structures. apply() is a variant of sapply() that works with matrices and arrays. You can think of it as an operation that summarises a matrix or array by collapsing each row or column to a single number. It has four arguments: X, the matrix or array to summarise MARGIN, an integer vector giving the dimensions to summarise over, 1 = rows, 2 = columns, etc. FUN, a summary function ... other arguments passed on to FUN A typical example of apply() looks like this a &lt;- matrix(1:20, nrow = 5) apply(a, 1, mean) #&gt; [1] 8.5 9.5 10.5 11.5 12.5 apply(a, 2, mean) #&gt; [1] 3 8 13 18 There are a few caveats to using apply(). It doesn’t have a simplify argument, so you can never be completely sure what type of output you’ll get. This means that apply() is not safe to use inside a function unless you carefully check the inputs. apply() is also not idempotent in the sense that if the summary function is the identity operator, the output is not always the same as the input: a1 &lt;- apply(a, 1, identity) identical(a, a1) #&gt; [1] FALSE identical(a, t(a1)) #&gt; [1] TRUE a2 &lt;- apply(a, 2, identity) identical(a, a2) #&gt; [1] TRUE (You can put high-dimensional arrays back in the right order using aperm(), or use plyr::aaply(), which is idempotent.) sweep() allows you to “sweep” out the values of a summary statistic. It is often used with apply() to standardise arrays. The following example scales the rows of a matrix so that all values lie between 0 and 1. x &lt;- matrix(rnorm(20, 0, 10), nrow = 4) x1 &lt;- sweep(x, 1, apply(x, 1, min), `-`) x2 &lt;- sweep(x1, 1, apply(x1, 1, max), `/`) The final matrix functional is outer(). It’s a little different in that it takes multiple vector inputs and creates a matrix or array output where the input function is run over every combination of the inputs: # Create a times table outer(1:3, 1:10, &quot;*&quot;) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 1 2 3 4 5 6 7 8 9 10 #&gt; [2,] 2 4 6 8 10 12 14 16 18 20 #&gt; [3,] 3 6 9 12 15 18 21 24 27 30 Good places to learn more about apply() and friends are: “Using apply, sapply, lapply in R” by Peter Werner. “The infamous apply function” by Slawa Rokicki. “The R apply function - a tutorial with examples” by axiomOfChoice. The stackoverflow question “R Grouping functions: sapply vs. lapply vs. apply vs. tapply vs. by vs. aggregate”. 9.3.2 Group apply You can think about tapply() as a generalisation to apply() that allows for “ragged” arrays, arrays where each row can have a different number of columns. This is often needed when you’re trying to summarise a data set. For example, imagine you’ve collected pulse rate data from a medical trial, and you want to compare the two groups: pulse &lt;- round(rnorm(22, 70, 10 / 3)) + rep(c(0, 5), c(10, 12)) group &lt;- rep(c(&quot;A&quot;, &quot;B&quot;), c(10, 12)) tapply(pulse, group, length) #&gt; A B #&gt; 10 12 tapply(pulse, group, mean) #&gt; A B #&gt; 70.5 75.0 tapply() works by creating a “ragged” data structure from a set of inputs, and then applying a function to the individual elements of that structure. The first task is actually what the split() function does. It takes two inputs and returns a list which groups elements together from the first vector according to elements, or categories, from the second vector: split(pulse, group) #&gt; $A #&gt; [1] 69 71 74 66 71 67 73 69 73 72 #&gt; #&gt; $B #&gt; [1] 73 79 74 72 74 76 76 68 77 74 79 78 Then tapply() is just the combination of split() and sapply(): tapply2 &lt;- function(x, group, f, ..., simplify = TRUE) { pieces &lt;- split(x, group) sapply(pieces, f, simplify = simplify) } tapply2(pulse, group, length) #&gt; A B #&gt; 10 12 tapply2(pulse, group, mean) #&gt; A B #&gt; 70.5 75.0 Being able to rewrite tapply() as a combination of split() and sapply() is a good indication that we’ve identified some useful building blocks. 9.3.3 The plyr package One challenge with using the base functionals is that they have grown organically over time, and have been written by multiple authors. This means that they are not very consistent: With tapply() and sapply(), the simplify argument is called simplify. With mapply(), it’s called SIMPLIFY. With apply(), the argument is absent. vapply() is a variant of sapply() that allows you to describe what the output should be, but there are no corresponding variants for tapply(), apply(), or Map(). The first argument of most base functionals is a vector, but the first argument in Map() is a function. This makes learning these operators challenging, as you have to memorise all of the variations. Additionally, if you think about the possible combinations of input and output types, base R only covers a partial set of cases: list data frame array list lapply() sapply() data frame by() array apply() This was one of the driving motivations behind the creation of the plyr package. It provides consistently named functions with consistently named arguments and covers all combinations of input and output data structures: list data frame array list llply() ldply() laply() data frame dlply() ddply() daply() array alply() adply() aaply() Each of these functions splits up the input, applies a function to each piece, and then combines the results. Overall, this process is called “split-apply-combine”. You can read more about it and plyr in “The Split-Apply-Combine Strategy for Data Analysis”, an open-access article published in the Journal of Statistical Software. 9.3.4 Exercises How does apply() arrange the output? Read the documentation and perform some experiments. There’s no equivalent to split() + vapply(). Should there be? When would it be useful? Implement one yourself. Implement a pure R version of split(). (Hint: use unique() and subsetting.) Can you do it without a for loop? What other types of input and output are missing? Brainstorm before you look up some answers in the plyr paper. 9.4 Manipulating lists Another way of thinking about functionals is as a set of general tools for altering, subsetting, and collapsing lists. Every functional programming language has three tools for this: Map(), Reduce(), and Filter(). We’ve seen Map() already, and the following sections describe Reduce(), a powerful tool for extending two-argument functions, and Filter(), a member of an important class of functionals that work with predicates, functions that return a single TRUE or FALSE. 9.4.1 Reduce() Reduce() reduces a vector, x, to a single value by recursively calling a function, f, two arguments at a time. It combines the first two elements with f, then combines the result of that call with the third element, and so on. Calling Reduce(f, 1:3) is equivalent to f(f(1, 2), 3). Reduce is also known as fold, because it folds together adjacent elements in the list. The following two examples show what Reduce does with an infix and prefix function: Reduce(`+`, 1:3) # -&gt; ((1 + 2) + 3) Reduce(sum, 1:3) # -&gt; sum(sum(1, 2), 3) The essence of Reduce() can be described by a simple for loop: Reduce2 &lt;- function(f, x) { out &lt;- x[[1]] for(i in seq(2, length(x))) { out &lt;- f(out, x[[i]]) } out } The real Reduce() is more complicated because it includes arguments to control whether the values are reduced from the left or from the right (right), an optional initial value (init), and an option to output intermediate results (accumulate). Reduce() is an elegant way of extending a function that works with two inputs into a function that can deal with any number of inputs. It’s useful for implementing many types of recursive operations, like merges and intersections. (We’ll see another use in the final case study.) Imagine you have a list of numeric vectors, and you want to find the values that occur in every element: l &lt;- replicate(5, sample(1:10, 15, replace = T), simplify = FALSE) str(l) #&gt; List of 5 #&gt; $ : int [1:15] 10 8 8 1 10 6 6 7 3 9 ... #&gt; $ : int [1:15] 5 1 2 10 4 1 1 9 9 6 ... #&gt; $ : int [1:15] 1 6 2 7 9 10 8 9 4 6 ... #&gt; $ : int [1:15] 9 4 6 10 1 6 9 3 4 4 ... #&gt; $ : int [1:15] 4 4 7 7 1 8 1 7 2 3 ... You could do that by intersecting each element in turn: intersect(intersect(intersect(intersect(l[[1]], l[[2]]), l[[3]]), l[[4]]), l[[5]]) #&gt; [1] 10 1 6 4 2 That’s hard to read. With Reduce(), the equivalent is: Reduce(intersect, l) #&gt; [1] 10 1 6 4 2 9.4.2 Predicate functionals A predicate is a function that returns a single TRUE or FALSE, like is.character, all, or is.NULL. A predicate functional applies a predicate to each element of a list or data frame. There are three useful predicate functionals in base R: Filter(), Find(), and Position(). Filter() selects only those elements which match the predicate. Find() returns the first element which matches the predicate (or the last element if right = TRUE). Position() returns the position of the first element that matches the predicate (or the last element if right = TRUE). Another useful predicate functional is where(), a custom functional that generates a logical vector from a list (or a data frame) and a predicate: where &lt;- function(f, x) { vapply(x, f, logical(1)) } The following example shows how you might use these functionals with a data frame: df &lt;- data.frame(x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) where(is.factor, df) #&gt; x y #&gt; FALSE TRUE str(Filter(is.factor, df)) #&gt; &#39;data.frame&#39;: 3 obs. of 1 variable: #&gt; $ y: Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 2 3 str(Find(is.factor, df)) #&gt; Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 2 3 Position(is.factor, df) #&gt; [1] 2 9.4.3 Exercises Why isn’t is.na() a predicate function? What base R function is closest to being a predicate version of is.na()? Use Filter() and vapply() to create a function that applies a summary statistic to every numeric column in a data frame. What’s the relationship between which() and Position()? What’s the relationship between where() and Filter()? Implement Any(), a function that takes a list and a predicate function, and returns TRUE if the predicate function returns TRUE for any of the inputs. Implement All() similarly. Implement the span() function from Haskell: given a list x and a predicate function f, span returns the location of the longest sequential run of elements where the predicate is true. (Hint: you might find rle() helpful.) 9.5 Mathematical functionals Functionals are very common in mathematics. The limit, the maximum, the roots (the set of points where f(x) = 0), and the definite integral are all functionals: given a function, they return a single number (or vector of numbers). At first glance, these functions don’t seem to fit in with the theme of eliminating loops, but if you dig deeper you’ll find out that they are all implemented using an algorithm that involves iteration. In this section we’ll use some of R’s built-in mathematical functionals. There are three functionals that work with functions to return single numeric values: integrate() finds the area under the curve defined by f() uniroot() finds where f() hits zero optimise() finds the location of lowest (or highest) value of f() Let’s explore how these are used with a simple function, sin(): integrate(sin, 0, pi) #&gt; 2 with absolute error &lt; 2.2e-14 str(uniroot(sin, pi * c(1 / 2, 3 / 2))) #&gt; List of 5 #&gt; $ root : num 3.14 #&gt; $ f.root : num 1.22e-16 #&gt; $ iter : int 2 #&gt; $ init.it : int NA #&gt; $ estim.prec: num 6.1e-05 str(optimise(sin, c(0, 2 * pi))) #&gt; List of 2 #&gt; $ minimum : num 4.71 #&gt; $ objective: num -1 str(optimise(sin, c(0, pi), maximum = TRUE)) #&gt; List of 2 #&gt; $ maximum : num 1.57 #&gt; $ objective: num 1 In statistics, optimisation is often used for maximum likelihood estimation (MLE). In MLE, we have two sets of parameters: the data, which is fixed for a given problem, and the parameters, which vary as we try to find the maximum. These two sets of parameters make the problem well suited for closures. Combining closures with optimisation gives rise to the following approach to solving MLE problems. The following example shows how we might find the maximum likelihood estimate for \\(\\lambda\\), if our data come from a Poisson distribution. First, we create a function factory that, given a dataset, returns a function that computes the negative log likelihood (NLL) for parameter lambda. In R, it’s common to work with the negative since optimise() defaults to finding the minimum. poisson_nll &lt;- function(x) { n &lt;- length(x) sum_x &lt;- sum(x) function(lambda) { n * lambda - sum_x * log(lambda) # + terms not involving lambda } } Note how the closure allows us to precompute values that are constant with respect to the data. We can use this function factory to generate specific NLL functions for input data. Then optimise() allows us to find the best values (the maximum likelihood estimates), given a generous starting range. x1 &lt;- c(41, 30, 31, 38, 29, 24, 30, 29, 31, 38) x2 &lt;- c(6, 4, 7, 3, 3, 7, 5, 2, 2, 7, 5, 4, 12, 6, 9) nll1 &lt;- poisson_nll(x1) nll2 &lt;- poisson_nll(x2) optimise(nll1, c(0, 100))$minimum #&gt; [1] 32.1 optimise(nll2, c(0, 100))$minimum #&gt; [1] 5.47 We can check that these values are correct by comparing them to the analytic solution: in this case, it’s just the mean of the data, 32.1 and 5.467. Another important mathematical functional is optim(). It is a generalisation of optimise() that works with more than one dimension. If you’re interested in how it works, you might want to explore the Rvmmin package, which provides a pure-R implementation of optim(). Interestingly Rvmmin is no slower than optim(), even though it is written in R, not C. For this problem, the bottleneck lies not in controlling the optimisation but with having to evaluate the function multiple times. 9.5.1 Exercises Implement arg_max(). It should take a function and a vector of inputs, and return the elements of the input where the function returns the highest value. For example, arg_max(-10:5, function(x) x ^ 2) should return -10. arg_max(-5:5, function(x) x ^ 2) should return c(-5, 5). Also implement the matching arg_min() function. Challenge: read about the fixed point algorithm. Complete the exercises using R. 9.6 Loops that should be left as is Some loops have no natural functional equivalent. In this section you’ll learn about three common cases: modifying in place recursive functions while loops It’s possible to torture these problems to use a functional, but it’s not a good idea. You’ll create code that is harder to understand, eliminating the main reason for using functionals in the first case. 9.6.1 Modifying in place If you need to modify part of an existing data frame, it’s often better to use a for loop. For example, the following code performs a variable-by-variable transformation by matching the names of a list of functions to the names of variables in a data frame. trans &lt;- list( disp = function(x) x * 0.0163871, am = function(x) factor(x, labels = c(&quot;auto&quot;, &quot;manual&quot;)) ) for(var in names(trans)) { mtcars[[var]] &lt;- trans[[var]](mtcars[[var]]) } We wouldn’t normally use lapply() to replace this loop directly, but it is possible. Just replace the loop with lapply() by using &lt;&lt;-: lapply(names(trans), function(var) { mtcars[[var]] &lt;&lt;- trans[[var]](mtcars[[var]]) }) The for loop is gone, but the code is longer and much harder to understand. The reader needs to understand &lt;&lt;- and how x[[y]] &lt;&lt;- z works (it’s not simple!). In short, we’ve taken a simple, easily understood for loop, and turned it into something few people will understand: not a good idea! 9.6.2 Recursive relationships It’s hard to convert a for loop into a functional when the relationship between elements is not independent, or is defined recursively. For example, exponential smoothing works by taking a weighted average of the current and previous data points. The exps() function below implements exponential smoothing with a for loop. exps &lt;- function(x, alpha) { s &lt;- numeric(length(x) + 1) for (i in seq_along(s)) { if (i == 1) { s[i] &lt;- x[i] } else { s[i] &lt;- alpha * x[i] + (1 - alpha) * s[i - 1] } } s } x &lt;- runif(6) exps(x, 0.5) #&gt; [1] 0.0518 0.3078 0.5284 0.6148 0.5978 0.3628 NA We can’t eliminate the for loop because none of the functionals we’ve seen allow the output at position i to depend on both the input and output at position i - 1. One way to eliminate the for loop in this case is to solve the recurrence relation by removing the recursion and replacing it with explicit references. This requires a new set of mathematical tools, and is challenging, but it can pay off by producing a simpler function. 9.6.3 While loops Another type of looping construct in R is the while loop. It keeps running until some condition is met. while loops are more general than for loops: you can rewrite every for loop as a while loop, but you can’t do the reverse. For example, we could turn this for loop: for (i in 1:10) print(i) into this while loop: i &lt;- 1 while(i &lt;= 10) { print(i) i &lt;- i + 1 } Not every while loop can be turned into a for loop because many while loops don’t know in advance how many times they will be run: i &lt;- 0 while(TRUE) { if (runif(1) &gt; 0.9) break i &lt;- i + 1 } This is a common problem when you’re writing simulations. In this case we can remove the loop by recognising a special feature of the problem. Here we’re counting the number of successes before Bernoulli trial with p = 0.1 fails. This is a geometric random variable, so you could replace the code with i &lt;- rgeom(1, 0.1). Reformulating the problem in this way is hard to do in general, but you’ll benefit greatly if you can do it for your problem. 9.7 A family of functions To finish off the chapter, this case study shows how you can use functionals to take a simple building block and make it powerful and general. I’ll start with a simple idea, adding two numbers together, and use functionals to extend it to summing multiple numbers, computing parallel and cumulative sums, and summing across array dimensions. We’ll start by defining a very simple addition function, one which takes two scalar arguments: add &lt;- function(x, y) { stopifnot(length(x) == 1, length(y) == 1, is.numeric(x), is.numeric(y)) x + y } (We’re using R’s existing addition operator here, which does much more, but the focus here is on how we can take very simple building blocks and extend them to do more.) I’ll also add an na.rm argument. A helper function will make this a bit easier: if x is missing it should return y, if y is missing it should return x, and if both x and y are missing then it should return another argument to the function: identity. This function is probably a bit more general than what we need now, but it’s useful if we implement other binary operators. rm_na &lt;- function(x, y, identity) { if (is.na(x) &amp;&amp; is.na(y)) { identity } else if (is.na(x)) { y } else { x } } rm_na(NA, 10, 0) #&gt; [1] 10 rm_na(10, NA, 0) #&gt; [1] 10 rm_na(NA, NA, 0) #&gt; [1] 0 This allows us to write a version of add() that can deal with missing values if needed: add &lt;- function(x, y, na.rm = FALSE) { if (na.rm &amp;&amp; (is.na(x) || is.na(y))) rm_na(x, y, 0) else x + y } add(10, NA) #&gt; [1] NA add(10, NA, na.rm = TRUE) #&gt; [1] 10 add(NA, NA) #&gt; [1] NA add(NA, NA, na.rm = TRUE) #&gt; [1] 0 Why did we pick an identity of 0? Why should add(NA, NA, na.rm = TRUE) return 0? Well, for every other input it returns a number, so even if both arguments are NA, it should still do that. What number should it return? We can figure it out because addition is associative, which means that the order of addition doesn’t matter. That means that the following two function calls should return the same value: add(add(3, NA, na.rm = TRUE), NA, na.rm = TRUE) #&gt; [1] 3 add(3, add(NA, NA, na.rm = TRUE), na.rm = TRUE) #&gt; [1] 3 This implies that add(NA, NA, na.rm = TRUE) must be 0, and hence identity = 0 is the correct default. Now that we have the basics working, we can extend the function to deal with more complicated inputs. One obvious generalisation is to add more than two numbers. We can do this by iteratively adding two numbers: if the input is c(1, 2, 3) we compute add(add(1, 2), 3). This is a simple application of Reduce(): r_add &lt;- function(xs, na.rm = TRUE) { Reduce(function(x, y) add(x, y, na.rm = na.rm), xs) } r_add(c(1, 4, 10)) #&gt; [1] 15 This looks good, but we need to test a few special cases: r_add(NA, na.rm = TRUE) #&gt; [1] NA r_add(numeric()) #&gt; NULL These are incorrect. In the first case, we get a missing value even though we’ve explicitly asked to ignore them. In the second case, we get NULL instead of a length one numeric vector (as we do for every other set of inputs). The two problems are related. If we give Reduce() a length one vector, it doesn’t have anything to reduce, so it just returns the input. If we give it an input of length zero, it always returns NULL. The easiest way to fix this problem is to use the init argument of Reduce(). This is added to the start of every input vector: r_add &lt;- function(xs, na.rm = TRUE) { Reduce(function(x, y) add(x, y, na.rm = na.rm), xs, init = 0) } r_add(c(1, 4, 10)) #&gt; [1] 15 r_add(NA, na.rm = TRUE) #&gt; [1] 0 r_add(numeric()) #&gt; [1] 0 r_add() is equivalent to sum(). It would be nice to have a vectorised version of add() so that we can perform the addition of two vectors of numbers in element-wise fashion. We could use Map() or vapply() to implement this, but neither is perfect. Map() returns a list, instead of a numeric vector, so we need to use simplify2array(). vapply() returns a vector but it requires us to loop over a set of indices. v_add1 &lt;- function(x, y, na.rm = FALSE) { stopifnot(length(x) == length(y), is.numeric(x), is.numeric(y)) if (length(x) == 0) return(numeric()) simplify2array( Map(function(x, y) add(x, y, na.rm = na.rm), x, y) ) } v_add2 &lt;- function(x, y, na.rm = FALSE) { stopifnot(length(x) == length(y), is.numeric(x), is.numeric(y)) vapply(seq_along(x), function(i) add(x[i], y[i], na.rm = na.rm), numeric(1)) } A few test cases help to ensure that it behaves as we expect. We’re a bit stricter than base R here because we don’t do recycling. (You could add that if you wanted, but I find that recycling is a frequent source of silent bugs.) # Both versions give the same results v_add1(1:10, 1:10) #&gt; [1] 2 4 6 8 10 12 14 16 18 20 v_add1(numeric(), numeric()) #&gt; numeric(0) v_add1(c(1, NA), c(1, NA)) #&gt; [1] 2 NA v_add1(c(1, NA), c(1, NA), na.rm = TRUE) #&gt; [1] 2 0 Another variant of add() is the cumulative sum. We can implement it with Reduce() by setting the accumulate argument to TRUE: c_add &lt;- function(xs, na.rm = FALSE) { Reduce(function(x, y) add(x, y, na.rm = na.rm), xs, accumulate = TRUE) } c_add(1:10) #&gt; [1] 1 3 6 10 15 21 28 36 45 55 c_add(10:1) #&gt; [1] 10 19 27 34 40 45 49 52 54 55 This is equivalent to cumsum(). Finally, we might want to define addition for more complicated data structures like matrices. We could create row and col variants that sum across rows and columns, respectively, or we could go the whole hog and define an array version that could sum across any arbitrary set of dimensions. These are easily implemented as combinations of r_add() and apply(). row_sum &lt;- function(x, na.rm = FALSE) { apply(x, 1, r_add, na.rm = na.rm) } col_sum &lt;- function(x, na.rm = FALSE) { apply(x, 2, r_add, na.rm = na.rm) } arr_sum &lt;- function(x, dim, na.rm = FALSE) { apply(x, dim, r_add, na.rm = na.rm) } The first two are equivalent to rowSums() and colSums(). If every function we have created has an existing equivalent in base R, why did we bother? There are two main reasons: Since all variants were implemented by combining a simple binary operator (add()) and a well-tested functional (Reduce(), Map(), apply()), we know that our variants will behave consistently. We can apply the same infrastructure to other operators, especially those that might not have the full suite of variants in base R. The downside of this approach is that these implementations are not that efficient. (For example, colSums(x) is much faster than apply(x, 2, sum).) However, even if they aren’t that fast, simple implementations are still a good starting point because they’re less likely to have bugs. When you create faster versions, you can compare the results to make sure your fast versions are still correct. If you enjoyed this section, you might also enjoy “List out of lambda”, a blog article by Steve Losh that shows how you can produce high level language structures (like lists) out of more primitive language features (like closures, aka lambdas). 9.7.1 Exercises Implement smaller and larger functions that, given two inputs, return either the smaller or the larger value. Implement na.rm = TRUE: what should the identity be? (Hint: smaller(x, smaller(NA, NA, na.rm = TRUE), na.rm = TRUE) must be x, so smaller(NA, NA, na.rm = TRUE) must be bigger than any other value of x.) Use smaller and larger to implement equivalents of min(), max(), pmin(), pmax(), and new functions row_min() and row_max(). Create a table that has and, or, add, multiply, smaller, and larger in the columns and binary operator, reducing variant, vectorised variant, and array variants in the rows. Fill in the cells with the names of base R functions that perform each of the roles. Compare the names and arguments of the existing R functions. How consistent are they? How could you improve them? Complete the matrix by implementing any missing functions. How does paste() fit into this structure? What is the scalar binary function that underlies paste()? What are the sep and collapse arguments to paste() equivalent to? Are there any paste variants that don’t have existing R implementations? "],
["function-operators.html", "10 Function operators 10.1 Behavioural FOs 10.2 Output FOs 10.3 Input FOs 10.4 Combining FOs", " 10 Function operators In this chapter, you’ll learn about function operators (FOs). A function operator is a function that takes one (or more) functions as input and returns a function as output. In some ways, function operators are similar to functionals: there’s nothing you can’t do without them, but they can make your code more readable and expressive, and they can help you write code faster. The main difference is that functionals extract common patterns of loop use, where function operators extract common patterns of anonymous function use. The following code shows a simple function operator, chatty(). It wraps a function, making a new function that prints out its first argument. It’s useful because it gives you a window to see how functionals, like vapply(), work. chatty &lt;- function(f) { function(x, ...) { res &lt;- f(x, ...) cat(&quot;Processing &quot;, x, &quot;\\n&quot;, sep = &quot;&quot;) res } } f &lt;- function(x) x ^ 2 s &lt;- c(3, 2, 1) chatty(f)(1) #&gt; Processing 1 #&gt; [1] 1 vapply(s, chatty(f), numeric(1)) #&gt; Processing 3 #&gt; Processing 2 #&gt; Processing 1 #&gt; [1] 9 4 1 In the last chapter, we saw that many built-in functionals, like Reduce(), Filter(), and Map(), have very few arguments, so we had to use anonymous functions to modify how they worked. In this chapter, we’ll build specialised substitutes for common anonymous functions that allow us to communicate our intent more clearly. For example, in multiple inputs we used an anonymous function with Map() to supply fixed arguments: Map(function(x, y) f(x, y, zs), xs, ys) Later in this chapter, we’ll learn about partial application using the partial() function. Partial application encapsulates the use of an anonymous function to supply default arguments, and allows us to write succinct code: Map(partial(f, zs = zs), xs, yz) This is an important use of FOs: by transforming the input function, you eliminate parameters from a functional. In fact, as long as the inputs and outputs of the function remain the same, this approach allows your functionals to be more extensible, often in ways you haven’t thought of. The chapter covers four important types of FO: behaviour, input, output, and combining. For each type, I’ll show you some useful FOs, and how you can use as another to decompose problems: as combinations of multiple functions instead of combinations of arguments. The goal is not to exhaustively list every possible FO, but to show a selection that demonstrate how they work together with other FP techniques. For your own work, you’ll need to think about and experiment with how function operators can help you solve recurring problems. 10.0.0.0.1 Outline Behavioural FOs introduces you to FOs that change the behaviour of a function like automatically logging usage to disk or ensuring that a function is run only once. Output FOs shows you how to write FOs that manipulate the output of a function. These can do simple things like capturing errors, or fundamentally change what the function does. Input FOs describes how to modify the inputs to a function using a FO like Vectorize() or partial(). Combining FOs shows the power of FOs that combine multiple functions with function composition and logical operations. 10.0.0.0.2 Prerequisites As well as writing FOs from scratch, this chapter uses function operators from the memoise, plyr, and pryr packages. Install them by running install.packages(c(&quot;memoise&quot;, &quot;plyr&quot;, &quot;pryr&quot;)). 10.1 Behavioural FOs Behavioural FOs leave the inputs and outputs of a function unchanged, but add some extra behaviour. In this section, we’ll look at functions which implement three useful behaviours: Add a delay to avoid swamping a server with requests. Print to console every n invocations to check on a long running process. Cache previous computations to improve performance. To motivate these behaviours, imagine we want to download a long vector of URLs. That’s pretty simple with lapply() and download_file(): download_file &lt;- function(url, ...) { download.file(url, basename(url), ...) } lapply(urls, download_file) (download_file() is a simple wrapper around utils::download.file() which provides a reasonable default for the file name.) There are a number of useful behaviours we might want to add to this function. If the list was long, we might want to print a . every ten URLs so we know that the function’s still working. If we’re downloading files over the internet, we might want to add a small delay between each request to avoid hammering the server. Implementing these behaviours in a for loop is rather complicated. We can no longer use lapply() because we need an external counter: i &lt;- 1 for(url in urls) { i &lt;- i + 1 if (i %% 10 == 0) cat(&quot;.&quot;) Sys.delay(1) download_file(url) } Understanding this code is hard because different concerns (iteration, printing, and downloading) are interleaved. In the remainder of this section we’ll create FOs that encapsulate each behaviour and allow us to write code like this: lapply(urls, dot_every(10, delay_by(1, download_file))) Implementing delay_by() is straightforward, and follows the same basic template that we’ll see for the majority of FOs in this chapter: delay_by &lt;- function(delay, f) { function(...) { Sys.sleep(delay) f(...) } } system.time(runif(100)) #&gt; User System verstrichen #&gt; 0 0 0 system.time(delay_by(0.1, runif)(100)) #&gt; User System verstrichen #&gt; 0.000 0.000 0.103 dot_every() is a little bit more complicated because it needs to manage a counter. Fortunately, we saw how to do that in mutable state. dot_every &lt;- function(n, f) { i &lt;- 1 function(...) { if (i %% n == 0) cat(&quot;.&quot;) i &lt;&lt;- i + 1 f(...) } } x &lt;- lapply(1:100, runif) x &lt;- lapply(1:100, dot_every(10, runif)) #&gt; .......... Notice that I’ve made the function the last argument in each FO. This makes it easier to read when we compose multiple function operators. If the function were the first argument, then instead of: download &lt;- dot_every(10, delay_by(1, download_file)) we’d have download &lt;- dot_every(delay_by(download_file, 1), 10) That’s harder to follow because (e.g.) the argument of dot_every() is far away from its call. This is sometimes called the Dagwood sandwich problem: you have too much filling (too many long arguments) between your slices of bread (parentheses). I’ve also tried to give the FOs descriptive names: delay by 1 (second), (print a) dot every 10 (invocations). The more clearly the function names used in your code express your intent, the easier it will be for others (including future you) to read and understand the code. 10.1.1 Memoisation Another thing you might worry about when downloading multiple files is accidentally downloading the same file multiple times. You could avoid this by calling unique() on the list of input URLs, or manually managing a data structure that mapped the URL to the result. An alternative approach is to use memoisation: modify a function to automatically cache its results. library(memoise) slow_function &lt;- function() { Sys.sleep(1) 10 } system.time(slow_function()) #&gt; User System verstrichen #&gt; 0 0 1 system.time(slow_function()) #&gt; User System verstrichen #&gt; 0.001 0.000 1.006 fast_function &lt;- memoise(slow_function) system.time(fast_function()) #&gt; User System verstrichen #&gt; 0.001 0.000 1.005 system.time(fast_function()) #&gt; User System verstrichen #&gt; 0.014 0.000 0.013 Memoisation is an example of the classic computer science tradeoff of memory versus speed. A memoised function can run much faster because it stores all of the previous inputs and outputs, using more memory. A realistic use of memoisation is computing the Fibonacci series. The Fibonacci series is defined recursively: the first two values are 1 and 1, then f(n) = f(n - 1) + f(n - 2). A naive version implemented in R would be very slow because, for example, fib(10) computes fib(9) and fib(8), and fib(9) computes fib(8) and fib(7), and so on. As a result, the value for each value in the series gets computed many, many times. Memoising fib() makes the implementation much faster because each value is computed only once. fib &lt;- function(n) { if (n &lt; 2) return(1) fib(n - 2) + fib(n - 1) } system.time(fib(23)) #&gt; User System verstrichen #&gt; 0.042 0.003 0.046 system.time(fib(24)) #&gt; User System verstrichen #&gt; 0.052 0.000 0.052 fib2 &lt;- memoise(function(n) { if (n &lt; 2) return(1) fib2(n - 2) + fib2(n - 1) }) system.time(fib2(23)) #&gt; User System verstrichen #&gt; 0.027 0.000 0.028 system.time(fib2(24)) #&gt; User System verstrichen #&gt; 0.001 0.000 0.001 It doesn’t make sense to memoise all functions. For example, a memoised random number generator is no longer random: runifm &lt;- memoise(runif) runifm(5) #&gt; [1] 0.883 0.678 0.073 0.920 0.988 runifm(5) #&gt; [1] 0.883 0.678 0.073 0.920 0.988 Once we understand memoise(), it’s straightforward to apply to our problem: download &lt;- dot_every(10, memoise(delay_by(1, download_file))) This gives a function that we can easily use with lapply(). However, if something goes wrong with the loop inside lapply(), it can be difficult to tell what’s going on. The next section will show how we can use FOs to pull back the curtain and look inside. 10.1.2 Capturing function invocations One challenge with functionals is that it can be hard to see what’s going on inside of them. It’s not easy to pry open their internals like it is with a for loop. Fortunately we can use FOs to peer behind the curtain with tee(). tee(), defined below, has three arguments, all functions: f, the function to modify; on_input, a function that’s called with the inputs to f; and on_output, a function that’s called with the output from f. ignore &lt;- function(...) NULL tee &lt;- function(f, on_input = ignore, on_output = ignore) { function(...) { on_input(...) output &lt;- f(...) on_output(output) output } } (The function is inspired by the unix shell command tee, which is used to split up streams of file operations so that you can both display what’s happening and save intermediate results to a file.) We can use tee() to look inside the uniroot() functional, and see how it iterates its way to a solution. The following example finds where x and cos(x) intersect: g &lt;- function(x) cos(x) - x zero &lt;- uniroot(g, c(-5, 5)) show_x &lt;- function(x, ...) cat(sprintf(&quot;%+.08f&quot;, x), &quot;\\n&quot;) # The location where the function is evaluated: zero &lt;- uniroot(tee(g, on_input = show_x), c(-5, 5)) #&gt; -5.00000000 #&gt; +5.00000000 #&gt; +0.28366219 #&gt; +0.87520341 #&gt; +0.72298040 #&gt; +0.73863091 #&gt; +0.73908529 #&gt; +0.73902425 #&gt; +0.73908529 # The value of the function: zero &lt;- uniroot(tee(g, on_output = show_x), c(-5, 5)) #&gt; +5.28366219 #&gt; -4.71633781 #&gt; +0.67637474 #&gt; -0.23436269 #&gt; +0.02685676 #&gt; +0.00076012 #&gt; -0.00000026 #&gt; +0.00010189 #&gt; -0.00000026 cat() allows us to see what’s happening as the function runs, but it doesn’t give us a way to work with the values after the function as completed. To do that, we could capture the sequence of calls by creating a function, remember(), that records every argument called and retrieves them when coerced into a list. The small amount of S3 code needed is explained in S3. remember &lt;- function() { memory &lt;- list() f &lt;- function(...) { # This is inefficient! memory &lt;&lt;- append(memory, list(...)) invisible() } structure(f, class = &quot;remember&quot;) } as.list.remember &lt;- function(x, ...) { environment(x)$memory } print.remember &lt;- function(x, ...) { cat(&quot;Remembering...\\n&quot;) str(as.list(x)) } Now we can draw a picture showing how uniroot zeroes in on the final answer: locs &lt;- remember() vals &lt;- remember() zero &lt;- uniroot(tee(g, locs, vals), c(-5, 5)) x &lt;- unlist(as.list(locs)) error &lt;- unlist(as.list(vals)) plot(x, type = &quot;b&quot;); abline(h = 0.739, col = &quot;grey50&quot;) plot(error, type = &quot;b&quot;); abline(h = 0, col = &quot;grey50&quot;) 10.1.3 Laziness The function operators we’ve seen so far follow a common pattern: funop &lt;- function(f, otherargs) { function(...) { # maybe do something res &lt;- f(...) # maybe do something else res } } Unfortunately there’s a problem with this implementation because function arguments are lazily evaluated: f() may have changed between applying the FO and evaluating the function. This is a particular problem if you’re using a for loop to create multiple function operators. In the following example, we take a list of functions and delay each one. But when we try to evaluate the mean, we get the sum instead. funs &lt;- list(mean = mean, sum = sum) funs_m &lt;- vector(&quot;list&quot;, length(funs)) for (fun in names(funs)) { funs_m[[fun]] &lt;- delay_by(funs[[fun]], delay = 0.1) } funs_m$mean(1:10) #&gt; [1] 55 We can avoid that problem by explicitly forcing the evaluation of f(): delay_by &lt;- function(delay, f) { force(f) function(...) { Sys.sleep(delay) f(...) } } funs_m &lt;- vector(&quot;list&quot;, length(funs)) for (fun in names(funs)) { funs_m[[fun]] &lt;- delay_by(funs[[fun]], delay = 0.1) } funs_m$mean(1:10) #&gt; [1] 5.5 Note that lapply() and friends have special “non-lazy” behaviour so you don’t see this problem: delay_by &lt;- function(delay, f) { function(...) { Sys.sleep(delay) f(...) } } funs_m &lt;- lapply(funs, delay_by, delay = 0.1) funs_m$mean(1:10) #&gt; [1] 5.5 10.1.4 Exercises Write a FO that logs a time stamp and message to a file every time a function is run. What does the following function do? What would be a good name for it? f &lt;- function(g) { force(g) result &lt;- NULL function(...) { if (is.null(result)) { result &lt;&lt;- g(...) } result } } runif2 &lt;- f(runif) runif2(5) #&gt; [1] 0.128 0.756 0.459 0.877 0.618 runif2(10) #&gt; [1] 0.128 0.756 0.459 0.877 0.618 Modify delay_by() so that instead of delaying by a fixed amount of time, it ensures that a certain amount of time has elapsed since the function was last called. That is, if you called g &lt;- delay_by(1, f); g(); Sys.sleep(2); g() there shouldn’t be an extra delay. Write wait_until() which delays execution until a specific time. There are three places we could have added a memoise call: why did we choose the one we did? download &lt;- memoise(dot_every(10, delay_by(1, download_file))) download &lt;- dot_every(10, memoise(delay_by(1, download_file))) download &lt;- dot_every(10, delay_by(1, memoise(download_file))) Why is the remember() function inefficient? How could you implement it in more efficient way? Why does the following code, from stackoverflow, not do what you expect? # return a linear function with slope a and intercept b. f &lt;- function(a, b) function(x) a * x + b # create a list of functions with different parameters. fs &lt;- Map(f, a = c(0, 1), b = c(0, 1)) fs[[1]](3) #&gt; [1] 0 # should return 0 * 3 + 0 = 0 How can you modify f so that it works correctly? 10.2 Output FOs The next step up in complexity is to modify the output of a function. This could be quite simple, or it could fundamentally change the operation of the function by returning something completely different to its usual output. In this section you’ll learn about two simple modifications, Negate() and failwith(), and two fundamental modifications, capture_it() and time_it(). 10.2.1 Minor modifications base::Negate() and plyr::failwith() offer two minor, but useful, modifications of a function that are particularly handy in conjunction with functionals. Negate() takes a function that returns a logical vector (a predicate function), and returns the negation of that function. This can be a useful shortcut when a function returns the opposite of what you need. The essence of Negate() is very simple: Negate &lt;- function(f) { force(f) function(...) !f(...) } (Negate(is.null))(NULL) #&gt; [1] FALSE I often use this idea to make a function, compact(), that removes all null elements from a list: compact &lt;- function(x) Filter(Negate(is.null), x) plyr::failwith() turns a function that throws an error into a function that returns a default value when there’s an error. Again, the essence of failwith() is simple; it’s just a wrapper around try(), the function that captures errors and allows execution to continue. failwith &lt;- function(default = NULL, f, quiet = FALSE) { force(f) function(...) { out &lt;- default try(out &lt;- f(...), silent = quiet) out } } log(&quot;a&quot;) #&gt; Error in log(&quot;a&quot;): Nicht-numerisches Argument für mathematische Funktion failwith(NA, log)(&quot;a&quot;) #&gt; Error in f(...) : non-numeric argument to mathematical function #&gt; NA failwith(NA, log, quiet = TRUE)(&quot;a&quot;) #&gt; [1] NA (If you haven’t seen try() before, it’s discussed in more detail in exceptions and debugging.) failwith() is very useful in conjunction with functionals: instead of the failure propagating and terminating the higher-level loop, you can complete the iteration and then find out what went wrong. For example, imagine you’re fitting a set of generalised linear models (GLMs) to a list of data frames. While GLMs can sometimes fail because of optimisation problems, you’d still want to be able to try to fit all the models, and later look back at those that failed: # If any model fails, all models fail to fit: models &lt;- lapply(datasets, glm, formula = y ~ x1 + x2 * x3) # If a model fails, it will get a NULL value models &lt;- lapply(datasets, failwith(NULL, glm), formula = y ~ x1 + x2 * x3) # remove failed models (NULLs) with compact ok_models &lt;- compact(models) # extract the datasets corresponding to failed models failed_data &lt;- datasets[vapply(models, is.null, logical(1))] I think this is a great example of the power of combining functionals and function operators: it lets you succinctly express what you need to solve a common data analysis problem. 10.2.2 Changing what a function does Other output function operators can have a more profound effect on the operation of the function. Instead of returning the original return value, we can return some other effect of the function evaluation. Here are two examples: Return text that the function print()ed: capture_it &lt;- function(f) { force(f) function(...) { capture.output(f(...)) } } str_out &lt;- capture_it(str) str(1:10) #&gt; int [1:10] 1 2 3 4 5 6 7 8 9 10 str_out(1:10) #&gt; [1] &quot; int [1:10] 1 2 3 4 5 6 7 8 9 10&quot; Return how long a function took to run: time_it &lt;- function(f) { force(f) function(...) { system.time(f(...)) } } time_it() allows us to rewrite some of the code from the functionals chapter: compute_mean &lt;- list( base = function(x) mean(x), sum = function(x) sum(x) / length(x) ) x &lt;- runif(1e6) # Previously we used an anonymous function to time execution: # lapply(compute_mean, function(f) system.time(f(x))) # Now we can compose function operators: call_fun &lt;- function(f, ...) f(...) lapply(compute_mean, time_it(call_fun), x) #&gt; $base #&gt; User System verstrichen #&gt; 0.002 0.000 0.002 #&gt; #&gt; $sum #&gt; User System verstrichen #&gt; 0.001 0.000 0.001 In this example, there’s not a huge benefit to using function operators, because the composition is simple and we’re applying the same operator to each function. Generally, using function operators is most effective when you are using multiple operators or if the gap between creating them and using them is large. 10.2.3 Exercises Create a negative() FO that flips the sign of the output of the function to which it is applied. The evaluate package makes it easy to capture all the outputs (results, text, messages, warnings, errors, and plots) from an expression. Create a function like capture_it() that also captures the warnings and errors generated by a function. Create a FO that tracks files created or deleted in the working directory (Hint: use dir() and setdiff().) What other global effects of functions might you want to track? 10.3 Input FOs The next step up in complexity is to modify the inputs of a function. Again, you can modify how a function works in a minor way (e.g., setting default argument values), or in a major way (e.g., converting inputs from scalars to vectors, or vectors to matrices). 10.3.1 Prefilling function arguments: partial function application A common use of anonymous functions is to make a variant of a function that has certain arguments “filled in” already. This is called “partial function application”, and is implemented by pryr::partial(). Once you have read metaprogramming, I encourage you to read the source code for partial() and figure out how it works — it’s only 5 lines of code! partial() allows us to replace code like f &lt;- function(a) g(a, b = 1) compact &lt;- function(x) Filter(Negate(is.null), x) Map(function(x, y) f(x, y, zs), xs, ys) with f &lt;- partial(g, b = 1) compact &lt;- partial(Filter, Negate(is.null)) Map(partial(f, zs = zs), xs, ys) We can use this idea to simplify the code used when working with lists of functions. Instead of: funs2 &lt;- list( sum = function(...) sum(..., na.rm = TRUE), mean = function(...) mean(..., na.rm = TRUE), median = function(...) median(..., na.rm = TRUE) ) we can write: library(pryr) #&gt; #&gt; Attache Paket: &#39;pryr&#39; #&gt; The following object is masked _by_ &#39;.GlobalEnv&#39;: #&gt; #&gt; f funs2 &lt;- list( sum = partial(sum, na.rm = TRUE), mean = partial(mean, na.rm = TRUE), median = partial(median, na.rm = TRUE) ) Using partial function application is a straightforward task in many functional programming languages, but it’s not entirely clear how it should interact with R’s lazy evaluation rules. The approach pryr::partial() takes is to create a function that is as similar as possible to the anonymous function that you’d create by hand. Peter Meilstrup takes a different approach in his ptools package. If you’re interested in the topic, you might want to read about the binary operators he created: %()%, %&gt;&gt;%, and %&lt;&lt;%. 10.3.2 Changing input types It’s also possible to make a major change to a function’s input, making a function work with fundamentally different types of data. There are a few existing functions that work along these lines: base::Vectorize() converts a scalar function to a vector function. It takes a non-vectorised function and vectorises it with respect to the arguments specified in the vectorize.args argument. This doesn’t give you any magical performance improvements, but it’s useful if you want a quick and dirty way of making a vectorised function. A mildly useful extension to sample() would be to vectorize it with respect to size. Doing so would allow you to generate multiple samples in one call. sample2 &lt;- Vectorize(sample, &quot;size&quot;, SIMPLIFY = FALSE) str(sample2(1:5, c(1, 1, 3))) #&gt; List of 3 #&gt; $ : int 2 #&gt; $ : int 1 #&gt; $ : int [1:3] 2 1 5 str(sample2(1:5, 5:3)) #&gt; List of 3 #&gt; $ : int [1:5] 5 1 2 3 4 #&gt; $ : int [1:4] 2 3 5 1 #&gt; $ : int [1:3] 5 4 2 In this example we have used SIMPLIFY = FALSE to ensure that our newly vectorised function always returns a list. This is usually what you want. splat() converts a function that takes multiple arguments to a function that takes a single list of arguments. splat &lt;- function (f) { force(f) function(args) { do.call(f, args) } } This is useful if you want to invoke a function with varying arguments: x &lt;- c(NA, runif(100), 1000) args &lt;- list( list(x), list(x, na.rm = TRUE), list(x, na.rm = TRUE, trim = 0.1) ) lapply(args, splat(mean)) #&gt; [[1]] #&gt; [1] NA #&gt; #&gt; [[2]] #&gt; [1] 10.4 #&gt; #&gt; [[3]] #&gt; [1] 0.478 plyr::colwise() converts a vector function to one that works with data frames: median(mtcars) #&gt; Error in median.default(mtcars): need numeric data median(mtcars$mpg) #&gt; [1] 19.2 plyr::colwise(median)(mtcars) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 1 19.2 6 196 123 3.7 3.33 17.7 0 0 4 2 10.3.3 Exercises Our previous download() function only downloads a single file. How can you use partial() and lapply() to create a function that downloads multiple files at once? What are the pros and cons of using partial() vs. writing a function by hand? Read the source code for plyr::colwise(). How does the code work? What are colwise()’s three main tasks? How could you make colwise() simpler by implementing each task as a function operator? (Hint: think about partial().) Write FOs that convert a function to return a matrix instead of a data frame, or a data frame instead of a matrix. If you understand S3, call them as.data.frame.function() and as.matrix.function(). You’ve seen five functions that modify a function to change its output from one form to another. What are they? Draw a table of the various combinations of types of outputs: what should go in the rows and what should go in the columns? What function operators might you want to write to fill in the missing cells? Come up with example use cases. Look at all the examples of using an anonymous function to partially apply a function in this and the previous chapter. Replace the anonymous function with partial(). What do you think of the result? Is it easier or harder to read? 10.4 Combining FOs Besides just operating on single functions, function operators can take multiple functions as input. One simple example of this is plyr::each(). It takes a list of vectorised functions and combines them into a single function. summaries &lt;- plyr::each(mean, sd, median) summaries(1:10) #&gt; mean sd median #&gt; 5.50 3.03 5.50 Two more complicated examples are combining functions through composition, or through boolean algebra. These capabilities are the glue that allow us to join multiple functions together. 10.4.1 Function composition An important way of combining functions is through composition: f(g(x)). Composition takes a list of functions and applies them sequentially to the input. It’s a replacement for the common pattern of anonymous function that chains multiple functions together to get the result you want: sapply(mtcars, function(x) length(unique(x))) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 25 3 27 22 22 29 30 2 2 3 6 A simple version of compose looks like this: compose &lt;- function(f, g) { function(...) f(g(...)) } (pryr::compose() provides a more full-featured alternative that can accept multiple functions and is used for the rest of the examples.) This allows us to write: sapply(mtcars, compose(length, unique)) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 25 3 27 22 22 29 30 2 2 3 6 Mathematically, function composition is often denoted with the infix operator, o, (f o g)(x). Haskell, a popular functional programming language, uses . to the same end. In R, we can create our own infix composition function: &quot;%o%&quot; &lt;- compose sapply(mtcars, length %o% unique) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 25 3 27 22 22 29 30 2 2 3 6 sqrt(1 + 8) #&gt; [1] 3 compose(sqrt, `+`)(1, 8) #&gt; [1] 3 (sqrt %o% `+`)(1, 8) #&gt; [1] 3 Compose also allows for a very succinct implementation of Negate, which is just a partially evaluated version of compose(). Negate &lt;- partial(compose, `!`) We could implement the population standard deviation with function composition: square &lt;- function(x) x^2 deviation &lt;- function(x) x - mean(x) sd2 &lt;- sqrt %o% mean %o% square %o% deviation sd2(1:10) #&gt; [1] 2.87 This type of programming is called tacit or point-free programming. (The term point-free comes from the use of “point” to refer to values in topology; this style is also derogatorily known as pointless). In this style of programming, you don’t explicitly refer to variables. Instead, you focus on the high-level composition of functions rather than the low-level flow of data. The focus is on what’s being done, not on objects it’s being done to. Since we’re using only functions and not parameters, we use verbs and not nouns. This style is common in Haskell, and is the typical style in stack based programming languages like Forth and Factor. It’s not a terribly natural or elegant style in R, but it is fun to play with. compose() is particularly useful in conjunction with partial(), because partial() allows you to supply additional arguments to the functions being composed. One nice side effect of this style of programming is that it keeps a function’s arguments near its name. This is important because as the size of the chunk of code you have to hold in your head grows code becomes harder to understand. Below I take the example from the first section of the chapter and modify it to use the two styles of function composition described above. Both results are longer than the original code, but they may be easier to understand because the function and its arguments are closer together. Note that we still have to read them from right to left (bottom to top): the first function called is the last one written. We could define compose() to work in the opposite direction, but in the long run, this is likely to lead to confusion since we’d create a small part of the language that reads differently from every other part. download &lt;- dot_every(10, memoise(delay_by(1, download_file))) download &lt;- pryr::compose( partial(dot_every, 10), memoise, partial(delay_by, 1) )(download_file) download &lt;- (partial(dot_every, 10) %o% memoise %o% partial(delay_by, 1))(download_file) 10.4.2 Logical predicates and boolean algebra When I use Filter() and other functionals that work with logical predicates, I often find myself using anonymous functions to combine multiple conditions: Filter(function(x) is.character(x) || is.factor(x), iris) As an alternative, we could define function operators that combine logical predicates: and &lt;- function(f1, f2) { force(f1); force(f2) function(...) { f1(...) &amp;&amp; f2(...) } } or &lt;- function(f1, f2) { force(f1); force(f2) function(...) { f1(...) || f2(...) } } not &lt;- function(f) { force(f) function(...) { !f(...) } } This would allow us to write: Filter(or(is.character, is.factor), iris) Filter(not(is.numeric), iris) And we now have a boolean algebra on functions, not on the results of functions. 10.4.3 Exercises Implement your own version of compose() using Reduce and %o%. For bonus points, do it without calling function. Extend and() and or() to deal with any number of input functions. Can you do it with Reduce()? Can you keep them lazy (e.g., for and(), the function returns once it sees the first FALSE)? Implement the xor() binary operator. Implement it using the existing xor() function. Implement it as a combination of and() and or(). What are the advantages and disadvantages of each approach? Also think about what you’ll call the resulting function to avoid a clash with the existing xor() function, and how you might change the names of and(), not(), and or() to keep them consistent. Above, we implemented boolean algebra for functions that return a logical function. Implement elementary algebra (plus(), minus(), multiply(), divide(), exponentiate(), log()) for functions that return numeric vectors. "],
["oo.html", "11 Introduction 11.1 OOP Systems 11.2 OOP in R 11.3 Field guide", " 11 Introduction In the following five chapters you’ll learn about object oriented programming (OOP) in R. OOP in R is a little more challenging than in other languages, because: There are multiple OOP systems to choose between. In this book, I’ll focus on the three that are most important in my opinion: S3, S4, and R6. S3 and S4 come from a very different heritage than the OOP found in most other popular languages. This means your existing OOP skills are unlikely to be of much help. Indeed, for day-to-day use of R, FP is much more important than OOP. There are three main reasons to learn OOP: Learning a little S3 allows your functions to return richer results that have a user friendly display and programmer friendly internals. It also defines syntactic standards can apply across multiple packages. This is why S3 is used throughout base R. Investing in S4 can be helpful for building up large systems that evolve over many years and are written by many programmers. This is why the Bioconductor project uses S4 as fundamental infrastructure. Mastering R6 gives you a standard way to escape R’s copy-on-modify semantics when needed. This is particularly important if you want to model real-world objects that change over time. This chapter will give you a rough lay of the land, and a field guide to help you identify OOP systems in the wild. The following four chapters (Base types, S3, S4, and R6) will dive into the details, starting with R’s base types. These are not technically an OOP system, but they’re important to understand because they’re the fundamental building block of the true OOP systems. 11.1 OOP Systems We’ll begin with an info dump of vocabulary and terminology. Don’t worry if it doesn’t stick. We’ll come back to these ideas multiple times in the subsequent chapters. Central to any OOP system are the concepts of class and method. A class defines the behaviour of a set of objects, or instances, by describing their attributes and their relationship to other classes. The class is also used when selecting methods, functions that behave differently depending on the class of their input. A class defines what something is and methods describe what something can do. Classes are usually organised in a hierarchy: if a method does not exist for a child, then the parent’s method is used instead. This means that a child class will inherit behaviour from the parent class. Inheritance is one of the most important parts of OOP because it allows you to reduce the amount of code you have to write. Following the notation of Extending R, there are two main styles of OOP: In encapsulated OOP, methods belong to objects or classes. This is the most common paradigm in modern programming languages, and method calls typically look like object.method. This is called encapsulated because the object encapsulates all its metadata. In functional OOP, methods belong to functions called generics. Method calls look like ordinary function calls: generic(object). This is called functional because from the outside it just looks like function calls. 11.2 OOP in R Base R provides three OOP systems: S3, S4, and reference classes (RC): S3 is R’s first OOP system, and is described Statistical Models in S (1991). It informally implements the functional style. It provides no ironclad guarantees but instead relies on a set of conventions. This makes it easy to get started with, and a low cost way of solving many simple problems. S4 is similar to S3, but much more formal. It was introduced in Programming with Data (1998). It requires more upfront work and in return provides greater consistency. S4 is implemented in the methods package, which is attached by default. The only package in base R to make use of S4 is stats4. (You might wonder if S1 and S2 exist. They don’t: S3 and S4 were named according to the versions of S that they accompanied.) RC implements encapsulated OO. RC objects are also mutable: they don’t use R’s usual copy-on-modify semantics, but are modified in place. This makes them harder to reason about, but allows them to solve problems that are difficult to solve with S3 or S4. There are a number other OOP systems provided by packages. Three of the most popular are: R6 implements encapsulated OOP like RC, but resolves some important issues. You’ll learn R6 instead of RC in this book. More on why later. R.oo provides some formalism on top of S3, and makes it possible to have mutable S3 objects. proto implements another style of OOP, called prototype based. It blurs the distinctions between classes and instances of classes (objects). There is some more information about prototype based programming http://vita.had.co.nz/papers/mutatr.html. Most OO systems in external packages are primarily of academic interest: they will help you understand the spectrum of OOP better, and can make it easier to solve certain classes of problems. However, they come with a big drawback: few R users know and understand them, so it is hard for others to read and contribute to your code. 11.3 Field guide Before we go on to discuss base types, S3, S4, and R6 in more detail I want to introduce the sloop package: # install_github(&quot;hadley/sloop&quot;) library(sloop) The sloop package (think sail the seas of OOP in R) provides a number of helpers to fill in missing pieces in base R. The first helper to know about is sloop::otype(). It makes it easy to figure what OOP system an object found in the wild uses: otype(1:10) #&gt; [1] &quot;base&quot; otype(mtcars) #&gt; [1] &quot;S3&quot; mle_obj &lt;- stats4::mle(function(x = 1) (x - 2) ^ 2) otype(mle_obj) #&gt; [1] &quot;S4&quot; Without otype(), you need to work your way through the base functions: is.object() distinguishes between base types (FALSE) and everything else (TRUE). isS4() distinguishes between S3 and S4. inherits() lets you figure out if you have an R6 object (an S3 object that inherits from “R6”) or an RC object (an S4 object that inherits from “refClass”). "],
["base-types.html", "12 Base types 12.1 Base objects vs OO objects 12.2 Base types 12.3 The is functions", " 12 Base types To talk about objects and OOP in R we need to first deal with a fundamental confusion: we use the word object to mean two different things. In this book so far, we’ve used object in a general sense, as captured by John Chambers’ pithy quote: “Everything that exists in R is an object”. However, while everything is an object, not everything is “object-oriented”. This confusion arises because the base objects come from S, and were developed before anyone was thinking that S might need an OOP system. The tools and nomenclature evolved organically over many years without a single guiding principle. Most of the time, the distinction between objects and object-oriented objects is not important. But here we need to get into the nitty gritty details so we’ll use the terms base objects and OO objects to distinguish them. We’ll also discuss the is.* functions here. These functions are used for many purposes, but are commonly used to determine if an object has a specific base type. 12.1 Base objects vs OO objects To tell the difference between a base and an OO object, use is.object(): # A base object: is.object(1:10) #&gt; [1] FALSE # An OO object is.object(mtcars) #&gt; [1] TRUE (This function would be better called is.oo() because it tells you if an object is a base object or a OO object.) The primary attribute that distinguishes between base and OO object is the “class”. Base objects do not have a class attribute: attr(1:10, &quot;class&quot;) #&gt; NULL attr(mtcars, &quot;class&quot;) #&gt; [1] &quot;data.frame&quot; Note that attr(x, &quot;class&quot;) and class(x) do not always return the same thing, as class() returns a value, not NULL, for base objects. We’ll talk about exactly what it does return in the next chapter. 12.2 Base types While only OO objects have a class attribute, every object has a base type: typeof(1:10) #&gt; [1] &quot;integer&quot; typeof(mtcars) #&gt; [1] &quot;list&quot; Base types do not form an OOP system because functions that behave differently for different base types are primarily written in C, where dispatch occurs using switch statements. This means only R-core can create new types, and creating a new type is a lot of work. As a consequence, new base types are rarely added. The most recent change, in 2011, added two exotic types that you never see in R, but are needed for diagnosing memory problems (NEWSXP and FREESXP). Prior to that, the last type added was a special base type for S4 objects (S4SXP) added in 2005. In total, there are 25 different base types. They are listed below, loosely grouped according to where they’re discussed in this book. The vectors: NULL, logical, integer, double, complex, character, list, raw. typeof(1:10) #&gt; [1] &quot;integer&quot; typeof(NULL) #&gt; [1] &quot;NULL&quot; typeof(1i) #&gt; [1] &quot;complex&quot; Functions: closure (regular R functions), special (internal functions), builtin (primitive functions) and environment. typeof(mean) #&gt; [1] &quot;closure&quot; typeof(`[`) #&gt; [1] &quot;special&quot; typeof(sum) #&gt; [1] &quot;builtin&quot; typeof(globalenv()) #&gt; [1] &quot;environment&quot; Language components: symbol (aka names), language (usually called calls), pairlist (used for function arguments). typeof(quote(a)) #&gt; [1] &quot;symbol&quot; typeof(quote(a + 1)) #&gt; [1] &quot;language&quot; typeof(formals(mean)) #&gt; [1] &quot;pairlist&quot; “Expression” is a special purpose type that’s only returned by parse() and expression(). They are not needed in user code. There are a few esoteric types that are important for C code but not generally available at the R level: externalptr, weakref, bytecode, S4, promise, “…”, and any. You may have heard of mode() and storage.mode(). I recommend ignoring these functions because they just provide S compatible aliases of typeof(). Read the source code if you want to understand exactly what they do. 12.3 The is functions This is also a good place to discuss the is functions because they’re often used to check if an object has a specific type: is.function(mean) #&gt; [1] TRUE is.primitive(sum) #&gt; [1] TRUE “Is” functions are often surprising because there are several different classes, they often have a few special cases, and their names are historical so don’t always reflect the usage in this book. They fall roughly into six classes: A specific value of typeof(): is.call(), is.character(), is.complex(), is.double(), is.environment(), is.expression(), is.list(), is.logical(), is.name(), is.null(), is.pairlist(), is.raw(), is.symbol(). is.integer() is almost in this class, but it specifically checks for the absense of a class attribute containing “factor”. Also note that is.vector() belongs to the “attributes” class, and is.numeric() is described specially below. A set of possible of base types: is.atomic() = logical, integer, double, characer, raw, and (surprisingly) NULL. is.function() = special, builtin, closure. is.primitive() = special, builtin. is.language() = symbol, language, expression. is.recursive() = list, language, expression. Attributes: is.vector(x) tests that x has no attributes apart from names. It does not check if an object is an atomic vector or list. is.matrix(x) tests if length(dim(x)) is 2. is.array(x) tests if length(dim(x)) is 1 or 3+. Has an S3 class: is.data.frame(), is.factor(), is.numeric_version(), is.ordered(), is.package_version(), is.qr(), is.table(). Vectorised mathematical operation: is.finite(), is.infinite(), is.na(), is.nan(). Finally there are a bunch of special purpose functions that don’t fall into any other category: is.loaded(): tests if a C/Fortran subroutine is loaded. is.object(): discussed above. is.R() and is.single(): are included for S+ compatibility is.unsorted() tests if a vector is unsorted. is.element(x, y) checks if x is an element of y: it’s even more different as it takes two arguments, unlike every other is. function. One function, is.numeric(), is sufficiently complicated and important that it needs a little extra discussion. The complexity comes about because R uses “numeric” to mean three slightly different things: In some places it’s used as an alias for “double”. For example as.numeric() is identical to as.double(), and numeric() is identical to double(). R also occasionally uses “real” instead of double; NA_real_ is the one place that you’re likely to encounter this in practice. In S3 and S4 it is used to mean either integer or double. We’ll talk about s3_class() in the next chapter: sloop::s3_class(1) #&gt; [1] &quot;double&quot; &quot;numeric&quot; sloop::s3_class(1L) #&gt; [1] &quot;integer&quot; &quot;numeric&quot; In is.numeric() it means an object built on a base type of integer or double that is not a factor (i.e. it is a number and behaves like a number). is.numeric(1) #&gt; [1] TRUE is.numeric(1L) #&gt; [1] TRUE is.numeric(factor(&quot;x&quot;)) #&gt; [1] FALSE "],
["s3.html", "13 S3 13.1 Basics 13.2 Classes 13.3 Generics and methods 13.4 Method dispatch 13.5 Inheritance 13.6 Dispatch details", " 13 S3 S3 is R’s first and simplest OO system. S3 is informal and ad hoc, but it has a certain elegance in its minimalism: you can’t take away any part of it and still have a useful OO system. Because of these reasons, S3 should be your default choice for OO programming: you should use it unless you have a compelling reason otherwise. S3 is the only OO system used in the base and stats packages, and it’s the most commonly used system in CRAN packages. S3 is a very flexible system: it allows you to do a lot of things that are quite ill-advised. If you’re coming from a strict environment like Java, this will seem pretty frightening (and it is!) but it does give R programmers a tremendous amount of freedom. While it’s very difficult to prevent someone from doing something you don’t want them to do, your users will never be held back because there is something you haven’t implemented yet. Since S3 has few built-in constraints, the key to its successful use is applying the constraints yourself. This chapter will teach you the conventions you should (almost) always adhere to in order to use S3 safely. We’ll use the sloop package to fill in some missing pieces when it comes to S3. # install_github(&quot;hadley/sloop&quot;) library(sloop) 13.1 Basics An S3 object is built on top of a base type with the “class” attribute set. The base type is typically a vector, although we will see later that it’s possible to use other types of classes. For example, take the factor. It is built on top of an integer vector, and the value of the class attribute is “factor”. It stores information about the “levels” in another attribute. f &lt;- factor(&quot;a&quot;) typeof(f) #&gt; [1] &quot;integer&quot; attributes(f) #&gt; $levels #&gt; [1] &quot;a&quot; #&gt; #&gt; $class #&gt; [1] &quot;factor&quot; An S3 object behaves differently from its underlying base type because of generic functions, or generics for short. A generic executes different code depending on the class of one of its arguments, almost always the first. You can see this difference with the most important generic function: print(). print(f) #&gt; [1] a #&gt; Levels: a print(unclass(f)) #&gt; [1] 1 #&gt; attr(,&quot;levels&quot;) #&gt; [1] &quot;a&quot; unclass() strips the class attribute from its input, so it is a useful tool for seeing what special behaviour an S3 class adds. str() shows the internal structure of S3 objects. Be careful when using str(): some S3 classes provide a custom str() method which can hide the underlying details. For example, take the POSIXlt class, which is one of the two classes used to represent date-time data: time &lt;- strptime(&quot;2017-01-01&quot;, &quot;%Y-%m-%d&quot;) str(time) #&gt; POSIXlt[1:1], format: &quot;2017-01-01&quot; str(unclass(time), list.len = 5) #&gt; List of 11 #&gt; $ sec : num 0 #&gt; $ min : int 0 #&gt; $ hour : int 0 #&gt; $ mday : int 1 #&gt; $ mon : int 0 #&gt; [list output truncated] A generic and its methods are functions that operate on classes. The role of a generic is to find the right method for the arguments that it is provided, the process of method dispatch. A method is a function that implements the generic behaviour for a specific class. In other words the job of the generic is to find the right method; the job of the method is to do the work. S3 methods are functions with a special naming scheme, generic.class(). For example, the Date method for the mean() generic is called mean.Date(), and the factor method for print() is called print.factor(). This is the reason that most modern style guides discourage the use of . in function names: it makes them look like S3 methods. For example, is t.test() the t method for test objects? You can find some S3 methods (those in the base package and those that you’ve created) by typing their names. However, this will not work with most packages because S3 methods are not exported: they live only inside the package, and are not available from the global environment. Instead, you can use getS3method(), which will work regardless of where the method lives: # Only works because the method is in the base package mean.Date #&gt; function (x, ...) #&gt; structure(mean(unclass(x), ...), class = &quot;Date&quot;) #&gt; &lt;bytecode: 0x7fea95696e68&gt; #&gt; &lt;environment: namespace:base&gt; # Always works getS3method(&quot;mean&quot;, &quot;Date&quot;) #&gt; function (x, ...) #&gt; structure(mean(unclass(x), ...), class = &quot;Date&quot;) #&gt; &lt;bytecode: 0x7fea95696e68&gt; #&gt; &lt;environment: namespace:base&gt; 13.1.1 Exercises The most important S3 objects in base R are factors, data frames, and date/times (Dates, POSIXct, POSIXlt). You’ve already seen the attributes and base type that factors are built on. What base types and attributes are the others built on? Describe the difference in behaviour in these two calls. set.seed(1014) some_days &lt;- as.Date(&quot;2017-01-31&quot;) + sample(10, 5) mean(some_days) #&gt; [1] &quot;2017-02-05&quot; mean(unclass(some_days)) #&gt; [1] 17202 Draw a Venn diagram illustrating the relationships between functions, generics, and methods. What does the as.data.frame.data.frame() method do? Why is it confusing? How should you avoid this confusion in your own code? What does the following code return? What base type is it built on? What attributes does it use? x &lt;- ecdf(rpois(100, 10)) x #&gt; Empirical CDF #&gt; Call: ecdf(rpois(100, 10)) #&gt; x[1:18] = 2, 3, 4, ..., 2e+01, 2e+01 13.2 Classes S3 is a simple and ad hoc system, and has no formal definition of a class. To make an object an instance of a class, you simply take an existing object and set the class attribute. You can do that during creation with structure(), or after the fact with class&lt;-(): # Create and assign class in one step foo &lt;- structure(list(), class = &quot;foo&quot;) # Create, then set class foo &lt;- list() class(foo) &lt;- &quot;foo&quot; You can determine the class of any object using class(x), and see if an object inherits from a specific class using inherits(x, &quot;classname&quot;). class(foo) #&gt; [1] &quot;foo&quot; inherits(foo, &quot;foo&quot;) #&gt; [1] TRUE The class name can be any character vector, but I recommend using only letters and _. Avoid .. Opinion is mixed whether to use underscores (my_class) or CamelCase (MyClass) for multi-word class names. Pick one convention and stick with it. It’s possible to provide a vector of class names, which allows S3 to implement a basic style of inheritance. This allows you to reduce your workload by allowing classes to share code where possible. We’ll come back to this idea in inheritance. S3 has no checks for correctness. This means you can change the class of existing objects: # Create a linear model mod &lt;- lm(log(mpg) ~ log(disp), data = mtcars) class(mod) #&gt; [1] &quot;lm&quot; print(mod) #&gt; #&gt; Call: #&gt; lm(formula = log(mpg) ~ log(disp), data = mtcars) #&gt; #&gt; Coefficients: #&gt; (Intercept) log(disp) #&gt; 5.381 -0.459 # Turn it into a data frame (?!) class(mod) &lt;- &quot;data.frame&quot; # Unsurprisingly this doesn&#39;t work very well print(mod) #&gt; [1] coefficients residuals effects rank fitted.values #&gt; [6] assign qr df.residual xlevels call #&gt; [11] terms model #&gt; &lt;0 Zeilen&gt; (oder row.names mit Länge 0) If you’ve used other OO languages, this might make you feel queasy. But surprisingly, this flexibility causes few problems: while you can change the type of an object, you never should. R doesn’t protect you from yourself: you can easily shoot yourself in the foot. As long as you don’t aim the gun at your foot and pull the trigger, you won’t have a problem. To avoid foot-bullet intersections when creating your own class, you should always provide: A constructor, new_x(), that efficiently creates new objects with the correct structure. For more complicated classes, you may also want to provide: A validator, validate_x(), that performs more expensive checks that the object has correct values. A helper, x(), that provides a convenient and neatly parameterised way for others to construct and validate (create) objects of this class. 13.2.1 Constructors S3 doesn’t provide a formal definition of a class, so it has no built-in way to ensure that all objects of a given class have the same structure (i.e. same attributes with the same types). Instead, you should enforce a consistent structure yourself by using a constructor. A constructor is a function whose job is to create objects of a given class, ensuring that they always have the same structure. There are three rules that a constructor should follow. It should: Be called new_class_name(). Have one argument for the base object, and one for each attribute. (More if the class can be subclassed, see inheritance.) Check the types of the base object and each attribute. Base R generally does not provide constructors (three exceptions are the internal .difftime(), .POSIXct(), and .POSIXlt()) so we’ll demonstrate constructors by filling in some missing pieces in base. (If you want to use these constructors in your own code, you can use the versions exported by the sloop package, which complete a few details that we skip here in order to focus on the core issues.) We’ll start with one of the simplest S3 classes in base R: Date, which is just a double with a class attribute. The constructor rules lead to the slightly awkward name new_Date(), because the existing base class uses a capital letter. I recommend using lower case class names to avoid this problem. new_Date &lt;- function(x) { stopifnot(is.double(x)) structure(x, class = &quot;Date&quot;) } new_Date(c(-1, 0, 1)) #&gt; [1] &quot;1969-12-31&quot; &quot;1970-01-01&quot; &quot;1970-01-02&quot; You can use the new_s3_*() helpers provided by the sloop to make this even simpler. They are wrappers around structure that require a class argument, and check the base type of x. new_Date &lt;- function(x) { sloop::new_s3_dbl(x, class = &quot;Date&quot;) } The purpose of the constructor is to help the developer (you). That means you can keep them simple, and you don’t need to optimise the error messages for user friendliness. If you expect others to create your objects, you should also create a friendly helper function, called class_name(), that we’ll describe shortly. A slightly more complicated example is POSIXct, which is used to represent date-times. It is again built on a double, but has an attribute that specifies the time zone, a length 1 character vector. R defaults to using the local time zone, which is represented by the empty string. To create the constructor, we need to make sure each attribute of the class gets an argument to the constructor. This gives us: new_POSIXct &lt;- function(x, tzone = &quot;&quot;) { stopifnot(is.double(x)) stopifnot(is.character(tzone), length(tzone) == 1) structure(x, class = c(&quot;POSIXct&quot;, &quot;POSIXt&quot;), tzone = tzone ) } new_POSIXct(1) #&gt; [1] &quot;1969-12-31 16:00:01 PST&quot; new_POSIXct(1, tzone = &quot;UTC&quot;) #&gt; [1] &quot;1970-01-01 00:00:01 UTC&quot; The constructor checks that x is a double, and that tzone is a length 1 character vector. We use stopifnot() here since the constructor is a developer focussed function so error messages don’t need to be that friendly. Note that POSIXct uses a class vector; we’ll come back to what that means in inheritance. Generally, the constructor should not check that the values are valid because such checks are often expensive. For example, our new_POSIXct() constructor does not check that tzone is a valid value, and we get a warning when the object is printed. x &lt;- new_POSIXct(1, &quot;Auckland NZ&quot;) x #&gt; Warning in as.POSIXlt.POSIXct(x, tz): unknown timezone &#39;Auckland NZ&#39; #&gt; [1] &quot;1970-01-01 00:00:01 GMT&quot; 13.2.2 Validators More complicated classes will require more complicated checks for validity. Take factors, for example. The constructor function only checks that the structure is correct: new_factor &lt;- function(x, levels) { stopifnot(is.integer(x)) stopifnot(is.character(levels)) structure( x, levels = levels, class = &quot;factor&quot; ) } So it’s possible to use this to create invalid factors: new_factor(1:5, &quot;a&quot;) #&gt; Error in as.character.factor(x): malformed factor new_factor(0:1, &quot;a&quot;) #&gt; Error in as.character.factor(x): malformed factor Rather than encumbering the constructor with complicated checks, it’s better to put them in a separate function. This is a good idea because it allows you to cheaply create new objects when you know that the values are correct, and to re-use the checks in other places. validate_factor &lt;- function(x) { values &lt;- unclass(x) levels &lt;- attr(x, &quot;levels&quot;) if (!all(!is.na(values) &amp; values &gt; 0)) { stop( &quot;All `x` values must be non-missing and greater than zero&quot;, call. = FALSE ) } if (length(levels) &lt; max(values)) { stop( &quot;There must at least as many `levels` as possible values in `x`&quot;, call. = FALSE ) } x } validate_factor(new_factor(1:5, &quot;a&quot;)) #&gt; Error: There must at least as many `levels` as possible values in `x` validate_factor(new_factor(0:1, &quot;a&quot;)) #&gt; Error: All `x` values must be non-missing and greater than zero This function is called primarily for its side-effects (throwing an error if the object is invalid) so you’d expect it to invisibly return its primary input. However, unlike most functions called for their side effects, its useful for validation methods to return visibly, as we’ll see next. 13.2.3 Helpers If you want others to construct objects from your class, you should also provide a helper method that makes their life as easy as possible. This should have the same name as the class, and should be parameterised in a convenient way. factor() is a good example of this as well: you want to automatically derive the internal representation from a vector. The simplest possible implementation looks something like this: factor &lt;- function(x, levels = unique(x)) { ind &lt;- match(x, levels) validate_factor(new_factor(ind, levels)) } factor(c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;)) #&gt; [1] a a b #&gt; Levels: a b The validator prevents the construction of invalid objects, but for a real helper you’d spend more time creating user friendly error messages. factor(c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;), levels = &quot;a&quot;) #&gt; Error: All `x` values must be non-missing and greater than zero In base R, neither Date nor POSIXct has a helper function. Instead there are two ways to construct them: By coercing from another type with as.Date() and as.POSIXct(). These functions should be S3 generics, so we’ll come back to them in coercion. With a helper function that either parses a string (strptime()) or creates a date from individual components (ISODatetime()). These missing helpers mean that there’s no obvious default way to create a date or date-time in R. We can fill in those missing pieces with a couple of helpers: Date &lt;- function(year, month, day) { as.Date(ISOdate(year, month, day, tz = &quot;&quot;)) } POSIXct &lt;- function(year, month, day, hour, minute, sec, tzone = &quot;&quot;) { ISOdatetime(year, month, day, hour, minute, sec, tz = tzone) } These helpers fill a useful role, but are not computationally efficient: behind the scenes ISODatetime() works by pasting the components into a string and then using strptime(). More efficient equivalents are lubridate::make_datetime() and lubridate::make_date(). 13.2.4 Object styles S3 gives you the freedom to build a new class on top of any existing base type. So far, we’ve focussed on vector-style where you take an existing vector type and add some attributes. Importantly, a single vector-style object represents multiple values. There are two other important styles: scalar-style and data-frame-style. Each scalar-style object represents a single “value”, and are built on top of named lists. This is the style that you are most likely to use in practice. The constructor for the scalar type is slightly different because the arguments become named elements of the list, rather than attributes. new_scalar_class &lt;- function(x, y, z) { structure( list( x = x, y = y, z = z ), class = &quot;scalar_class&quot; ) } (For a real constructor, you’d also check that the x, y, and z fields are the types that you expect.) In base R, the most important example of this style is lm, the class returned when you fit a linear model: mod &lt;- lm(mpg ~ wt, data = mtcars) typeof(mod) #&gt; [1] &quot;list&quot; names(mod) #&gt; [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; #&gt; [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; #&gt; [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; The data-frame-style builds on top of a data frame (a named list where each element is a vector of the same length), and adds additional attributes to store important metadata. A data-frame-style constructor looks like: new_df_class &lt;- function(df, attr1, attr2) { stopifnot(is.data.frame(df)) structure( df, attr1 = attr1, attr2 = attr2, class = c(&quot;df_class&quot;, &quot;data.frame&quot;) ) } The most common data-frame-style class is the tibble, a modern reimagining of the data frame provided by the tibble package, and used extensively within the tidyverse. Collectively, we’ll call the attributes of a vector-style or data-frame-style class and the names of a list-style class the fields of an object. When creating your own classes, you should pick the vector style if your class closely resembles an existing vector type. Otherwise, use a scalar (list) style. The scalar type is generally easier to work with because implementing a full range of convenient vectorised methods is usually a lot of work. It’s typically obvious when you need to use a data-frame-style. 13.2.5 Exercises Categorise the objects returned by lm(), factor(), table(), as.Date(), ecdf(), ordered(), I() into “vector”, “scalar”, and “other”. Write a constructor for difftime objects. What base type are they built on? What attributes do they use? You’ll need to consult the documentation, read some code, and perform some experiments. Write a constructor for data.frame objects. What base type is a data frame built on? What attributes does it use? What are the restrictions placed on the individual elements? What about the names? Enhance our factor() helper to have better behaviour when one or more values is not found in levels. What does base::factor() do in this situation? Carefully read the source code of factor(). What does it do that our constructor does not? What would a constructor function for lm objects, new_lm(), look like? Why is a constructor function less useful for linear models? 13.3 Generics and methods The job of an S3 generic is to perform method dispatch, i.e. find the function designed to work specifically for the given class. S3 generics have a simple structure: they call UseMethod(), which then calls the right method. UseMethod() takes two arguments: the name of the generic function (required), and the argument to use for method dispatch (optional). If you omit the second argument it will dispatch based on the first argument, which is what I generally advise. # Dispatches on x generic &lt;- function(x, y, ...) { UseMethod(&quot;generic&quot;) } # Dispatches on y generic2 &lt;- function(x, y, ...) { UseMethod(&quot;generic2&quot;, y) } Note that you don’t pass any of the arguments of the generic to UseMethod(); it uses black magic to pass them on automatically. Generally, you should avoid doing any computation in a generic, because the semantics are complicated and few people know the details. In general, any modifications to the arguments of the generic will be undone, leading to much confusion. A generic isn’t useful without some methods, which are just functions that follow a naming scheme (generic.class). Because a method is just a function with a special name, you can call methods directly, but you generally shouldn’t. The main reason to call the method directly is that it sometimes leads to considerable performance improvements. See performance for an example. generic.foo &lt;- function(x, y, ...) { message(&quot;foo method&quot;) } generic(new_s3_scalar(class = &quot;foo&quot;)) #&gt; foo method You can see all the methods defined for a generic with s3_methods_generic(): s3_methods_generic(&quot;generic&quot;) #&gt; # A tibble: 2 x 4 #&gt; generic class visible source #&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; #&gt; 1 generic foo T .GlobalEnv #&gt; 2 generic skeleton T methods Note the false positive: generic.skeleton() is not a method for our generic but an existing function in the methods package. It’s picked up because method definition relies only on a naming convention. This is another reason that you should avoid using . in non-method function names. Remember that apart from methods that you’ve created, and those defined in the base package, most S3 methods will not be directly accessible. You’ll need to use getS3method(&quot;generic&quot;, &quot;class&quot;) to see their source code. 13.3.1 Coercion Many S3 objects can be naturally created from an existing object through coercion. If this is the case for your class, you should provide a coercion function, an S3 generic called as_class_name. Base R generally does not follow this convention, which can cause problems as illustrated by as.factor(): The name is confusing, since as.factor() is not the factor method of the as() generic. as.factor() is not a generic, which means that if you create a new class that could be usefully converted to a factor, you can not extend as.factor(). We can fix these issues by creating a new generic coercion function and providing it with some methods: as_factor &lt;- function(x, ...) { UseMethod(&quot;as_factor&quot;) } Every as_y() generic should have a y method that returns its input unchanged: as_factor.factor &lt;- function(x, ...) x This ensures that as_factor() works if the input is already a factor. Two useful methods would be for character and integer vectors. as_factor.character &lt;- function(x, ...) { factor(x, levels = unique(x)) } as_factor.integer &lt;- function(x, ...) { factor(x, levels = as.character(unique(x))) } Typically the coercion methods will either call the constructor or the helper; pick the function that makes the code simpler. Here the helper is simplest. If you use the constructor, remember to also call the validator function. If you think your coercion function will be frequently used, it’s worth providing a default method that gives a better error message. Default methods are called when no other method is appropriate, and are discussed in more detail in inheritance. as_factor(1) #&gt; Error in UseMethod(&quot;as_factor&quot;): nicht anwendbare Methode für &#39;as_factor&#39; auf Objekt der Klasse &quot;c(&#39;double&#39;, &#39;numeric&#39;)&quot; angewendet as_factor.default &lt;- function(x, ...) { stop( &quot;Don&#39;t know how to coerce object of class &quot;, paste(class(x), collapse = &quot;/&quot;), &quot; into a factor&quot;, call. = FALSE ) } as_factor(1) #&gt; Error: Don&#39;t know how to coerce object of class numeric into a factor 13.3.2 Arguments Methods should always have the same arguments as their generics. This is not usually enforced, but it is good practice because it will avoid confusing behaviour. If you do eventually turn your code into a package, R CMD check will enforce it, so it’s good to get into the habit now. There is one exception to this rule: if the generic has ..., the method must still have all the same arguments (including ...), but can also have its own additional arguments. This allows methods to take additional arguments, which is important because you don’t know what additional arguments that a method for someone else’s class might need. The downside of using ..., however, is that any misspelled arguments will be silently swallowed. 13.3.3 Exercises Read the source code for t() and t.test() and confirm that t.test() is an S3 generic and not an S3 method. What happens if you create an object with class test and call t() with it? Why? x &lt;- structure(1:10, class = &quot;test&quot;) t(x) #&gt; #&gt; One Sample t-test #&gt; #&gt; data: x #&gt; t = 6, df = 9, p-value = 3e-04 #&gt; alternative hypothesis: true mean is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 3.33 7.67 #&gt; sample estimates: #&gt; mean of x #&gt; 5.5 Carefully read the documentation for UseMethod() and explain why the following code returns the results that it does. What two usual rules of function evaluation does UseMethod() violate? g &lt;- function(x) { x &lt;- 10 y &lt;- 10 UseMethod(&quot;g&quot;) } g.default &lt;- function(x) c(x = x, y = y) x &lt;- 1 y &lt;- 1 g(x) #&gt; x y #&gt; 1 10 13.4 Method dispatch At a high-level, S3 method dispatch is simple, and revolves around two functions, UseMethod() and NextMethod(). You’ll learn about these two functions below, and then we’ll come back to some of the additional wrinkles in dispatch details. 13.4.1 UseMethod() The purpose of UseMethod() is to find the appropriate method to call given a generic and a class. It does this by creating a vector of function names, paste0(&quot;generic&quot;, &quot;.&quot;, c(class(x), &quot;default&quot;)), and looking for each method in turn. As soon as it finds a matching method, it calls it. If no matching method is found, it throws an error. To explore dispatch, we’ll use sloop::s3_dispatch(). You give it a call to an S3 generic, and it lists all the possible methods, noting which ones exist. For example, what happens when you try and print a POSIXct object? x &lt;- Sys.time() s3_dispatch(print(x)) #&gt; -&gt; print.POSIXct #&gt; print.POSIXt #&gt; * print.default print() will look for three possible methods, of which two exist, and one, print.POSIXct(), will be called. The last method is always the “default” method. This doesn’t correspond to a specific class, so is a useful catch all. 13.4.2 NextMethod() Method dispatch usually terminates as soon as a matching method is found. However, methods can explicitly choose to call the next available method using NextMethod(). This is useful because it allows you to rely on code that others have already written, which we’ll come back to in inheritance. Let’s make NextMethod() concrete with an example. Here, I define a new generic (“showoff”) with three methods. Each method signals that it’s been called, and then calls the “next” method: showoff &lt;- function(x) { UseMethod(&quot;showoff&quot;) } showoff.default &lt;- function(x) { message(&quot;showoff.default&quot;) TRUE } showoff.a &lt;- function(x) { message(&quot;showoff.a&quot;) NextMethod() } showoff.b &lt;- function(x) { message(&quot;showoff.b&quot;) NextMethod() } Let’s create a dummy object with classes “b” and “a”. s3_dispatch() shows that all three potential methods are available: x &lt;- new_s3_scalar(class = c(&quot;b&quot;, &quot;a&quot;)) s3_dispatch(showoff(x)) #&gt; -&gt; showoff.b #&gt; * showoff.a #&gt; * showoff.default When you call NextMethod() it finds and calls the next available method in the dispatch list. When we call showoff(), the method for b forwards to the method for a, which forwards to the default method. showoff(x) #&gt; showoff.b #&gt; showoff.a #&gt; showoff.default #&gt; [1] TRUE Like UseMethod(), the precise semantics of NextMethod() are complex. It doesn’t actually work with the class attribute of the object, but instead uses a special global variable (.Class) to keep track of which method to call next. This means that modifying the argument that is dispatched upon has no impact, and you should avoid modifying the object that is being dispatched on. Generally, you call NextMethod() without any arguments. However, if you do give arguments, they are passed on to the next method, as if they’d been supplied to the generic. 13.4.3 Exercises Which base generic has the greatest number of defined methods? Explain what is happening in the following code. generic2 &lt;- function(x) UseMethod(&quot;generic2&quot;) generic2.a1 &lt;- function(x) &quot;a1&quot; generic2.a2 &lt;- function(x) &quot;a2&quot; generic2.b &lt;- function(x) { class(x) &lt;- &quot;a1&quot; NextMethod() } generic2(new_s3_scalar(class = c(&quot;b&quot;, &quot;a2&quot;))) #&gt; [1] &quot;a2&quot; 13.5 Inheritance The class attribute is not limited to a single string, but can be a character vector. This, along with S3 method dispatch and NextMethod(), gives a surprising amount of flexibility that can be used creatively to reduce code duplication. However, this flexibility can also lead to code that is hard to understand or reason about, so you are best constraining yourself to simple styles of inheritance. Here we will focus on defining subclasses that inherit their fields, and some behaviour, from a parent class. Subclasses use a character vector for the class attribute. There are two examples of subclasses that you might have come across in base R: Generalised linear models are a generalisation of linear models that allow the error term to belong to a richer set of distributions, not just the normal distribution like the linear model. This is a natural case for the use of inheritance and indeed, in R, glm() returns objects of class c(&quot;glm&quot;, &quot;lm&quot;). Ordered factors are used when the levels of a factor have some intrinsic ordering, like c(&quot;Good&quot;, &quot;Better&quot;, &quot;Best&quot;). Ordered factors are produced by ordered() which returns an object with class c(&quot;ordered&quot;, &quot;factor&quot;). You can think of the glm class “inheriting” behaviour from the lm class, and the ordered class inheriting behaviour from the factor class because of the way method dispatch works. If there is a method available for the subclass, R will use it, otherwise it will fall back to the “parent” class. For example, if you “plot” a glm object, it falls back to the lm method, but if you compute the ANOVA, it uses a glm-specific method. mod1 &lt;- glm(mpg ~ wt, data = mtcars) s3_dispatch(plot(mod1)) #&gt; plot.glm #&gt; -&gt; plot.lm #&gt; * plot.default s3_dispatch(anova(mod1)) #&gt; -&gt; anova.glm #&gt; * anova.lm #&gt; anova.default 13.5.1 Constructors There are three principles to adhere to when creating a subclass: A subclass should be built on the same base type as a parent. The class() of the subclass should be of the form c(subclass, parent_class) The fields of the subclass should include the fields of the parent. And these properties should be enforced by the constructor. When you create a class, you need to decide if you want to allow subclasses, because it requires changes to the constructor and careful thought in your methods. To allow subclasses, the parent constructor needs to have ... and subclass arguments: new_my_class &lt;- function(x, y, ..., subclass = NULL) { stopifnot(is.numeric(x)) stopifnot(is.logical(y)) structure( x, y = y, ..., class = c(subclass, &quot;my_class&quot;) ) } Then the implementation of the subclass constructor is simple: it checks the types of the new fields, then calls the parent constructor. new_subclass &lt;- function(x, y, z) { stopifnot(is.character(z)) new_my_class(x, y, z, subclass = &quot;subclass&quot;) } If you wanted to allow this subclass to be futher subclassed, you’d need to include ... and subclass arguments: new_subclass &lt;- function(x, y, z, ..., subclass = NULL) { stopifnot(is.character(z)) new_my_class(x, y, z, ..., subclass = c(subclass, &quot;subclass&quot;)) } If your subclass is more complicated, you’d also provide validator and helper functions, as described previously. 13.5.2 Coercion You also need to make sure that there’s some way to convert the subclass back to the parent class. The best way to do that is to add a method to the coercion generic. Generally, this method should call the parent constructor: as_my_class.sub_class &lt;- function(x) { new_my_class(attr(x, &quot;x&quot;), attr(x, &quot;y&quot;)) } 13.5.3 Methods The goal of creating a subclass is to reuse as much code as possible from the parent class. This means that you should not have to define every method that the parent class provides (if you do, reconsider if you actually need a subclass!). Generally, defining new methods is straightforward: you simply create a new method (generic.subclass) whenever the parent method doesn’t do quite the right thing. In many cases, the new method will be able to call NextMethod() in order to take advantage of the computation done in the parent. One wrinkle arises when you have methods that return the same type of object as the primary input. For example, dplyr has many functions (arrange(), summarise(), mutate(), …) that input a data frame (or data frame-like object) and output a modified version of that data frame. Imagine you want to store the provenance of each data frame, i.e. who created it and when. To do so, you might create a data frame subclass called provenance: new_provenance &lt;- function(data, author, date = Sys.Date()) { stopifnot(is.data.frame(data)) stopifnot(is.character(author), length(author) == 1) stopifnot(is.Date(date), length(date) == 1) structure( data, author = author, date = date, class = c(&quot;provenance&quot;, &quot;data.frame&quot;) ) } And now you want to make this class work with dplyr. The class doesn’t change any of the computation related to the data frame, it just needs to preserve the attributes, which dplyr doesn’t know anything about. That means you need to provide a method for each dplyr generic. The computation is unchanged, so you can use NextMethod() to do all the hard work, but you need to manually reconstruct the provenance object. arrange.provenance &lt;- function(.data, ...) { new_provenance( NextMethod(), author = attr(.data, &quot;author&quot;), date = attr(.data, &quot;date&quot;) ) } mutate.provenance &lt;- function(.data, ...) { new_provenance( NextMethod(), author = attr(.data, &quot;author&quot;), date = attr(.data, &quot;date&quot;) ) } To do this for all the dplyr generics would require a lot of copying and pasting. Let’s reduce some of that duplication by taking advantage of sloop::reconstruct(). reconstruct() is a generic function designed to reconstruct a subclass from an instance of the parent class, typically created by NextMethod(), and the original subclass. In other words, the job of a reconstructor is to take an object from a parent class, and copy over attributes from the subclass. (Note that reconstruct() is unusual in that it dispatches on the second argument. This allows a more natural specification.) reconstruct.provenance &lt;- function(new, old) { new_provenance( new, author = attr(old, &quot;author&quot;), date = attr(old, &quot;date&quot;) ) } Now we can rewrite the methods to minimise the amount of duplicated code: arrange.provenance &lt;- function(.data, ...) { reconstruct(NextMethod(), .data) } mutate.provenance &lt;- function(.data, ...) { reconstruct(NextMethod(), .data) } This duplicated code could be avoided completely if arrange.data.frame(), provided by dplyr, called reconstruct() for you. And indeed, a future version of that function will. When designing a class that can be subclassed, you need to carefully think through these issues. Generally, whenever you implement a method that returns the same type of object as the primary input, you should call reconstruct() to ensure that it also works for subclasses. That way implementors of a subclass will only need to provide methods when the computation is actually different. 13.5.4 Exercises The ordered class is a subclass of factor, but it’s implemented in a very ad hoc way in base R. Implement it in a principled way by building a constructor and an as_ordered generic. f1 &lt;- factor(&quot;a&quot;, c(&quot;a&quot;, &quot;b&quot;)) as.factor(f1) #&gt; [1] a #&gt; Levels: a b as.ordered(f1) # loses levels #&gt; [1] a #&gt; Levels: a What classes have a method for the Math group generic in base R? Read the source code. How do the methods work? R has two classes for representing date time data, POSIXct and POSIXlt, which both inherit from POSIXt. Which generics have different behaviours for the two classes? Which generics share the same behaviour? 13.6 Dispatch details This chapter concludes with a few additional details about method dispatch that is not well documented elsewhere. It is safe to skip these details if you’re new to S3. 13.6.1 Environments and namespaces The precise rules for where a generic looks for the methods are a little complicated because there are two paths for discovery: In the calling environment of the function that called the generic. In the special .__S3MethodsTable__. object in the function environment of the generic. Every package has an .__S3MethodsTable__ which lists all the S3 methods exported by the package. These details are not usually important, but are necessary in order for S3 generics to find the correct method when the generic and method are in different packages. 13.6.2 S3 and base types What happens when you call an S3 generic with a non-S3 object, i.e. an object that doesn’t have the class attribute set? You might think it would dispatch on what class() returns: class(matrix(1:5)) #&gt; [1] &quot;matrix&quot; But unfortunately dispatch actually occurs on the implicit class, which has three components: “array” or “matrix” (if the object has dimensions). typeof() (with a few minor tweaks). If it’s “integer” or “double”, “numeric”. There is no base function that will compute the implicit class, but you can use a helper from the sloop package: s3_class(matrix(1:5)) #&gt; [1] &quot;matrix&quot; &quot;integer&quot; &quot;numeric&quot; s3_dispatch() knows about the implicit class, so use it if you’re ever in doubt about method dispatch: s3_dispatch(print(matrix(1:5))) #&gt; print.matrix #&gt; print.integer #&gt; print.numeric #&gt; -&gt; print.default Note that this can lead to different dispatch for objects that look similar: x1 &lt;- 1:5 class(x1) #&gt; [1] &quot;integer&quot; s3_dispatch(mean(x1)) #&gt; mean.integer #&gt; mean.numeric #&gt; -&gt; mean.default x2 &lt;- structure(x1, class = &quot;integer&quot;) class(x2) #&gt; [1] &quot;integer&quot; s3_dispatch(mean(x2)) #&gt; mean.integer #&gt; -&gt; mean.default 13.6.3 Internal generics Some S3 generics, like [, sum(), and cbind(), don’t call UseMethod() because they are implemented in C. Instead, they call the C functions DispatchGroup() or DispatchOrEval(). These functions are called internal generics, because they do dispatch internally, in C code. Internal generics only exist in base R, so you can not create an internal generic in a package. s3_dispatch() shows internal generics by including the name of the generic at the bottom of the method class. If this method is called, all the work happens in C code, typically using [switchpatch]. s3_dispatch(Sys.time()[1]) #&gt; -&gt; [.POSIXct #&gt; [.POSIXt #&gt; [.default #&gt; * [ For performance reasons, internal generics do not dispatch to methods unless the class attribute has been set (is.object() is true). This means that internal generics do not use the implicit class. Again, if you’re confused, rely on s3_dispatch() to show you the difference. x &lt;- sample(10) class(x) #&gt; [1] &quot;integer&quot; s3_dispatch(x[1]) #&gt; [.integer #&gt; [.numeric #&gt; [.default #&gt; -&gt; [ class(y) #&gt; [1] &quot;numeric&quot; s3_dispatch(mtcars[1]) #&gt; -&gt; [.data.frame #&gt; [.default #&gt; * [ 13.6.4 Group generics Group generics are the most complicated part of S3 method dispatch because they involve both NextMethod() and internal generics. Group generics are worth learning about, however, because they allow you to implement a whole swath of methods with one function. Like internal generics, they only exist in base R, and you can not define your own group generic. Base R has four group generics, which are made up of the following generics: Math: abs, sign, sqrt, floor, cos, sin, log, exp, … Ops: +, -, *, /, ^, %%, %/%, &amp;, |, !, ==, !=, &lt;, &lt;=, &gt;=, &gt; Summary: all, any, sum, prod, min, max, range Complex: Arg, Conj, Im, Mod, Re Defining a single group generic for your class overrides the default behaviour for all of the members of the group. Methods for group generics are looked for only if the methods for the specific generic do not exist: s3_dispatch(sum(Sys.time())) #&gt; sum.POSIXct #&gt; sum.POSIXt #&gt; sum.default #&gt; -&gt; Summary.POSIXct #&gt; Summary.POSIXt #&gt; Summary.default #&gt; * sum Most group generics involve a call to NextMethod(). For example, take difftime() objects. If you look at the method dispatch for abs(), you’ll see there’s a Math group generic defined. y &lt;- as.difftime(10, units = &quot;mins&quot;) s3_dispatch(abs(y)) #&gt; abs.difftime #&gt; abs.default #&gt; -&gt; Math.difftime #&gt; Math.default #&gt; * abs Math.difftime basically looks like this: Math.difftime &lt;- function(x, ...) { new_difftime(NextMethod(), units = attr(x, &quot;units&quot;)) } It dispatches to the next method, here the internal default, to perform the actual computation, then copies back over the the class and attributes. Note that inside a group generic function a special variable .Generic provides the actual generic function called. This can be useful when producing error messages, and can sometimes be useful if you need to manually re-call the generic with different arguments. 13.6.5 Double dispatch Generics in the “Ops” group, which includes the two-argument mathematical and logical operators like - and &amp;, implement a special type of method dispatch. They dispatch on the type of both of the arguments, so called double dispatch. This is necessary to preserve the commutative property of many operators, i.e. a + b should equal b + a. Take the following simple example: date &lt;- as.Date(&quot;2017-01-01&quot;) integer &lt;- 1L date + integer #&gt; [1] &quot;2017-01-02&quot; integer + date #&gt; [1] &quot;2017-01-02&quot; If + dispatched only on the first argument, it would return different values for the two cases. To overcome this problem, generics in the Ops group use a slightly different strategy from usual. Rather than doing a single method dispatch, they do two, one for each input. There are three possible outcomes of this lookup: The methods are the same, so it doesn’t matter which method is used. The methods are different, and R calls the first method with a warning. One method is internal, in which case R calls the other method. For the example above, we can look at the possible methods for each argument, taking advantage of the fact that we can call + with a single argument. In this case, the second argument would dispatch to the internal + function, so R will call +.Date. s3_dispatch(+date) #&gt; -&gt; +.Date #&gt; +.default #&gt; * Ops.Date #&gt; Ops.default #&gt; * + s3_dispatch(+integer) #&gt; +.integer #&gt; +.numeric #&gt; +.default #&gt; Ops.integer #&gt; Ops.numeric #&gt; Ops.default #&gt; -&gt; + Let’s take a look at another case. What happens if you try and add a date to a factor? There is no method in common, so R calls the internal + method (which preserves the attributes of the LHS), with a warning. factor &lt;- factor(&quot;a&quot;) s3_dispatch(+factor) #&gt; +.factor #&gt; +.default #&gt; -&gt; Ops.factor #&gt; Ops.default #&gt; * + date + factor #&gt; Warning: Inkompatible Methoden (&quot;+.Date&quot;, &quot;Ops.factor&quot;) für &quot;+&quot; #&gt; [1] &quot;2017-01-02&quot; factor + date #&gt; Warning: Inkompatible Methoden (&quot;Ops.factor&quot;, &quot;+.Date&quot;) für &quot;+&quot; #&gt; Error in as.character.factor(x): malformed factor Finally, what happens if we try to substract a POSIXct from a POSIXlt? A common -.POSIXt method is found and called. dt1 &lt;- as.POSIXct(date) dt2 &lt;- as.POSIXlt(date) s3_dispatch(-dt1) #&gt; -.POSIXct #&gt; -&gt; -.POSIXt #&gt; -.default #&gt; Ops.POSIXct #&gt; * Ops.POSIXt #&gt; Ops.default #&gt; * - s3_dispatch(-dt2) #&gt; -.POSIXlt #&gt; -&gt; -.POSIXt #&gt; -.default #&gt; Ops.POSIXlt #&gt; * Ops.POSIXt #&gt; Ops.default #&gt; * - dt1 - dt2 #&gt; Time difference of 0 secs 13.6.6 Exercises Math.difftime() is more complicated than I described. Why? "],
["s4.html", "14 S4 14.1 Classes 14.2 Generics and methods 14.3 Method dispatch 14.4 S4 and existing code", " 14 S4 Like S3, S4 implements functional OOP, but is much more rigorous and strict. There are three main differences between S3 and S4: S4 classes have formal definitions provided by a call to setClass(). An S4 class can have multiple parents (multiple inheritance). The fields of an S4 object are not attributes or named elements, but instead are called slots and are accessed with the special @ operator. Methods are not defined with a naming convention, but are instead defined by a call to setMethod(). S4 generics can dispatch on multiple arguments (multiple dispatch). A good overview of the motivation of S4 and its historical context can be found in Chambers and others (2014), https://projecteuclid.org/download/pdfview_1/euclid.ss/1408368569. S4 is a rich system, and it’s not possible to cover all of it in one chapter. Instead, we’ll focus on what you need to know to read most S4 code, and write basic S4 components. Unfortunately there is not one good reference for S4 and as you move towards more advanced usage, you will need to piece together needed information by carefully reading the documentation and performing experiments. Some good places to start are: Bioconductor course materials, a list of all courses taught by Bioconductor, a big user of S4. One recent (2017) course by Martin Morgan and Hervé Pagès is S4 classes and methods. S4 questions on stackoverflow answered by Martin Morgan. Software for Data Analysis, a book by John Chambers. All S4 related functions live in the methods package. This package is always available when you’re running R interactively, but may not be available when running R in batch mode (i.e. from Rscript). For this reason, it’s a good idea to call library(methods) whenever you use S4. This also signals to the reader that you’ll be using the S4 object system. library(methods) 14.1 Classes Unlike S3, S4 classes have a formal definition. To define an S4 class, you must define three key properties: The class name. By convention, S4 class names use UpperCamelCase. A named character vector that describes the names and classes of the slots (fields). For example, a person might be represented by a character name and a numeric age: c(name = &quot;character&quot;, age = &quot;numeric&quot;). The pseudo-class “ANY” allows a slot to accept objects of any type. The name of a class (or classes) to inherit behaviour from, or in S4 terminology, the classes that it contains. Slots and contains can specify the names of S4 classes, S3 classes (if registered), and base types. We’ll go into more detail about non-S4 classes at the end of the chapter, in S4 and existing code. To create a class, you call setClass(), supplying these three properties. Lets make this concrete with an example. Here we create two classes: a person with character name and numeric age, and an Employee that inherits slots and methods from Person, adding an additional boss slot that must be a Person. setClass() returns a low-level constructor function, which should be given the class name with a . prefix. .Person &lt;- setClass(&quot;Person&quot;, slots = c( name = &quot;character&quot;, age = &quot;numeric&quot; ) ) .Employee &lt;- setClass(&quot;Employee&quot;, contains = &quot;Person&quot;, slots = c( boss = &quot;Person&quot; ) ) setClass() has 10 other arguments, but they are all either deprecated or not recommended. If you have existing S4 code that uses them, I’d recommend carefully reading the documentation and upgrading to modern practice. We can now use the constructor to create an object from that class: hadley &lt;- .Person(name = &quot;Hadley&quot;, age = 37) hadley #&gt; An object of class &quot;Person&quot; #&gt; Slot &quot;name&quot;: #&gt; [1] &quot;Hadley&quot; #&gt; #&gt; Slot &quot;age&quot;: #&gt; [1] 37 It’s also possible to create an instance using new() and the name of the class. This is not recommended because it introduces some ambiguity. What happens if there are two packages that both define the Person class? hadley2 &lt;- new(&quot;Person&quot;, name = &quot;Hadley&quot;, age = 37) In most programming languages, class definition occurs at compile-time, and object construction occurs later, at run-time. In R, however, both definition and construction occur at run time. When you call setClass(), you are registering a class definition in a (hidden) global variable. As with all state-modifying functions you need to use setClass() with care. It’s possible to create invalid objects if you redefine a class after already having instantiated an object: .A &lt;- setClass(&quot;A&quot;, slots = c(x = &quot;numeric&quot;)) a &lt;- .A(x = 10) .A &lt;- setClass(&quot;A&quot;, slots = c(a_different_slot = &quot;numeric&quot;)) a #&gt; An object of class &quot;A&quot; #&gt; Slot &quot;a_different_slot&quot;: #&gt; Error in slot(object, what): kein Slot des Namens &quot;a_different_slot&quot; für dieses Objekt der Klasse &quot;A&quot; This isn’t usually a problem, because you’ll define a class once, then leave the definition alone. If you want to enforce a single class definition, you can “seal” it: setClass(&quot;Sealed&quot;, sealed = TRUE) setClass(&quot;Sealed&quot;) #&gt; Error in setClass(&quot;Sealed&quot;): &quot;Sealed&quot; has a sealed class definition and cannot be redefined 14.1.1 Slots You can access the slots with @ or slot(): @ is equivalent to $, and slot() to [[. hadley@age #&gt; [1] 37 slot(hadley, &quot;age&quot;) #&gt; [1] 37 You can list all available slots with slotNames(): slotNames(hadley) #&gt; [1] &quot;name&quot; &quot;age&quot; Slots should be considered an internal implementation detail. That means: As a user, you should not reach into someone else’s object with @, but instead, look for a method that provides the information you want. As a developer, you should make sure that all public facing slots have their own accessor methods. We’ll come back how to implement accessors in [Accessors], once you’ve learned how S4 generics and methods work. 14.1.2 Helper The result of setClass() is a low-level constructor, which means that don’t need to write one yourself. However, this default constructor has three drawbacks: The constructor takes ..., not individual named slots. This mean that printing the function is not revealing, and autocomplete doesn’t have the data it needs to be helpful. .Person #&gt; class generator function for class &quot;Person&quot; from package &#39;.GlobalEnv&#39; #&gt; function (...) #&gt; new(&quot;Person&quot;, ...) If you don’t supply values for a slot, the constructor will automatically supply a default value: .Person() #&gt; An object of class &quot;Person&quot; #&gt; Slot &quot;name&quot;: #&gt; character(0) #&gt; #&gt; Slot &quot;age&quot;: #&gt; numeric(0) Here, you might prefer that name is required, or that age defaults to NA. While it’s not possible to create an S4 object with the wrong slots or slots of the wrong type: .Person(name = &quot;Hadley&quot;, age = &quot;thirty&quot;) #&gt; Error in validObject(.Object): invalid class &quot;Person&quot; object: invalid object for slot &quot;age&quot; in class &quot;Person&quot;: got class &quot;character&quot;, should be or extend class &quot;numeric&quot; .Person(name = &quot;Hadley&quot;, sex = &quot;male&quot;) #&gt; Error in initialize(value, ...): invalid name for slot of class &quot;Person&quot;: sex It is possible to create slots with the wrong lengths, or otherwise invalid values: .Person(name = &quot;Hadley&quot;, age = c(37, 99)) #&gt; An object of class &quot;Person&quot; #&gt; Slot &quot;name&quot;: #&gt; [1] &quot;Hadley&quot; #&gt; #&gt; Slot &quot;age&quot;: #&gt; [1] 37 99 Like with S3, we resolve these issues by writing a helper function. Person &lt;- function(name, age = NULL, ...) { if (is.null(age)) { age &lt;- rep(NA_real_, length(name)) } stopifnot(length(name) == length(age)) .Person(name = name, age = age) } This provides the behaviour that we want: # Name is now required Person() #&gt; Error in Person(): Argument &quot;name&quot; fehlt (ohne Standardwert) # And name and age must have same length Person(&quot;Hadley&quot;, age = c(30, 37)) #&gt; Error: length(name) == length(age) ist nicht TRUE # And if not supplied, age gets a default value of NA Person(&quot;Hadley&quot;) #&gt; An object of class &quot;Person&quot; #&gt; Slot &quot;name&quot;: #&gt; [1] &quot;Hadley&quot; #&gt; #&gt; Slot &quot;age&quot;: #&gt; [1] NA It is possible to achieve the same effect by implementing an initialize() method, but the initialize() generic has a complicated contract and it is very hard to get all the details right. To re-use checking code in a subclass, you can take advantage of a detail of the constructor: an unnamed argument is interpreted as predefined object from the parent class. For example, to define a constructor for the Employee class that reuses the Person helper, you first create a Person(), then pass that to the .Employee constructor. Employee &lt;- function(name, age, boss) { person &lt;- Person(name = name, age = age) .Employee(person, boss = boss) } As with S3, if the validity checking code is lengthy or expensive, you should pull it out into a separate function which the helper calls. 14.1.3 Introspection To determine what classes an object inherits from, use is(): is(hadley) #&gt; [1] &quot;Person&quot; To test if an object inherits from a specific class, use the second argument of is(): is(hadley, &quot;person&quot;) #&gt; [1] FALSE If you are using a class provided by a package you can get help on it with class?Person. 14.1.4 Exercises What happens if you define a new S4 class that doesn’t “contain” an existing class? (Hint: read about virtual classes in ?setClass.) Imagine you were going to reimplement ordered factors, dates, and data frames in S4. Sketch out the setClass() calls that you would use to define the classes. What should they inherit from? What slots should they use? 14.2 Generics and methods The job of a generic is to perform method dispatch, i.e. find the method designed to handle the combination of classes passed to the generic. Here you’ll learn how to define S4 generics and methods, then in the next section we’ll explore precisely how S4 method dispatch works. S4 generics have a similar structure to S3 generics, but are a little more formal. To create an new S4 generic, you call setGeneric() with a function that calls standardGeneric(). . setGeneric(&quot;myGeneric&quot;, function(x) standardGeneric(&quot;myGeneric&quot;)) Note that it is bad practice to use { in the generic function. This triggers a special case that is more expensive, and generally best avoided. Like setClass(), setGeneric() has many other arguments. There is only one that you need to know about: signature. This allows you to control the arguments that are used for method dispatch. If signature is not supplied, all arguments (apart from ...) are used. It is occassionally useful to remove arguments from dispatch. This allows you to require that methods provide arguments like verbose = TRUE or quiet = FALSE, but they don’t take part in dispatch. A generic isn’t useful without some methods, and in S4 you add methods with setMethod(). There are three important arguments: the name of the generic, the name of the class, and the method itself. setMethod(&quot;myGeneric&quot;, &quot;Person&quot;, function(x) { # method implementation }) (Again, just like setClass(), setMethod() has other arguments, but you should never use them.) 14.2.1 Show method As with S3, the most commonly defined S4 method controls printing, but in S4 we use a different generic: show(). When defining a method for an existing generic, you need to first determine the arguments. You can get those from the documentation or by looking at the formals of the generic: names(formals(getGeneric(&quot;show&quot;))) #&gt; [1] &quot;object&quot; Our show method needs to have a single argument object: setMethod(&quot;show&quot;, &quot;Person&quot;, function(object) { cat(is(object)[[1]], &quot;\\n&quot;, &quot; Name: &quot;, object@name, &quot;\\n&quot;, &quot; Age: &quot;, object@age, &quot;\\n&quot;, sep = &quot;&quot; ) }) hadley #&gt; Person #&gt; Name: Hadley #&gt; Age: 37 More formally, the second argument to setMethod() is called the signature. In S4, unlike S3, the signature can include multiple arguments. This makes method dispatch in S4 substantially more complicated, but avoids having to implement double-dispatch as a special case. We’ll talk more about multiple dispatch in the next section. 14.2.2 Accessor methods Slots are generally considered to be an internal implementation detail: they can change without warning and user code should avoid accessing them directly. Instead, all user-readble slots should get an accessor. If the slot is unique to the class, this can just be a function: person_name &lt;- function(x) x@name But typically, you will want to define a generic and provide a method for your class: setGeneric(&quot;name&quot;, function(x) standardGeneric(&quot;name&quot;)) setMethod(&quot;name&quot;, &quot;Person&quot;, function(x) x@name) name(hadley) #&gt; [1] &quot;Hadley&quot; If the slot is also writeable, you should provide a setter function. Typically this function will be more complicated than the getter because you’ll need to check that the new value is valid, or you may need to modify other slots. Here we make sure that this functions only allows changing the values, not the length: `person_name&lt;-` &lt;- function(x, value) { stopifnot(length(x@name) == length(value)) x@name &lt;- value x } Again, you’ll typically want to do this with a method: setGeneric(&quot;name&lt;-&quot;, function(x, value) standardGeneric(&quot;name&lt;-&quot;)) setMethod(&quot;name&lt;-&quot;, &quot;Person&quot;, function(x, value) { stopifnot(length(x@name) == length(value)) x@name &lt;- value x }) name(hadley) &lt;- &quot;Hadley Wickham&quot; name(hadley) #&gt; [1] &quot;Hadley Wickham&quot; 14.2.3 Coercion methods To coerce S4 object from one class to another, use as(). One nice feature of S4 is that it provides default coercion methods for you: mary &lt;- new(&quot;Person&quot;, name = &quot;Mary&quot;, age = 34) roger &lt;- new(&quot;Employee&quot;, name = &quot;Roger&quot;, age = 36, boss = mary) as(roger, &quot;Person&quot;) #&gt; Person #&gt; Name: Roger #&gt; Age: 36 The defaults are not always quite right. For example, what happens if we try and coerce a Person to an Employee? The coercion succeeds because the boss slot is “helpfully” filled in with a default object: mary_employee &lt;- as(mary, &quot;Employee&quot;) mary_employee@boss #&gt; Person #&gt; Name: #&gt; Age: We can override the default coercion to supply an informative error. setAs(&quot;Person&quot;, &quot;Employee&quot;, function(from) { stop(&quot;Can not coerce an Person to an Employee&quot;, call. = FALSE) }) as(mary, &quot;Employee&quot;) #&gt; Error: Can not coerce an Person to an Employee 14.2.4 Introspection To list all the methods that belong to a generic, or that are associated with a class, use sloop::s4_methods_generic() and s4_methods_class(): library(sloop) s4_methods_generic(&quot;initialize&quot;) #&gt; # A tibble: 14 x 4 #&gt; generic class visible source #&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; #&gt; 1 initialize .environment T &quot;&quot; #&gt; 2 initialize ANY T methods #&gt; 3 initialize array T &quot;&quot; #&gt; 4 initialize environment T &quot;&quot; #&gt; 5 initialize envRefClass T methods #&gt; 6 initialize externalRefMethod T &quot;&quot; #&gt; 7 initialize matrix T &quot;&quot; #&gt; 8 initialize MethodsList T &quot;&quot; #&gt; 9 initialize Module T Rcpp #&gt; 10 initialize mts T &quot;&quot; #&gt; 11 initialize oldClass T &quot;&quot; #&gt; 12 initialize signature T &quot;&quot; #&gt; 13 initialize traceable T &quot;&quot; #&gt; 14 initialize ts T &quot;&quot; s4_methods_class(&quot;Person&quot;) #&gt; # A tibble: 7 x 4 #&gt; generic class visible source #&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; #&gt; 1 coerce Person T R_GlobalEnv #&gt; 2 coerce Person T R_GlobalEnv #&gt; 3 coerce&lt;- Person T R_GlobalEnv #&gt; 4 myGeneric Person T R_GlobalEnv #&gt; 5 name Person T R_GlobalEnv #&gt; 6 name&lt;- Person T R_GlobalEnv #&gt; 7 show Person T R_GlobalEnv If you’re looking for the implementation of a specific method, you can use selectMethod(). You give it the name of the generic and the class (or classes) that it’s called with: selectMethod(&quot;show&quot;, &quot;Person&quot;) #&gt; Method Definition: #&gt; #&gt; function (object) #&gt; { #&gt; cat(is(object)[[1]], &quot;\\n&quot;, &quot; Name: &quot;, object@name, &quot;\\n&quot;, #&gt; &quot; Age: &quot;, object@age, &quot;\\n&quot;, sep = &quot;&quot;) #&gt; } #&gt; &lt;bytecode: 0x7fc24a09bf10&gt; #&gt; #&gt; Signatures: #&gt; object #&gt; target &quot;Person&quot; #&gt; defined &quot;Person&quot; If you’re using a method defined in a package, the easiest way to get help on it is to construct a valid call, and then put ? in front it. ? will use the arguments to figure out which help file you need: ?show(hadley) 14.2.5 Exercises In the definition of the generic, why is it necessary to repeat the name of the generic twice? What’s the difference between the generics generated by these two calls? setGeneric(&quot;myGeneric&quot;, function(x) standardGeneric(&quot;myGeneric&quot;)) setGeneric(&quot;myGeneric&quot;, function(x) { standardGeneric(&quot;myGeneric&quot;) }) What happens if you define a method with different argument names to the generic? What other ways can you find help for a method? Read ?&quot;?&quot; and summarise the details. 14.3 Method dispatch S4 dispatch is complicated because S4 has two important features: Multiple inheritance, i.e. a class can have multiple parents, Multiple dispatch, i.e. a generic can use multiple arguments to pick a method. These features make S4 very powerful, but can also make it hard to understand which method will get selected for a given combination of inputs. To explain method dispatch, we’ll start simple with single inheritance and single dispatch, and work our way up to the more complicated cases. To illustrate the ideas without getting bogged down in the details, we’ll use an imaginary class graph based on emoji: Emoji give us very compact class names (just one symbol) that evoke the relationships between the classes. It should be straightforward to remember that 😜 inherits from 😉 which inherits from 😶, and that 😎 inherits from both 🕶 and 🙂 14.3.1 Single dispatch Let’s start with the simplest case: a generic function that dispatches on a single class with a single parent. The method dispatch here is quite simple, and the same as S3, but this will serve to define the graphical conventions we’ll use for the more complex cases. There are two parts to this diagram: The top part, f(...), defines the scope of the diagram. Here we have a generic with one argument, and we’re going to explore method dispatch for a class hierarchy that is three levels deep. We’ll only ever look at a small fragment of the complete class graph. This keeps individual diagrams simple while helping you build intuition that you apply to more complex class graphs. The bottom part is the method graph and displays all the possible methods that could be defined. Methods that have been defined (i.e. with setMethod()) have a grey background. To find the method that gets called, you start with the class of the actual arguments, then follow the arrows until you find a method that exists. For example, if you called the function with an object of class 😉 you would follow the arrow right to find the method defined for the more general 😶 class. If no method is found, method dispatch has failed and you get an error. For this reason, class graphs should usually have methods defined for all the terminal nodes, i.e. those on the far right. There are two pseudo-classes that you can define methods for. These are called pseudo-classes because they don’t actually exist, but allow you to define useful behaviours. The first pseudo-class is “ANY”. This matches any class, and plays the same role as the default pseudo-class in S3. For technical reasons that we’ll get to later, the link to the “ANY” method is longer than the links between the other classes: The second pseudo-class is “MISSING”. If you define a method for this “class”, it will match whenever the argument is missing. It’s generally not useful for functions that take a single argument, but can be used for functions like + and - that behave differently depending on whether they have one or two arguments. 14.3.2 Multiple inheritance Things get more complicated when the class has multiple parents. The basic process remains the same: you start from the actual class supplied to the generic, then follow the arrows until you find a defined method. The wrinkle is now that there are multiple arrows to follow, so you might find multiple methods. If that happens, you pick the method that is closest, i.e. requires travelling the fewest arrows. (The method graph is a powerful metaphor that helps you understand how method dispatch works. However, implementing method dispatch in this way would be rather inefficient so the actual approach that S4 uses is somewhat different. You can read the details in ?Methods_Details) What happens if methods are the same distance? For example, imagine we’ve defined methods for 🕶 and 🙂, and we call the generic with 😎. Note that there’s no implementation for the 😶 class, as indicated by the red double outline. This is called an ambiguous method, and in diagrams I’ll illustrate it with a thick dotted border. When this happens in R, you’ll get a warning, and one of the two methods is basically picked at random (it uses the method that comes first in the alphabet). When you discover ambiguity you should always resolve it by providing a more precise method: The fallback “ANY” method still exists but the rules are little more complex. As indicated by the wavy dotted lines, the “ANY” method is always considered further away than a method for a real class. This means that it will never contribute to ambiguity. It is hard to simultaneously prevent ambiguity, ensure that every terminal method has an implementation, and minimise the number of defined methods (in order to benefit from OOP). For example, of the six ways to define only two methods for this call, only one is free from problems. For this reason, I recommend using multiple inheritance with extreme care: you will need to carefully think about the method graph and plan accordingly. 14.3.3 Multiple dispatch Once you understand multiple inheritance, understanding multiple dispatch is straightforward. You follow multiple arrows in the same way as previously, but now each method is specified by two classes (separated by a comma). I’m not going to show examples of dispatching on more than two arguments, but you can follow the basic principles to generate your own method graphs. The main difference between multiple inheritance and multiple dispatch is that there are many more arrows to follow. The following diagram shows four defined methods which produce two ambiguous cases: Multiple dispatch tends to be less tricky to work with than multiple inheritance because there are usually fewer terminal class combinations. In this example, there’s only one. That means, at a minimum, you can define a single method and have default behaviour for all inputs. 14.3.4 Multiple dispatch and multiple inheritance Of course you can combine multiple dispatch with multiple inheritance: A still more complicated case dispatches on two classes, both of which have multiple inheritance: However, as the method graph gets more and more complicated it gets harder and harder to predict which actual method will get called given a combination of inputs, and it gets harder and harder to make sure that you haven’t introduced ambiguity. I highly recommend avoiding the combination of the two. There are some techniques (like mixins) that allow you to tame this complexity, but I am not aware of a detailed treatment as applied to S4. 14.3.5 Exercises Take the last example which shows multiple dispatch over two classes that use multiple inheritance. What happens if you define a method for all terminal classes? Why does method dispatch not save us much work here? 14.4 S4 and existing code Even when writing new S4 code, you’ll still need to interact with existing S3 classes and functions, including existing S3 generics. This section describes how S4 classes, methods, and generics interact with existing code. 14.4.1 Classes In slots and contains you can use S4 classes, S3 classes, or the implicit class of a base type. To use an S3 class, you must first register it with setOldClass(). You call this function once for each S3 class, giving it the class attribute. For example, the following definitions are already provided by base R: setOldClass(&quot;data.frame&quot;) setOldClass(c(&quot;ordered&quot;, &quot;factor&quot;)) setOldClass(c(&quot;glm&quot;, &quot;lm&quot;)) Generally, these definitions should be provided by the creator of the S3 class. If you’re trying to build an S4 class on top of a S3 class provided by a package, it is better to request that the package maintainer add this call to the package, rather than running it yourself. If an S4 object inherits from an S3 class or a base type, it will have a special virtual slot called .Data. This contains the underlying base type or S3 object: RangedNumeric &lt;- setClass( &quot;RangedNumeric&quot;, contains = &quot;numeric&quot;, slots = c(min = &quot;numeric&quot;, max = &quot;numeric&quot;) ) rn &lt;- RangedNumeric(1:10, min = 1, max = 10) rn@min #&gt; [1] 1 rn@.Data #&gt; [1] 1 2 3 4 5 6 7 8 9 10 It is possible to define S3 methods for S4 generics, and S4 methods for S3 generics (provided you’ve called setOldClass()). However, it’s more complicated than it might appear at first glance, so make sure you thoroughly read ?Methods_for_S3. 14.4.2 Generics As well as creating a new generic from scratch (as shown in generics and methods), it’s also possible to convert an existing function to a generic. sides &lt;- function(object) 0 setGeneric(&quot;sides&quot;) In this case, the existing function becomes the default (“ANY”) method: selectMethod(&quot;sides&quot;, &quot;ANY&quot;) #&gt; Method Definition (Class &quot;derivedDefaultMethod&quot;): #&gt; #&gt; function (object) #&gt; 0 #&gt; #&gt; Signatures: #&gt; object #&gt; target &quot;ANY&quot; #&gt; defined &quot;ANY&quot; Note that setMethod() will automatically call setGeneric() if the first argument isn’t already a generic, enabling you to turn any existing function into an S4 generic. I think it is ok to convert an existing S3 generic to S4, but you should avoid converting regular functions because it makes code harder to use (and requires coordination if done by multiple packages). 14.4.3 Exercises References "],
["r6.html", "15 R6 15.1 Classes and methods 15.2 Controlling access 15.3 Reference semantics", " 15 R6 This chapter describes the R6 object system. Unlike S3 and S4, it provides encapsulated OO, which means that: R6 methods belong to objects, not generics. R6 objects are mutable: the usual copy-on-modify semantics do not apply. These properties make R6 objects behave more like objects in programming languages such as Python, Ruby and Java. This does not mean that R6 is good, and S3 and S4 are bad, it just means that R has a different heritage than most modern mainstream programming languages. R6 is very similar to a built-in OO system called reference classes, or RC for short. I’m going to teach you R6 instead of RC for four reasons: R6 is much simpler. Both R6 and RC are built on top of environments, but while R6 uses S3, RC uses S4. R6 is only ~500 lines of R code (and ~1700 lines of tests!). We’re not going to discuss the implementation in depth here, but if you’ve mastered the contents of this book, you should be able to read the source code and figure out how it works. RC mingles variables and fields in the same stack of environments so that you get (field) and set fields (field &lt;&lt;- value) like regular values. R6 puts fields in a separate environment so you get (self$field) and set (self$field &lt;- value) with a prefix. The R6 approach is more verbose but is worth the tradeoff because it makes code easier to understand. It also makes inheritance across packages simpler and more robust. R6 is much faster than RC. Generally, the speed of method dispatch is not important outside of microbenchmarks but R6 is substantially better than RC. Switching from RC to R6 yielded substantial performance in shiny. vignette(&quot;Performance&quot;, &quot;R6&quot;) provides more details on the performance. Because the ideas that underlie R6 and RC are similar, it will only require a small amount of additional effort to learn RC if you need to. Because R6 is not built into base R, you’ll need to install and load a package in order to use it: library(R6) If you’d like to learn more about R6 after reading this chapter, the best place to start is the vignettes included in the package. You can list them by calling browseVignettes(package = &quot;R6&quot;). 15.1 Classes and methods R6 only needs a single function call to create both the class and its methods: R6::R6Class(). And this is the only function from the package that you’ll ever use! The following example shows the two most important arguments: The first argument is the classname. It’s not strictly needed, but it improves error messages and makes it possible to also use R6 objects with S3 generics. By convention, R6 classes use UpperCamelCase. The second argument, public, supplies a list of methods (functions) and fields (anything else) that make up the public interface of the object. By convention, methods and fields use snake_case. Methods can access the methods and fields of the current object via self$. Accumulator &lt;- R6Class(&quot;Accumulator&quot;, list( sum = 0, add = function(x = 1) { self$sum &lt;- self$sum + x invisible(self) }) ) You should always assign the result of R6Class() into a variable with the same name as the class. This creates an R6 object that defines the R6 class: Accumulator #&gt; &lt;Accumulator&gt; object generator #&gt; Public: #&gt; sum: 0 #&gt; add: function (x = 1) #&gt; clone: function (deep = FALSE) #&gt; Parent env: &lt;environment: R_GlobalEnv&gt; #&gt; Locked objects: TRUE #&gt; Locked class: FALSE #&gt; Portable: TRUE You construct a new object from the class by calling the new() method. Methods belong to R6 objects so you use $ to access new(): x &lt;- Accumulator$new() You can then call methods and access fields with $: x$add(4) x$sum #&gt; [1] 4 In this class, the fields and methods are public which means that you can get or set the value of any field. Later, we’ll see how to use private fields and methods to prevent casual access to the internals of your class. To make it clear when we’re talking about fields and methods as opposed to variables and functions, when referring to them in text, we’ll prefix with $. For example, the Accumulate class has field $sum and method $add(). 15.1.1 Method chaining $add() is called primarily for its side-effect of updating $sum. Accumulator &lt;- R6Class(&quot;Accumulator&quot;, list( sum = 0, add = function(x = 1) { self$sum &lt;- self$sum + x invisible(self) }) ) Side-effect R6 methods should always return self invisibly. This returns the “current” object and makes it possible to chain together multiple method calls: x$add(10)$add(10)$sum #&gt; [1] 24 Alternatively, for long chains, you can spread the call over multiple lines: x$ add(10)$ add(10)$ sum #&gt; [1] 44 This technique is called method chaining and is commonly used in encapsulated OO languages (like Python and JavaScript) to create fluent interfaces. Method chaining is deeply related to the pipe, and we’ll discuss the pros and cons of each approach in pipe vs message-chaining tradeoffs. 15.1.2 Important methods There are two important methods that will be defined for most classes: $initialize() and $print(). You don’t have to provide them, but it’s a good idea to do so because they will make your class easier to use. $initialize() overrides the default behaviour of $new(). For example, the following code defines an R6 Person class, similar to the S4 equivalent in S4. Unlike S4, R6 provides no checks for object type by default. $initialize() is a good place to check that name and age are the correct types. Person &lt;- R6Class(&quot;Person&quot;, list( name = NULL, age = NA, initialize = function(name, age = NA) { stopifnot(is.character(name), length(name) == 1) stopifnot(is.numeric(age), length(age) == 1) self$name &lt;- name self$age &lt;- age } )) hadley &lt;- Person$new(&quot;Hadley&quot;, age = 37) If you have more expensive validation requirements, implement them in a separate $validate() and only call when needed. Defining $print() allows you to override the default printing behaviour. As with any R6 method called for its side effects, $print() should return invisible(self). Person &lt;- R6Class(&quot;Person&quot;, list( name = NULL, age = NA, initialize = function(name, age = NA) { self$name &lt;- name self$age &lt;- age }, print = function(...) { cat(&quot;Person: \\n&quot;) cat(&quot; Name: &quot;, self$name, &quot;\\n&quot;, sep = &quot;&quot;) cat(&quot; Age: &quot;, self$age, &quot;\\n&quot;, sep = &quot;&quot;) invisible(self) } )) hadley2 &lt;- Person$new(&quot;Hadley&quot;) hadley2 #&gt; Person: #&gt; Name: Hadley #&gt; Age: NA This code illustrates an important aspect of R6. Because methods are bound to individual objects, the previously created hadley does not get this new method: hadley #&gt; &lt;Person&gt; #&gt; Public: #&gt; age: 37 #&gt; clone: function (deep = FALSE) #&gt; initialize: function (name, age = NA) #&gt; name: Hadley Indeed, from the perspective of R6, there is no relationship between hadley and hadley2. This can make interactive experimentation with R6 confusing. If you’re changing the code and can’t figure out why the results of method calls aren’t changed, make sure you’ve re-constructed R6 objects with the new class. There’s a useful alternative to $print(): implement $format(), which should return a character vector. This will automatically be used by both print() and format() S3 generics. Person &lt;- R6Class(&quot;Person&quot;, list( age = NA, name = NULL, initialize = function(name, age = NA) { self$name &lt;- name self$age &lt;- age }, format = function(...) { # The first `paste0()` is not necessary but it lines up # with the subsequent lines making it easier to see how # it will print c( paste0(&quot;Person:&quot;), paste0(&quot; Name: &quot;, self$name), paste0(&quot; Age: &quot;, self$age) ) } )) hadley3 &lt;- Person$new(&quot;Hadley&quot;) format(hadley3) #&gt; [1] &quot;Person:&quot; &quot; Name: Hadley&quot; &quot; Age: NA&quot; hadley3 #&gt; Person: #&gt; Name: Hadley #&gt; Age: NA 15.1.3 Adding methods after creation Instead of continuously creating new classes, it’s also possible to modify the methods of an existing class. This is useful when exploring interactively, and when you have a class with many functions that you’d like to break up into pieces. Once the class has been defined, you can add elements to it with $set(), supplying the visibility (more on that below), the name, and the component. Accumulator &lt;- R6Class(&quot;Accumulator&quot;) Accumulator$set(&quot;public&quot;, &quot;sum&quot;, 0) Accumulator$set(&quot;public&quot;, &quot;add&quot;, function(x = 1) { self$sum &lt;- self$sum + x invisible(self) }) $set() will not overwrite an existing method unless you explicitly ask for it: Accumulator$set(&quot;public&quot;, &quot;sum&quot;, 1) #&gt; Error in Accumulator$set(&quot;public&quot;, &quot;sum&quot;, 1): Can&#39;t add sum because it already present in Accumulator generator. Accumulator$set(&quot;public&quot;, &quot;sum&quot;, 1, overwrite = TRUE) Also note that adding methods will only affect new objects generated from the class. It does not retrospectively apply to existing objects: x1 &lt;- Accumulator$new() Accumulator$set(&quot;public&quot;, &quot;hello&quot;, function() message(&quot;Hi!&quot;)) x1$hello() #&gt; Error in eval(expr, envir, enclos): Versuch eine Nicht-Funktion anzuwenden x2 &lt;- Accumulator$new() x2$hello() #&gt; Hi! 15.1.4 Inheritance To inherit behaviour from an existing class, provide the class object to the inherit argument: AccumulatorChatty &lt;- R6Class(&quot;AccumulatorChatty&quot;, inherit = Accumulator, public = list( add = function(x = 1) { cat(&quot;Adding &quot;, x, &quot;\\n&quot;, sep = &quot;&quot;) super$add(x = x) } ) ) x2 &lt;- AccumulatorChatty$new() x2$add(10)$add(1)$sum #&gt; Adding 10 #&gt; Adding 1 #&gt; [1] 12 Note that $add() overrides the implementation in the superclass, but we can access the previous implementation through super$. Any methods which are overridden will automatically call the implementation in the parent class. Like S3, R6 only supports single inheritance: you cannot supply a vector of classes to inherit. 15.1.5 Introspection Every R6 object has an S3 class that reflects the hierarchy of R6 classes. This means that the easiest way to determine the class (and all classes it inherits from) is to use class(): class(hadley3) #&gt; [1] &quot;Person&quot; &quot;R6&quot; The S3 hierarchy includes the base “R6” class. This provides common behaviour, including an print.R6() method which calls $print() or $format(), as described above. You can list all methods and fields with names(): names(hadley3) #&gt; [1] &quot;.__enclos_env__&quot; &quot;name&quot; &quot;age&quot; &quot;clone&quot; #&gt; [5] &quot;format&quot; &quot;initialize&quot; There’s one method that we haven’t defined: $clone(). It’s provided by R6 and we’ll come back to it in reference semantics. 15.1.6 Exercises Can subclasses access private fields/methods from their parent? Perform an experiment to find out. 15.2 Controlling access R6Class() has two other arguments that work similarly to public: private and active. private allows you to create components that the user can not easily access, and active allows you to use accessor functions to define dynamic, or active, fields. 15.2.1 Privacy With R6 you can define private fields and methods, elements that can only be accessed from within the class, not from the outside. There are two things that you need to know to take advantage of private elements: The private argument works in the same way as the public argument: you give it a named list of methods (functions) and fields (everything else). Fields and methods defined in private are available within the methods with private$ instead of self$. You cannot access private fields or methods outside of the class. To make this concrete, we could make $age and $name fields of the Person class private. With this definition of Person we can only set $age and $name during object creation, and we cannot access their values from outside of the class. Person &lt;- R6Class(&quot;Person&quot;, public = list( initialize = function(name, age = NA) { private$name &lt;- name private$age &lt;- age }, print = function(...) { cat(&quot;Person: \\n&quot;) cat(&quot; Name: &quot;, private$name, &quot;\\n&quot;, sep = &quot;&quot;) cat(&quot; Age: &quot;, private$age, &quot;\\n&quot;, sep = &quot;&quot;) } ), private = list( age = NA, name = NULL ) ) hadley4 &lt;- Person$new(&quot;Hadley&quot;) hadley4$name #&gt; NULL The distinction between public and private fields is important when you create complex networks of classes, and you want to make it as clear as possible what it’s ok for others to access. Anything that’s private can be more easily refactored because you know others aren’t relying on it. Private methods tend to be more important in other programming languages compared to R because the object hierarchies in R tend to be simpler. 15.2.2 Active fields Active fields make allow you to define components that look like fields from the outside, but are defined with functions, like methods. For example, we can define an active field x that returns a different value every time you access it: Rando &lt;- R6::R6Class(&quot;Rando&quot;, active = list( random = function(value) { runif(1) } )) x &lt;- Rando$new() x$random #&gt; [1] 0.0808 x$random #&gt; [1] 0.834 x$random #&gt; [1] 0.601 Active fields are particularly useful in conjunction with privacy, because they make it possible to implement components that work like fields from the outside but provide additional checks. For example, you can use them to implement read-only fields or fields that validate their inputs. Active fields are implemented using active bindings from base R. Each active binding is a function that takes a single argument: value. If the argument is missing(), the value is being retrieved; otherwise it’s being modified. We can use that idea to make a read-only age field, and to ensure that name is a length 1 character vector. Person &lt;- R6Class(&quot;Person&quot;, private = list( .age = NA, .name = NULL ), active = list( age = function(value) { if (missing(value)) { private$.age } else { stop(&quot;`$age` is read only&quot;, call. = FALSE) } }, name = function(value) { if (missing(value)) { private$.name } else { stopifnot(is.character(name), length(name) == 1) private$.name &lt;- value self } } ), public = list( initialize = function(name, age = NA) { private$.name &lt;- name private$.age &lt;- age } ) ) hadley5 &lt;- Person$new(&quot;Hadley&quot;) hadley5$name #&gt; [1] &quot;Hadley&quot; hadley5$name &lt;- 10 #&gt; Error in stopifnot(is.character(name), length(name) == 1): Objekt &#39;name&#39; nicht gefunden hadley5$age #&gt; [1] NA hadley5$age &lt;- 20 #&gt; Error: `$age` is read only 15.2.3 Exercises How would you define a write-only field? 15.3 Reference semantics One of the big differences between R6 and most other objects in R is that they have reference semantics. This is because they are S3 objects built on top of environments: typeof(x2) #&gt; [1] &quot;environment&quot; The main consequence of reference semantics is that objects are not copied when modified: y1 &lt;- Accumulator$new() y2 &lt;- y1 y1$add(10) c(y1 = y1$sum, y2 = y2$sum) #&gt; y1 y2 #&gt; 11 11 Instead, if you want a copy, you’ll need to explicitly $clone() the object: y1 &lt;- Accumulator$new() y2 &lt;- y1$clone() y1$add(10) c(y1 = y1$sum, y2 = y2$sum) #&gt; y1 y2 #&gt; 11 1 (Note that $clone() does not recursively clone nested R6 objects. If you want that, you’ll need to use $clone(deep = TRUE). Note that this only clones R6 objects: if you have other fields with reference semantics (e.g. environments) you’ll need to define your own $clone().) There are three other less obvious consequences: It is harder to reason about code that uses R6 objects because you need to understand more context. It makes sense to think about when an R6 object is deleted, and you can write a finalizer() to complement the initializer(). If one of the fields is an R6 class, you must call $new() inside $initialize() not inside R6Class(). These are described in more detail below. 15.3.1 Reasoning Generally, reference semantics makes code harder to reason about. Take this very simple example: x &lt;- list(a = 1) y &lt;- list(b = 2) z &lt;- f(x, y) For the vast majority of functions, you know that the final line only modifies z. Take a similar equivalent that uses an imaginary List reference class: x &lt;- List$new(a = 1) y &lt;- List$new(b = 2) z &lt;- f(x, y) The final line is much harder to reason about - it’s completely possible that f() calls methods of x or y, modifying them in place. This is the biggest potential downside of R6. The best way to ameliorate this problem is to avoid writing functions that both return a value and modify R6 inputs. That said, modifying R6 inputs can lead to substantially simpler code in some cases. One challenge of working with immutable data is known as threading state: if you want to return a value that’s modified in a deeply nested function, you need to return the modified value up through every function. This can complicate code, particularly if you need to modify multiple values. For example, ggplot2 uses R6 objects for scales. Scales are complex because they need to combine data across every facet and every layer. Using R6 makes the code substantially simpler, at the cost of introducing subtle bugs. Fixing those bugs required careful placement of calls to $clone() to ensure that independent plots didn’t accidentally share scale data. We’ll come back to this idea in [oo-tradeoffs]. 15.3.2 Finalizer One useful property of reference semantics is that it makes sense to think about when an R6 object is finalised, i.e. when it’s deleted. This doesn’t make sense for S3 and S4 objects because copy-on-modify semantics mean that there may be many transient versions of an object. For example, in the following code, there are actually two factor objects: the second is created when the levels are modified, leaving the first to be destroyed at the next garbage collection. x &lt;- factor(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) levels(x) &lt;- c(&quot;c&quot;, &quot;b&quot;, &quot;a&quot;) Since R6 objects are not copied-on-modify they will only get deleted once, and it makes sense to think about $finalize() as a complement to $initialize(). Finalizers usually play a similar role to on.exit(), cleaning up any resources created by the initializer. For example, the following class wraps up a temporary file, automatically deleting it when the class is finalised. TemporaryFile &lt;- R6Class(&quot;TemporaryFile&quot;, list( path = NULL, initialize = function() { self$path &lt;- tempfile() }, finalize = function() { message(&quot;Cleaning up &quot;, self$path) unlink(self$path) } )) tf &lt;- TemporaryFile$new() The finalise method will be run when R exits, or by the first garbage collection after the object has been removed. Generally, this will happen when it happens, but it can occasionally be useful to force a run with an explicit call to gc(). rm(tf) invisible(gc()) 15.3.3 R6 fields A final consequence of reference semantics can crop up where you don’t expect it. Beware of setting a default value to an R6 class: it will be shared across all instances of the object. This is because the child object is only initialized once, when you defined the class, not each time you call new. TemporaryDatabase &lt;- R6Class(&quot;TemporaryDatabase&quot;, list( con = NULL, file = TemporaryFile$new(), initialize = function() { DBI::dbConnect(RSQLite::SQLite(), path = file$path) } )) db_a &lt;- TemporaryDatabase$new() db_b &lt;- TemporaryDatabase$new() db_a$file$path == db_b$file$path #&gt; [1] TRUE You can fix this by creating the object in $initialize(): TemporaryDatabase &lt;- R6Class(&quot;TemporaryDatabase&quot;, list( con = NULL, file = NULL, initialize = function() { self$file &lt;- TemporaryFile$new() DBI::dbConnect(RSQLite::SQLite(), path = file$path) } )) db_a &lt;- TemporaryDatabase$new() db_b &lt;- TemporaryDatabase$new() db_a$file$path == db_b$file$path #&gt; [1] FALSE 15.3.4 Exercises "],
["oo-tradeoffs.html", "16 Trade-offs 16.1 S4 vs S3 16.2 R6 vs S3", " 16 Trade-offs You now know about the three most important OOP toolkits available in R. Now that you understand their basic operation and the principles that underlie them, we can start to compare and constrast the systems in order to understand their strengths and weaknesses. This will help you pick the system that is most likely to solve new problems. When picking an OO system, I recommend that you default to S3. S3 is simple, and widely used throughout base R and CRAN. While it’s far from perfect, its idiosyncracries are well understood and there are known approaches to overcome most shortcomings. If you have an existing background in programming you are likely to lean towards R6 because it will feel familiar. I think you should resist this tendency for two reasons. Firstly, if you use R6 it’s very easy to create an non-idiomatic API that will feel very odd to native R users, and will have surprising pain points because of the reference semantics. Secondly, if you stick to R6, you’ll lose out on learning a new way of thinking about OOP that gives you a new set of tools for solving problems. This chapter is divided into two parts. S4 vs S3 compares S3 and S4. In brief, S4 is more formal and tends to require more upfront planning. That makes it more suitable for big projects developed by teams, not individuals. R6 vs S3 compares S3 and R6. This section is quite long because these two systems are fundamentally different and there are a number of tradeoffs that you need to consider. 16.1 S4 vs S3 Once you’ve mastered S3, S4 is relatively easy to pick up: the underlying ideas are the same, S4 is just more formal, more strict, and more verbose. The strictness and formality of S4 make it well suited for large teams. Since more structure is provided by the system itself, there is less need for convention, and new contributors don’t need as much training. S4 tends to require more upfront design than S3, and this investment tends to be more likely to pay off on larger projects because greater resources are available. One large team effort where S4 is used to good effect is Bioconductor. Bioconductor is similar to CRAN: it’s a way of sharing packages amongst a wider audience. Bioconductor is smaller than CRAN (~1,300 vs ~10,000 packages, July 2017) and the packages tend to be more tightly integrated because of the shared domain and because Bioconductor has a stricter review process. Bioconductor packages are not required to use S4, but most will because the key data structures (e.g. SummarizedExperiment, IRanges, DNAStringSet) are built using S4. S4 is also a good fit when you have a complicated system of interrelated objects, and it’s possible to minimise code duplication through careful implementation of methods. The best example of this use of S4 is the Matrix package by Douglas Bates and Martin Mächler. It is designed to efficiently store and compute with many different types of sparse and dense matrices. As of version 1.2.12, it defines 102 classes, 21 generic functions, and 1993 methods. To give you some idea of the complexity, a small subset of the class graph is shown in Figure 16.1. Figure 16.1: A small subset of the Matrix class graph showing the inheritance of sparse matrices. Each concrete class inherits from two virtual parents: one that describes how the data is stored (C = column oriented, R = row oriented, T = tagged) and one that describes any restriction on the matrix (s = symmetric, t = triangle, g = general) This domain is a good fit for S4 because there are often computational shortcuts for specific types of sparse matrix. S4 makes it easy to provide a general method that works for all inputs, and then provide a more specialised methods where the pair of data structures allow for a more efficient implementation. This requires careful planning to avoid method dispatch ambiguity, but the planning pays off for complicated systems. The biggest challenge to using S4 is the combination of increased complexity and absence of a single source of documentation. S4 is a complex system and it can be challenging to use effectively in practice. This wouldn’t be such a problem if S4 documentation wasn’t scattered through R documentation, books, and websites. S4 needs a book length treatment, but that book does not (yet) exist. (The documentation for S3 is no better, but the lack is less painful because S3 is much simpler.) 16.2 R6 vs S3 R6 is a profoundly different OO system from S3 and S4 because it is built on encapsulated objects, rather than generic functions. Additionally R6 objects have reference semantics, which means that they can be modified in place. These two big differences have a number of non-obvious consequences which we’ll explore in this chapter: A generic is a regular function so lives in the global namespace. A R6 method belongs to an object so lives in a local namespace. This influences how we think about naming. R6’s reference semantics allow methods to simultaneously return a value and update the object. This solves a painful problem called “threading state”. You invoke an R6 method using $, which is an infix operator. If you set up your methods correctly you can use chains of method calls as an alternative to the pipe. (All these trade-offs apply in general to immutable functional OOP vs mutable encapsulated OOP so also serve as a discussion of the tradeoffs between S3 and reference classes, and S3 and OOP in languages like Python.) 16.2.1 Namespacing One non-obvious difference between S3 and R6 is the “space” in which methods are found: Generic functions are global: all packages share the same namespace. Encapsulated methods are local: methods are bound to a single object. The advantage of a global namespace is that multiple packages can use exactly the same verbs for working with different types of objects. Generic functions provide a uniform API that makes it easier to perform typical actions with a new object because there are strong naming conventions. This works well for data analysis because you often want to do the same thing to different types of objects. In particular, this is one reason that R’s modelling system is so useful: regardless of where the model has been implemented you always work with it using the same set of tools (summary(), predict(), …). The disadvantage of a global namespace is that forces you to think more deeply about naming. You want to avoid multiple generics with the same name in different pakages because it requires the user to type :: frequently. This can be hard because function names are usually English verbs, and verbs often have multiple meanings. Take plot() for example: plot(data) # plot some data plot(bank_heist) # plot a crime plot(land) # create a new plot of land plot(movie) # extract plot of a movie Generally, you should avoid defining methods like this. Don’t use homonyms of the original generic, but instead define a new generic. This problem doesn’t occur with R6 methods because they are scoped to the object. The following code is fine, because there is no implication that the plot method of two different R6 objects has the same meaning: data$plot() bank_heist$plot() land$plot() movie$plot() These considerations also apply to the arguments to the generic. S3 generics must have the same core arguments, which mean they generally have to have non-specific names like x or .data. S3 generics generally need ... to pass on additional arguments to methods, but this has the downside that mispelled argument names will not create an error. In comparison, R6 methods can vary more widely and use more specific and evocative argument names. A secondary advantage of local namespacing is that creating an R6 method is very cheap. Most encapsulated OO languages encourage you to create many small methods, each doing one thing well with an evocative name. Creating a new S3 method is more expensive, because you may also have to create a generic, and think about the naming issues described above. That means that the advice to create many small methods does not apply to S3. It’s still a good idea to break your code down into small, easily understood chunks, but they should generally just be regular functions, not methods. 16.2.2 Threading state One challenge of programming with S3 is when you want to both return a value and modify the object. This violates our guideline that a function should either be called for its return value or for its side effects, but is necessary in a handful of cases. For example, imagine you want to create a stack of objects. A stack has two main methods: push() adds a new object to the top of the stack. pop() returns the top most value, and removes it from the stack. The implementation of the constructor and the push() method is straightforward. A stack contains a list of items, and pushing an object to the stack simply appends to this list. new_stack &lt;- function(items = list()) { structure(list(items = items), class = &quot;stack&quot;) } push &lt;- function(x, y) { x$items &lt;- c(x$items, list(y)) x } (Note that I haven’t created a real method for push() because making it generic would just make this example more complicated for no real benefit.) Implementing pop() is more challenging because it has to both return a value (the object at the top of the stack), and have a side-effect (remove that object from that top). Since we can’t modify the input object in S3 we need to return two things: the value, and the updated object. pop &lt;- function(x) { n &lt;- length(x$items) item &lt;- x$items[[n]] x$items &lt;- x$items[-n] list(item = item, x = x) } This leads to rather awkward usage: s &lt;- new_stack() s &lt;- push(s, 10) s &lt;- push(s, 20) out &lt;- pop(s) out$item #&gt; [1] 20 s &lt;- out$x s #&gt; $items #&gt; $items[[1]] #&gt; [1] 10 #&gt; #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;stack&quot; This problem is known as threading state or accumulator programming, because no matter how deeply the pop() is called, you have to feed the modified stack object all the way back to where the stack lives. One way that other FP languages deal with this challenge is to provide a “multiple assign” (or destructing bind) operator that allows you to assign multiple values in a single step. The zeallot R package, by Nathan and Paul Teetor, provides multi-assign for R with %&lt;-%. This makes the code more elegant, but doesn’t solve the key problem: library(zeallot) c(value, s) %&lt;-% pop(s) value #&gt; [1] 10 An R6 implementation of a stack is simpler because $pop() can modify the object in place, and return only the top-most value: Stack &lt;- R6::R6Class(&quot;Stack&quot;, list( items = list(), push = function(x) { self$items &lt;- c(self$items, x) invisible(self) }, pop = function() { item &lt;- self$items[[self$length()]] self$items &lt;- self$items[-self$length()] item }, length = function() { length(self$items) } )) This leads to more natural code: s &lt;- Stack$new() s$push(10) s$push(20) s$pop() #&gt; [1] 20 16.2.3 Method chaining The pipe, %&gt;%, is useful because it provides an infix operator that makes it easy to compose functions from left-to-right. Interestingly, the pipe is not so important for R6 objects because they already use an infix operator: $. This allows the user to chain together multiple method calls in a single expression, a technique known as method chaining: s &lt;- Stack$new() s$ push(10)$ push(20)$ pop() #&gt; [1] 20 This technique is commonly used in other programming languages, like Python and Javascript, and is made possible with one convention: any R6 method that is primarily called for its side-effects (usually modifying the object) should return invisible(self). The primary advantage of method chaining is that you can get useful autocomplete; the primary disadvantage is that only the creator of the class can add new methods (and there’s no way to use multiple dispatch). "],
["meta.html", "17 Introduction", " 17 Introduction “Flexibility in syntax, if it does not lead to ambiguity, would seem a reasonable thing to ask of an interactive programming language.” — Kent Pitman Compared to most programming languages, one of the most surprising things about R is its capability for metaprogramming: the ability of code to inspect and modify other code. Metaprogramming is particularly important in R because R is not just a programming language; it’s also an environment for doing interactive data analysis. Metaprogramming is useful for interactive exploration because it allows packages to create evaluation environments that use slightly different rules to usual code. This allows packages like ggplot2 and dplyr to create embedded domain specific languages tailored for solving specific data analysis problems. Embedded DSLs take advantage of a host language’s parsing and execution framework, but adjust the semantics to make them more suitable for a specific task. DSLs are a very large topic, and this chapter will only scratch the surface, focussing on important implementation techniques rather than on how you might come up with the language in the first place. If you’re interested in learning more, I highly recommend Domain Specific Languages by Martin Fowler. It discusses many options for creating a DSL and provides many examples of different languages. A common use of metaprogramming is to allow you to use names of variables in a dataframe as if they were objects in the environment. This makes interactive exploration more fluid at the cost of introducing some minor ambiguity. For example, take base::subset(). It allows you to pick rows from a dataframe based on the values of their observations: data(&quot;diamonds&quot;, package = &quot;ggplot2&quot;) subset(diamonds, x == 0 &amp; y == 0 &amp; z == 0) #&gt; carat cut color clarity depth table price x y z #&gt; 11964 1.00 Very Good H VS2 63.3 53 5139 0 0 0 #&gt; 15952 1.14 Fair G VS1 57.5 67 6381 0 0 0 #&gt; 24521 1.56 Ideal G VS2 62.2 54 12800 0 0 0 #&gt; 26244 1.20 Premium D VVS1 62.1 59 15686 0 0 0 #&gt; 27430 2.25 Premium H SI2 62.8 59 18034 0 0 0 #&gt; 49557 0.71 Good F SI2 64.1 60 2130 0 0 0 #&gt; 49558 0.71 Good F SI2 64.1 60 2130 0 0 0 (Base R functions like subset() and transform() inspired the development of dplyr.) subset() is considerably shorter than the equivalent code using [ and $ because you only need to provide the name of the data frame once: diamonds[diamonds$x == 0 &amp; diamonds$y == 0 &amp; diamonds$z == 0, ] #&gt; carat cut color clarity depth table price x y z #&gt; 11964 1.00 Very Good H VS2 63.3 53 5139 0 0 0 #&gt; 15952 1.14 Fair G VS1 57.5 67 6381 0 0 0 #&gt; 24521 1.56 Ideal G VS2 62.2 54 12800 0 0 0 #&gt; 26244 1.20 Premium D VVS1 62.1 59 15686 0 0 0 #&gt; 27430 2.25 Premium H SI2 62.8 59 18034 0 0 0 #&gt; 49557 0.71 Good F SI2 64.1 60 2130 0 0 0 #&gt; 49558 0.71 Good F SI2 64.1 60 2130 0 0 0 Functions like subset() are often said to use non-standard evalution, or NSE for short. That’s because they evaluate one (or more) of their arguments in a non-standard way. For example, if you take the second argument to subset() above and try and evaluate it directly, the code will not work: x == 0 | y == 0 | z == 0 As you might guess, defining these tools by what they are not (standard evaluation) is somewhat problematic. Additionally, implementation of the underlying ideas has occurred piecemeal over the last twenty years. These two forces tend to make base R code for NSE harder to understand than it could be; the key ideas are obscured by unimportant details. To avoid these problems here, these chapters will focus on functions from the rlang package. Then once you have the basic ideas, I’ll also show you the equivalent base R code so you can more easily understand existing code. In this section of the book, you’ll learn about the three big ideas that underpin NSE: In Expressions, Expressions, shows the hierarchical structure of R code. You’ll learn how to visualise the hierarchy for arbitrary code, how the rules of R’s grammar convert linear sequences of characters into a tree, and how to use recursive functions to work with code trees. In Quotation, Quotation, you’ll learn to use tools from rlang to capture unevaluated function arguments. You’ll also learn about quasiquotation, which provides a set of techniques for “unquoting” input, and see how you can generate R code by calling functions instead of writing it by hand. In Evaluation, Evaluation, you’ll learn about the inverse of quotation: evaluation. Here you’ll learn about an important data structure, the quosure, which ensures correct evaluation by capturing both the code to evaluate, and the environment in which to evaluate it. This chapter will show you how put all the pieces together to understand how NSE works, and how to write your own functions that work like subset(). This part of the book concludes with a case study in DSLs. This chapter pulls together the threads of metaprogramming and shows how you can use R to create two domain specific languages for translating R code into HTML and LATEX. You’ll learn important techniques similar to those used by shiny and dplyr. Each chapter follows the same basic structure. You’ll get the lay of the land in introduction, then see a motivating example. Next you’ll learn the big ideas using functions from rlang, and then we’ll circle back to talk about how those ideas are expressed in base R. Each chapter finishes with a case study, using the ideas to solve a bigger problem. If you’re reading these chapters primarily to better understand tidy evaluation so you can better program with the tidyverse, I’d recommend just reading the first 2-3 sections of each chapter; skip the sections about base R and more advanced techniques. "],
["expressions.html", "18 Expressions 18.1 Abstract syntax trees 18.2 R’s grammar 18.3 Data structures 18.4 Case study: anaphoric functions 18.5 Case study: Walking the AST with recursive functions", " 18 Expressions To compute on the language, we first need to understand its structure. That requires some new vocabulary, some new tools, and some new ways of thinking about R code. The first thing you’ll need to understand is the distinction between an operation and its result. Take this code, which takes a variable x multiplies it by 10 and saves the result to a new variable called y. It doesn’t work because we haven’t defined a variable called x: y &lt;- x * 10 #&gt; Error in eval(expr, envir, enclos): Objekt &#39;x&#39; nicht gefunden It would be nice if we could capture the intent of the code, without executing the code. In other words, how can we separate our description of the action from performing it? One way is to use rlang::expr(): z &lt;- expr(y &lt;- x * 10) z #&gt; y &lt;- x * 10 expr() returns a quoted expression: the R code that captures our intent. In this chapter, you’ll learn about the structure of those expressions, which will also help you understand how R executes code. Later, we’ll learn about eval() which allows you to take such an expression and perform, or evaluate, it: x &lt;- 4 eval(z) y #&gt; [1] 40 18.1 Abstract syntax trees Quoted expressions are also called abstract syntax trees (AST) because the structure of code is fundamentally hierarchical and can be naturally represented as a tree. To make that more obvious we’re going to introduce some graphical conventions, illustrated with the very simple call f(x, &quot;y&quot;, 1). Function calls define the hierarchy of the tree. Calls are shown with an orange square. The first child (f) is the function that gets called. The second and subsequent children (x, &quot;y&quot;, and 1) are the arguments. NB: Unlike many tree diagrams the order of the children is important: f(x, 1) is not the same as f(1, x). The leaves of the tree are either symbols, like f and x, or constants like 1 or &quot;y&quot;. Symbols have a purple border and rounded corners. Constants, which are atomic vectors of length one, have black borders and square corners. Strings are always surrounded in quotes to emphasise their difference to symbols — more on that later. Drawing these diagrams by hand takes me some time, and obviously you can’t rely on me to draw diagrams for your own code. I’ll supplement the hand-drawn trees with trees drawn by lobstr::ast(). ast() tries to make trees as similar as possible to my hand-drawn trees, while respecting the limitations of the console. Let’s use ast() to display the tree above: lobstr::ast(f(x, &quot;y&quot;, 1)) #&gt; █─f #&gt; ├─x #&gt; ├─&quot;y&quot; #&gt; └─1 Calls get an orange square, symbols are bold and purple, and strings are surrounded by quote marks. (The formatting is not currently shown in the rendered book, but you can see it if you run the code yourself.) ast() supports “unquoting” with !! (pronounced bang-bang). We’ll talk about unquoting in detail in the next chapter; for now note that it’s useful if you’ve already used expr() to capture the expression. x &lt;- expr(f(x, &quot;y&quot;, 1)) # not useful! lobstr::ast(x) #&gt; x # what we want lobstr::ast(!!x) #&gt; █─f #&gt; ├─x #&gt; ├─&quot;y&quot; #&gt; └─1 For more complex code, you can also use RStudio’s tree viewer to explore the AST interactively, e.g. View(expr(y &lt;- x * 10)). 18.1.1 Infix vs. prefix calls Every call in R can be written in tree form, even if it doesn’t look like it at first glance. Take y &lt;- x * 10 again: what are the functions that are being called? It not as easy to spot as f(x, 1) because this expression contains two calls in infix form: &lt;- and *. Infix functions come inbetween their arguments (so an infix function can only have two arguments), whereas most functions in R are prefix functions where the name of the function comes first.1 In R, any infix call can be converted to a prefix call if you escape the function name with backticks. That means that these two lines of code are equivalent: y &lt;- x * 10 `&lt;-`(y, `*`(x, 10)) And they have this AST: lobstr::ast(y &lt;- x * 10) #&gt; █─`&lt;-` #&gt; ├─y #&gt; └─█─`*` #&gt; ├─x #&gt; └─10 You might remember that code like names(x) &lt;- y ends up calling the names&lt;- function. That is not reflected in the parse tree because the translation needs to happen later, due to the complexities of nested assignments like names(x)[2] &lt;- &quot;z&quot;. lobstr::ast(names(x) &lt;- y) #&gt; █─`&lt;-` #&gt; ├─█─names #&gt; │ └─x #&gt; └─y 18.1.2 Special forms R has a small number of other syntactical constructs that don’t look like either prefix or infix function calls. These are called special forms and include function, the control flow operators (if, for, while, repeat), and parentheses ({, (, [[, and [). These can also be written in prefix form, and hence appear in the same way in the AST: lobstr::ast(function(x, y) { if (x &gt; y) { x } else { y } }) #&gt; █─`function` #&gt; ├─█─x = `` #&gt; │ └─y = `` #&gt; ├─█─`{` #&gt; │ └─█─`if` #&gt; │ ├─█─`&gt;` #&gt; │ │ ├─x #&gt; │ │ └─y #&gt; │ ├─█─`{` #&gt; │ │ └─x #&gt; │ └─█─`{` #&gt; │ └─y #&gt; └─&lt;inline srcref&gt; 18.1.3 Function factories Another small detail we need to consider are calls like f()(). The first component of the call is usually a symbol: lobstr::ast(f(a, 1)) #&gt; █─f #&gt; ├─a #&gt; └─1 But if you are using a function factory, a function that returns another function, the first component might be another call: lobstr::ast(f()(a, 1)) #&gt; █─█─f #&gt; ├─a #&gt; └─1 (See Function factories for more details) Of course that function might also take arguments: lobstr::ast(f(b, 2)()) #&gt; █─█─f #&gt; ├─b #&gt; └─2 These forms are relatively rare, but it’s good to be able to recognise them when they crop up. 18.1.4 Argument names So far the examples have only used unnamed arguments. Named arguments don’t change the parsing rules, but just add some additional metadata: lobstr::ast(mean(x = mtcars$cyl, na.rm = TRUE)) #&gt; █─mean #&gt; ├─x = █─`$` #&gt; │ ├─mtcars #&gt; │ └─cyl #&gt; └─na.rm = TRUE (And note the appearance of another infix function: $) 18.1.5 Exercises Use ast() and experimentation to figure out the three arguments to an if() call. What would you call them? Which components are required? What are the arguments to the for() and while() calls? What does the call tree of an if statement with multiple else if conditions look like? Why? Two arithmetic operators can be used in both prefix and infix style. What are they? 18.2 R’s grammar The process by which a computer language takes a sequence of tokens (like x, +, y) and constructs a tree is called parsing, and it is governed by a set of rules known as a grammar. In this section, we’ll use lobstr::ast() to explore some of the details of R’s grammar. If this is your first reading of the metaprogramming chapters, now is a good time to read the first sections of the next two chapters in order to get the big picture. Come back and learn more of the details once you’ve seen how all the big pieces fit together. 18.2.1 Operator precedence and associativity Infix functions introduce ambiguity in a way that prefix functions do not2. The parser has to resolve two sources of ambiguity when parsing infix operators. First, what does 1 + 2 * 3 yield? Do you get 9 (i.e. (1 + 2) * 3), or 7 (i.e. 1 + (2 * 3)). Which of the two possible parse trees below does R use? Programming langauges use conventions called operator precedence to resolve this ambiguity. We can use ast() to see what R does: lobstr::ast(1 + 2 * 3) #&gt; █─`+` #&gt; ├─1 #&gt; └─█─`*` #&gt; ├─2 #&gt; └─3 Predicting the precedence of arithmetic operations is usually easy because it’s drilled into you in school and is consistent across the vast majority of programming languages. Predicting the precedence of other operators is harder. There’s one particularly surprising finding in R: ! has a much lower precedence (i.e. it binds less tightly) than you might expect. This allows you to write useful operations like: lobstr::ast(!x %in% y) #&gt; █─`!` #&gt; └─█─`%in%` #&gt; ├─x #&gt; └─y Another source of ambiguity is repeated usage of the same infix function. For example, is 1 + 2 + 3 equivalent to (1 + 2) + 3 or to 1 + (2 + 3)? This normally doesn’t matter because x + y == y + x, i.e. addition is associative. However, some S3 classes define + in a non-associative way. For example, ggplot2 overloads + to build up a complex plot from simple pieces; this usage is non-associative because earlier layers are drawn underneath later layers. In R, most operators are left-associative, i.e. the operations on the left are evaluated first: lobstr::ast(1 + 2 + 3) #&gt; █─`+` #&gt; ├─█─`+` #&gt; │ ├─1 #&gt; │ └─2 #&gt; └─3 R has over 30 infix operators divided into 18 precedence groups. While the details are descrbed in ?Syntax, very few people have memorised the complete ordering. Indeed, if there’s any confusion, use parentheses! These also appear in the AST, like all other special forms: lobstr::ast(1 + (2 + 3)) #&gt; █─`+` #&gt; ├─1 #&gt; └─█─`(` #&gt; └─█─`+` #&gt; ├─2 #&gt; └─3 18.2.2 Whitespace R, in general, is not sensitive to white space. Most white space is not signficiant and is not recorded in the AST. x+y yields exactly the same AST as x + y. This means that you’re generally free to add whitespace to enhance the readability of your code. There’s one major exception: lobstr::ast(y &lt;- x) #&gt; █─`&lt;-` #&gt; ├─y #&gt; └─x lobstr::ast(y &lt; -x) #&gt; █─`&lt;` #&gt; ├─y #&gt; └─█─`-` #&gt; └─x 18.2.3 Parsing a string Most of the time you type code into the console, and R takes care of turning the characters you’ve typed into an AST. But occasionally you have code stored in a string, and you want to parse it yourself. You can do so using rlang::parse_expr(): x1 &lt;- &quot;y &lt;- x + 10&quot; lobstr::ast(!!x1) #&gt; &quot;y &lt;- x + 10&quot; x2 &lt;- rlang::parse_expr(x1) x2 #&gt; y &lt;- x + 10 lobstr::ast(!!x2) #&gt; █─`&lt;-` #&gt; ├─y #&gt; └─█─`+` #&gt; ├─x #&gt; └─10 (If you find yourself working with strings containing code very frequently, you should reconsider your work. Read the next chapter and consider if you can more safely generate expressions using quasiquotation.) The base equivalent to parse_expr() is parse(). It is a little harder to use because it’s specialised for parsing R code stored in files. That means you need supply your string to the text argument, and you’ll get back an expression object (more on that shortly) that you’ll need to subset: parse(text = x1)[[1]] #&gt; y &lt;- x + 10 18.2.4 Deparsing The opposite of parsing is deparsing: you have an AST and you want a string that would generate it: z &lt;- expr(y &lt;- x + 10) expr_text(z) #&gt; [1] &quot;y &lt;- x + 10&quot; Parsing and deparsing are not perfectly symmetrical because parsing throws away all information not directly related to the AST. This includes backticks around ordinary names, comments, and whitespace: cat(expr_text(expr({ # This is a comment x &lt;- `x` + 1 }))) #&gt; { #&gt; x &lt;- x + 1 #&gt; } Deparsing is often used to provide default names for data structures (like data frames), and default labels for messages or other output. rlang provides two helpers for those situations: z &lt;- expr(f(x, y, z)) expr_name(z) #&gt; [1] &quot;f(x, y, z)&quot; expr_label(z) #&gt; [1] &quot;`f(x, y, z)`&quot; Be careful when using the base R equivalent, deparse(): it returns a character vector, and with one element for each line. Whenever you use it, remember that the length of the output might be greater than one, and plan accordingly. 18.2.5 Exercises R uses parentheses in two slightly different ways as illustrated by this simple call: f((1)). Compare and contrast the two uses. = can also be used in two ways. Construct a simple example that shows both uses. What does !1 + !1 return? Why? Which arithmetic operation is right associative? Why does x1 &lt;- x2 &lt;- x3 &lt;- 0 work? There are two reasons. Compare x + y %+% z to x ^ y %+% z. What does that tell you about the precedence of custom infix functions? deparse() produces vectors when the input is long. For example, the following call produces a vector of length two: expr &lt;- expr(g(a + b + c + d + e + f + g + h + i + j + k + l + m + n + o + p + q + r + s + t + u + v + w + x + y + z)) deparse(expr) What do expr_text(), expr_name(), and expr_label() do? 18.3 Data structures Now that you have a good feel for ASTs and how R’s grammar helps to define them, it’s time to look into more detail about the underlying implementation. In this section you’ll learn about the data structures that R uses to implement the AST: Constants and symbols, the leaves of the tree. Calls, the branches of the tree. Pairlists, a historical data structure now only used for function arguments. Before we continue, a word of caution about the naming conventions used in this book. Unfortunately, base R does not have a consistent set of conventions, so we’ve had to make our own. We use them consistently in the book and in rlang, but you’ll need to remember some translations when reading base R documentation. We use expression to refer collectively to the data structures that can appear in an AST (constant, symbol, call, pairlist). In base R, “expression” is a special type that is basically equivalent to a list of what we call expressions. To avoid confusion (as much as possible) we’ll call these expression objects, and we’ll discuss in expression objects. Base R does not have an equivalent term for our “expression”. The closest is “language object”, which includes symbols and calls, but not constants or pairlists. But note that typeof() and str() use “language” to mean call. Base R uses symbol and name interchangeably; we prefer symbol because “name” has other common meanings (e.g. the name of a variable). 18.3.1 Constants and symbols Constants and symbols are the leaves of the AST. Symbols represent variable names, and constants represent literal values. Constants are “self-quoting” in the sense that the expression used to represent a constant is the constant itself: identical(expr(&quot;x&quot;), &quot;x&quot;) #&gt; [1] TRUE identical(expr(TRUE), TRUE) #&gt; [1] TRUE identical(expr(1), 1) #&gt; [1] TRUE identical(expr(2), 2) #&gt; [1] TRUE Symbols are used to represent variables. You can convert back and forth between symbols and the strings that represent them with sym() and as_string(): &quot;x&quot; #&gt; [1] &quot;x&quot; sym(&quot;x&quot;) #&gt; x as_string(sym(&quot;x&quot;)) #&gt; [1] &quot;x&quot; 18.3.1.1 The missing symbol There’s one special symbol that needs a little extra discussion: the empty symbol which is used to represent missing arguments. You can make it with missing_arg() (or expr()): missing_arg() The missing argument throws an error if the symbol it is bound to is accessed: m1 &lt;- missing_arg() m1 #&gt; Error in eval(expr, envir, enclos): Argument &quot;m1&quot; fehlt (ohne Standardwert) This gives it rather peculaiar behaviour since you can still access it in if it’s stored in another data structure: m2 &lt;- list(missing_arg()) m2[[1]] If you do need to work with a missing argument stored in a variable, you can wrap up any accesses with maybe_missing() : maybe_missing(m1) That prevents the error from occurring and instead returns another empty symbol. You can see if it’s misisng by using is_missing(): is_missing(m1) #&gt; [1] TRUE is_missing(m2[[1]]) #&gt; [1] TRUE You only need to know about the missing symbol if you’re programmatically creating functions with missing arguments; we’ll come back to that in the next chapter. 18.3.2 Calls Calls define the tree in AST. A call behaves similarly to a list. It has a length(); you can extract elements with [[, [, and $; and calls can contain other calls. The main difference is that the first element of a call is special: it’s the function that will get called. Let’s explore these ideas with a simple example: x &lt;- expr(read.table(&quot;important.csv&quot;, row = FALSE)) lobstr::ast(!!x) #&gt; █─read.table #&gt; ├─&quot;important.csv&quot; #&gt; └─row = FALSE The length of a call minus one gives the number of arguments: length(x) - 1 #&gt; [1] 2 The names of a call are empty, except for named arguments: names(x) #&gt; [1] &quot;&quot; &quot;&quot; &quot;row&quot; You can extract the leaves of the call by position and by name using [[ and $ in the usual way: x[[1]] #&gt; read.table x[[2]] #&gt; [1] &quot;important.csv&quot; x$row #&gt; [1] FALSE Extracting specific arguments from calls is challenging because of R’s flexible rules for argument matching: it could potentially be in any location, with the full name, with an abreviated name, or with no name. To work around this problem, you can use rlang::lang_standardise() which standardises all arguments to use the full name: rlang::lang_standardise(x) #&gt; read.table(file = &quot;important.csv&quot;, row.names = FALSE) (Note that if the function uses ... it’s not possible to standardise all arguments.) You can use [ to extract multiple components, but if you drop the the first element, you’re usually going to end up with a weird call: x[2:3] #&gt; &quot;important.csv&quot;(row = FALSE) If you do want to extract multiple elements in this way, it’s good practice to coerce the results to a list: as.list(x[2:3]) #&gt; [[1]] #&gt; [1] &quot;important.csv&quot; #&gt; #&gt; $row #&gt; [1] FALSE Calls can be modified in the same way as lists: x$header &lt;- TRUE x #&gt; read.table(&quot;important.csv&quot;, row = FALSE, header = TRUE) You can also construct a call from scratch using rlang::lang(). The first argument should be the function to be called (supplied either as a string or a symbol), and the subsequent arguments are the call to that function: lang(&quot;mean&quot;, x = expr(x), na.rm = TRUE) #&gt; mean(x = x, na.rm = TRUE) 18.3.3 Pairlists There is one data structure we need to discuss for completeness: the pairlist. Pairlists are a remnant of R’s past and have been replaced by lists almost everywhere. The only place you are likely to see pairlists in R is when working with function arguments: f &lt;- function(x = 10) x + 1 typeof(formals(f)) #&gt; [1] &quot;pairlist&quot; (If you’re working in C, you’ll encounter pairlists more often. For example, calls are also implemented using pairlists) Fortunately, whenever you encounter a pairlist, you can treat it just like a regular list: pl &lt;- pairlist(x = 1, y = 2) length(pl) #&gt; [1] 2 pl$x #&gt; [1] 1 However, behind the scenes pairlists are implemented using a different data structure, a linked list instead of a vector. That means that subsetting is slower with pairlists, and gets slower the further along the pairlist you index: l1 &lt;- as.list(1:20) l2 &lt;- as.pairlist(l1) microbenchmark::microbenchmark( l1[[1]], l1[[10]], l2[[1]], l2[[10]] ) #&gt; Unit: nanoseconds #&gt; expr min lq mean median uq max neval cld #&gt; l1[[1]] 186 198 268 210 234 4797 100 a #&gt; l1[[10]] 188 202 228 219 246 532 100 a #&gt; l2[[1]] 551 576 610 595 622 977 100 b #&gt; l2[[10]] 554 586 656 604 632 4781 100 b 18.3.4 Expression objects Finally, we need to discuss the expression object briefly. Expression objects are produced by only two base functions: expression() and parse(): exp1 &lt;- parse(text = c(&quot; x &lt;- 4 x &quot;)) exp2 &lt;- expression(x &lt;- 4, x) typeof(exp1) #&gt; [1] &quot;expression&quot; typeof(exp2) #&gt; [1] &quot;expression&quot; exp1 #&gt; expression(x &lt;- 4, x) exp2 #&gt; expression(x &lt;- 4, x) Like calls and pairlists, expression objects behave like a list: length(exp1) #&gt; [1] 2 exp1[[1]] #&gt; x &lt;- 4 Conceptually an expression object is just a list of expressions. The only difference is that calling eval() on an expression evaluates each individual expression. We don’t believe this advantage merits introducing a new data structure, so instead of expression objects we always use regular lists of expressions. Because there might be many top-level calls in a file, parse() doesn’t return just a single expression. Instead, it returns an expression object, which is essentially a list of expressions: length(exp) #&gt; [1] 1 typeof(exp) #&gt; [1] &quot;builtin&quot; 18.3.5 Exercises Which two of the six types of atomic vector can’t appear in an expression? Why? Why can’t you create an expression that contains an atomic vector of length greater than one? standardise_call() doesn’t work so well for the following calls. Why? lang_standardise(quote(mean(1:10, na.rm = TRUE))) #&gt; mean(x = 1:10, na.rm = TRUE) lang_standardise(quote(mean(n = T, 1:10))) #&gt; mean(x = 1:10, n = T) lang_standardise(quote(mean(x = 1:10, , TRUE))) #&gt; mean(x = 1:10, , TRUE) Why does this code not make sense? x &lt;- expr(foo(x = 1)) names(x) &lt;- c(&quot;x&quot;, &quot;&quot;) base::alist() is useful for creating pairlists to be used for function arguments: foo &lt;- function() {} formals(foo) &lt;- alist(x = , y = 1) foo #&gt; function (x, y = 1) #&gt; { #&gt; } What makes alist() special compared to list()? Concatenating a call and an expression with c() creates a list. Implement concat() so that the following code works to combine a call and an additional argument. c(quote(f()), list(a = 1, b = quote(mean(a))) concat(quote(f), a = 1, b = quote(mean(a))) #&gt; f(a = 1, b = mean(a)) 18.4 Case study: anaphoric functions One useful application of make_function() is in functions like curve(). curve() allows you to plot a mathematical function without creating an explicit R function: curve(sin(exp(4 * x)), n = 1000) Here x is a pronoun. x doesn’t represent a single concrete value, but is instead a placeholder that varies over the range of the plot. One way to implement curve() would be with make_function(): curve4 &lt;- function(expr, xlim = c(0, 1), n = 100) { expr &lt;- enquo(expr) f &lt;- new_function(alist(x = ), get_expr(expr), get_env(env)) x &lt;- seq(xlim[1], xlim[2], length = n) y &lt;- f(x) plot(x, y, type = &quot;l&quot;, ylab = deparse(substitute(expr))) } curve4(sin(exp(4 * x)), n = 1000) curve3 &lt;- function(expr, xlim = c(0, 1), n = 100) { expr &lt;- enquo(expr) e &lt;- rlang::expr({ function(x) !!get_expr(expr) }) f &lt;- eval_tidy(e, env = get_env(expr)) x &lt;- seq(xlim[1], xlim[2], length = n) y &lt;- f(x) plot(x, y, type = &quot;l&quot;, ylab = deparse(substitute(expr))) } curve3(sin(exp(4 * x)), n = 1000) Functions that use a pronoun are called anaphoric functions. They are used in Arc (a lisp like language), Perl, and Clojure. 18.4.1 Exercises How are alist(a) and alist(a = ) different? Think about both the input and the output. Read the documentation and source code for pryr::partial(). What does it do? How does it work? Read the documentation and source code for pryr::unenclose(). What does it do and how does it work? The actual implementation of curve() looks more like curve3 &lt;- function(expr, xlim = c(0, 1), n = 100, env = parent.frame()) { env2 &lt;- new.env(parent = env) env2$x &lt;- seq(xlim[1], xlim[2], length = n) y &lt;- eval(substitute(expr), env2) plot(env2$x, y, type = &quot;l&quot;, ylab = deparse(substitute(expr))) } How does this approach differ from curve2() defined above? 18.5 Case study: Walking the AST with recursive functions To conclude the chapter we’re going to pull together everything that you’ve learned about ASTs to write solve more complicated tasks. Some inspiration comes from the base codetools package, which provides two interesting functions: findGlobals() locates all global variables used by a function. This can be useful if you want to check that your function doesn’t inadvertently rely on variables defined in their parent environment. checkUsage() checks for a range of common problems including unused local variables, unused parameters, and the use of partial argument matching. Getting all of the details of this functions correct is fiddly and complex, so we’re not going to explore the full expression. Instead we’ll focus on the big ideas needed to implement functions that work like them, namely recursion. Recursive functions are a natural fit to tree-like data structures because a recursive function is made up of two parts: The recursive case handles the nodes in the tree. Typically, you’ll do something to each child of node, usually calling the recursive function again, and then combine the results back together again. For expressions, you’ll need to handle calls and pairlists in the recursive case. The base case handles the leaves of the tree. The base cases ensure that the function eventually terminates, by solving the simplest cases directly. For expressions, you need to base symbol and names in the base case. To make this pattern easier to see, we’ll use two helper functions. First we define expr_type() which will return “constant” for constant, “symbol” for symbols, “call”, for calls, “pairlist” for pairlists, and the “type” of anything else: expr_type &lt;- function(x) { if (rlang::is_syntactic_literal(x)) { &quot;constant&quot; } else if (is.symbol(x)) { &quot;symbol&quot; } else if (is.call(x)) { &quot;call&quot; } else if (is.pairlist(x)) { &quot;pairlist&quot; } else { typeof(x) } } expr_type(expr(&quot;a&quot;)) #&gt; [1] &quot;constant&quot; expr_type(expr(f(1, 2))) #&gt; [1] &quot;call&quot; We’ll couple this with a wrapper around the switch function: switch_expr &lt;- function(x, ...) { switch(expr_type(x), ..., stop(&quot;Don&#39;t know how to handle type &quot;, typeof(x), call. = FALSE) ) } With these two functions in hand, the basic template for any function that walks the AST is as follows: recurse_call &lt;- function(x) { switch_expr(x, # Base cases symbol = , constant = , # Recursive cases call = , pairlist = ) } Typically, solving the base case will easy so we’ll do that first, and check the results. The recursive cases are a little more tricky, but typically you just need to think about the data structure you want in the output and then find the correct purrr function. To that end, make sure you’re familiar with Functionals before continuing. 18.5.1 Finding F and T We’ll start simple with a function that determines whether a function uses the logical abbreviations T and F: it will return TRUE if it finds a logical abbreviation, and FALSE otherwise. Using T and F is generally considered to be poor coding practice, and is something that R CMD check will warn about. Let’s first compare the AST for T vs. TRUE: ast(TRUE) #&gt; TRUE ast(T) #&gt; T TRUE is parsed as a logical vector of length one, while T is parsed as a name. This tells us how to write our base cases for the recursive function: a constant is never a logical abbreviation, and a symbol is an abbreviation if it’s “F” or “T”: logical_abbr_rec &lt;- function(x) { switch_expr(x, constant = FALSE, symbol = as_string(x) %in% c(&quot;F&quot;, &quot;T&quot;) ) } logical_abbr_rec(expr(TRUE)) #&gt; [1] FALSE logical_abbr_rec(expr(T)) #&gt; [1] TRUE Our recursive function works with expressions which means we always need to use it with expr(). When writing a recursive function it’s common to write a wrapper that provides defaults or makes the function a little easier to use. Here we’ll typically make a wrapper that quotes its input (we’ll learn more about that in the next chapter), so we don’t need to use expr() every time. logical_abbr &lt;- function(x) { logical_abbr_rec(enexpr(x)) } logical_abbr(T) #&gt; [1] TRUE logical_abbr(FALSE) #&gt; [1] FALSE Next we need to implement the recursive cases. Here it’s simple because we want to do the same thing for calls and for pairlists: recursively apply the function to each subcomponent, and return TRUE if any subcomponent contains a logical abbreviation. This is made easy by purrr::some(), which iterates over a list and returns TRUE if the predicate function is true for any element. logical_abbr_rec &lt;- function(x) { switch_expr(x, # Base cases constant = FALSE, symbol = as_string(x) %in% c(&quot;F&quot;, &quot;T&quot;), # Recursive cases call = , pairlist = purrr::some(x, logical_abbr_rec) ) } logical_abbr(mean(x, na.rm = T)) #&gt; [1] TRUE logical_abbr(function(x, na.rm = T) FALSE) #&gt; [1] TRUE 18.5.2 Finding all variables created by assignment logical_abbr() is very simple: it only returns a single TRUE or FALSE. The next task, listing all variables created by assignment, is a little more complicated. We’ll start simply, and then make the function progressively more rigorous. We start by looking at the AST for assignment: ast(x &lt;- 10) #&gt; █─`&lt;-` #&gt; ├─x #&gt; └─10 Assignment is a call where the first element is the symbol &lt;-, the second is name of variable, and the third is the value to be assigned. Next, we need to decide what data structure we’re going to use for the results. Here I think it will be easiest it we return a character vector. If we return symbols, we’ll need to use a list() and that makes things a little more complicated. With that in hand we can start by implementing the base cases and providing a helpful wrapper around the recursive function. The base cases here are really simple! find_assign_rec &lt;- function(x) { switch_expr(x, constant = , symbol = character() ) } find_assign &lt;- function(x) find_assign_rec(enexpr(x)) find_assign(&quot;x&quot;) #&gt; character(0) find_assign(x) #&gt; character(0) Next we implement the recursive cases. This is made easier by a function that should exist in purrrr, but currently doesn’t: flat_map_chr &lt;- function(.x, .f, ...) { purrr::flatten_chr(purrr::map(.x, .f, ...)) } The recursive case for pairlists is simple: we iterate of every element of the pairlist (i.e. each function argument) and combine the results. The case for calls is a little bit more complex - if this is a call to &lt;- then we should return the second element of the call: find_assign_rec &lt;- function(x) { switch_expr(x, # Base cases constant = , symbol = character(), # Recursive cases pairlist = flat_map_chr(as.list(x), find_assign_rec), call = { if (is_call(x, &quot;&lt;-&quot;)) { as_string(x[[2]]) } else { flat_map_chr(as.list(x), find_assign_rec) } } ) } # find_assign(function(x = 1) {}) # Need to handle srcrefs # Maybe move to new section specifically about functions? find_assign(a &lt;- 1) #&gt; [1] &quot;a&quot; find_assign({ a &lt;- 1 { b &lt;- 2 } }) #&gt; [1] &quot;a&quot; &quot;b&quot; Now we need to make our function more robust but coming up with examples intended to break it. What happens we assign to the same variable multiple times? find_assign({ a &lt;- 1 a &lt;- 2 }) #&gt; [1] &quot;a&quot; &quot;a&quot; It’s easiest to fix this at the level of the wrapper function: find_assign &lt;- function(x) unique(find_assign_rec(enexpr(x))) find_assign({ a &lt;- 1 a &lt;- 2 }) #&gt; [1] &quot;a&quot; What happens if we have multiple calls to assign? Currently we only return the first. That’s because when &lt;- occurs we immediately terminate recursion. find_assign({ a &lt;- b &lt;- c &lt;- 1 }) #&gt; [1] &quot;a&quot; Instead we need to take a more rigorous approach. I think it’s best to keep the recursive function focused on the tree structure, so I’m going to extract out find_assign_call() into a separate function. find_assign_call &lt;- function(x) { if (is_call(x, &quot;&lt;-&quot;) &amp;&amp; is.symbol(x[[2]])) { lhs &lt;- as_string(x[[2]]) children &lt;- as.list(x)[-1] } else { lhs &lt;- character() children &lt;- as.list(x) } c(lhs, flat_map_chr(children, find_assign_rec)) } find_assign_rec &lt;- function(x) { switch_expr(x, # Base cases constant = , symbol = character(), # Recursive cases pairlist = flat_map_chr(x, find_assign_rec), call = find_assign_call(x) ) } find_assign(a &lt;- b &lt;- c &lt;- 1) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; find_assign(system.time(x &lt;- print(y &lt;- 5))) #&gt; [1] &quot;x&quot; &quot;y&quot; find_assign(names(l) &lt;- &quot;b&quot;) #&gt; character(0) While the complete version of this function is quite complicated, it’s important to remember we wrote it by working our way up by writing simple component parts. 18.5.3 Exercises logical_abbr() works with expressions. It currently fails when you give it a function. Why not? How could you modify logical_abbr() to make it work? What components will you need to recurse over? f &lt;- function(x = TRUE) { g(x + T) } logical_abbr(!!f) Write a function that extracts all calls to a function. Instead of writing a wrapper that quotes its input, we could have made the recursive function itself a quoting function. Why is this suboptimal? You might be familiar with the name Quine from “quines”, computer programs that when run return a copy of their own source code.↩ These two sources of ambiguity do not exist without infix operators, which can be considered an advantage of purely prefix and postfix languages. It’s interesting to compare a simple arithmetic operation in Lisp (prefix) and Forth (postfix). In Lisp you’d write (+ (+ 1 2) 3)); this avoids ambiguity by requiring parentheses everywhere. In Forth, you’d write 1 2 + 3 +; this doesn’t require any parentheses, at the cost of requiring more thought when reading.↩ "],
["quasiquotation.html", "19 Quasiquotation 19.1 Introduction 19.2 Quotation 19.3 Evaluation 19.4 Unquotation 19.5 Case studies 19.6 Dot-dot-dot (...)", " 19 Quasiquotation 19.1 Introduction Now that you understand the tree structure of R code, it’s time to come back to one of the fundamental ideas that make expr() and ast() work: quasiquotation. There are two sides to quasiquotation: Quotation allows the developer to capture the AST associated with a function argument without evaluating it. Unquotation allows the user to selectively evaluate parts of an expression that would otherwise be captured. The combination of these two ideas makes it easy to compose expressions that are mixtures of direct and indirect specification, and helps to solve a wide variety of challenging problems. In this chapter, we’ll … Quoting functions have deep connections to Lisp macros. But macros are ususally run at compile-time, which doesn’t have any meaning in R, and they always input and output ASTs. (Lumley (2001) shows one way you might implement them in R). Quoting functions are more closely related to Lisp fexprs, functions where all arguments are quoted by default. These terms are useful to know when looking for related techniques in other programming languages. library(rlang) if (packageVersion(&quot;rlang&quot;) &lt; &quot;0.1.6.9002&quot;) { stop(&quot;Chapter currently relies on development version of rlang&quot;) } 19.1.1 Motivation We’ll start with a simple and concrete example that helps motivate the need for unquoting, and hence quasiquotation. Imagine you’re creating a lot of strings by joining together words: paste(&quot;Good&quot;, &quot;morning&quot;, &quot;Hadley&quot;) #&gt; [1] &quot;Good morning Hadley&quot; paste(&quot;Good&quot;, &quot;afternoon&quot;, &quot;Alice&quot;) #&gt; [1] &quot;Good afternoon Alice&quot; You are sick and tired of writing all those quotes, and instead you just want to use bare words. To that end, you’ve managed to write the following function: library(rlang) library(tidyverse) #&gt; ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── #&gt; ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 #&gt; ✔ tibble 1.4.2 ✔ dplyr 0.7.4 #&gt; ✔ tidyr 0.8.0 ✔ stringr 1.2.0 #&gt; ✔ readr 1.1.1 ✔ forcats 0.2.0 #&gt; ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── #&gt; ✖ purrr::%@%() masks rlang::%@%() #&gt; ✖ purrr::%||%() masks rlang::%||%() #&gt; ✖ purrr::as_function() masks rlang::as_function() #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ purrr::flatten() masks rlang::flatten() #&gt; ✖ purrr::flatten_chr() masks rlang::flatten_chr() #&gt; ✖ purrr::flatten_dbl() masks rlang::flatten_dbl() #&gt; ✖ purrr::flatten_int() masks rlang::flatten_int() #&gt; ✖ purrr::flatten_lgl() masks rlang::flatten_lgl() #&gt; ✖ purrr::invoke() masks rlang::invoke() #&gt; ✖ dplyr::lag() masks stats::lag() #&gt; ✖ purrr::list_along() masks rlang::list_along() #&gt; ✖ purrr::modify() masks rlang::modify() #&gt; ✖ purrr::rep_along() masks rlang::rep_along() #&gt; ✖ purrr::splice() masks rlang::splice() cement &lt;- function(...) { exprs(...) %&gt;% map(expr_name) %&gt;% paste(collapse = &quot; &quot;) } cement(Good, morning, Hadley) #&gt; [1] &quot;Good morning Hadley&quot; cement(Good, afternoon, Alice) #&gt; [1] &quot;Good afternoon Alice&quot; (We’ll talk about the details of this implementation later; for now just look at the results.) Formally, this function quotes the arguments in .... You can think of it as automatically putting quotation marks around each argument. That’s not precisely true as the intermediate objects it generates are expressions, not strings, but it’s a useful approximation for now. This function is nice because we no longer need to type quotes. The problem, however, comes when we want to use variables. It’s easy to use variables with paste() as we just don’t surround them with quotes: name &lt;- &quot;Hadley&quot; time &lt;- &quot;morning&quot; paste(&quot;Good&quot;, time, name) #&gt; [1] &quot;Good morning Hadley&quot; Obviously this doesn’t work with cement() because every input is automatically quoted: cement(Good, time, name) #&gt; [1] &quot;Good time name&quot; We need some way to explicitly unquote the input, to tell cement() to remove the automatic quote marks. Here we need time and name to be treated differently to Good. Quasiquotation give us a standard tool to do so: !!, called “unquote”, and prounounced bang-bang. !! tells a quoting function to drop the implicit quotes: cement(Good, !!time, !!name) #&gt; [1] &quot;Good morning Hadley&quot; It’s useful to compare cement() and paste() directly. paste() evaluates its arguments, so we need to quote where needed; cement() quotes its arguments, so we need to unquote where needed. paste(&quot;Good&quot;, time, name) cement(Good, !!time, !!name) 19.1.2 Theory Now that you’ve seen the basic idea, it’s time to talk a little bit about the theory. The idea of quasiquotation is an old one. It was first developed by a philosopher, Willard van Orman Quine3, in the early 1940s. It’s needed in philosophy because it helps when precisely delineating the use and mention of words, i.e. between the object and the words we use to refer to that object. Quasiquotation was first used in a programming language, LISP, in the mid-1970s (Bawden 1999). LISP has one quoting function `, and uses , for unquoting. Most languages with a LISP heritage behave similarly. For example, racket (` and @), clojure (` and ~), and julia (: and @) all have quasiquotation tools that different only slightly from LISP. Quasiquotation has only come to R recently (2017). Despite its newness, I teach it in this book because it is a rich and powerful theory that makes many hard problems much easier. Quaisquotation in R is a little different to LISP and descendents. In LISP there is only one function that does quasiquotation (the quote function), and you must call it explicitly when needed. This makes these languages less ambiguous (because there’s a clear code signal that something odd is happening), but is less appropriate for R because quasiquotation is such an important part of DSLs for data analysis. 19.2 Quotation The first part of quasiquotation is quotation: capturing an AST without evaluating it. There are two components to this: capturing an expression directly, and capturing an expression from a lazily-evaluated functional argument. We’ll discuss two sets of tools for these two ways of capturing: those provided by rlang, and those provided by base R. 19.2.1 With rlang There are four important quoting functions. For interactive exploration, the most important quoting function is expr(). It captures its argument exactly as provided: expr(x + y) #&gt; x + y expr(1 / 2 / 3) #&gt; 1/2/3 (Remember that white space and comments are not part of the AST, so will not be captured by an quoting function.) expr() is great for interactive exploration, but it’s not useful inside a function, because it always returns exactly what it’s called with: f &lt;- function(x) expr(x) f(x + y + z) #&gt; x Instead, we need another function: enexpr(). This looks at the internal promise object that powers lazy evaluation in R and extracts the expression that was passed to the function: f &lt;- function(x) enexpr(x) f(x + y + z) #&gt; x + y + z (Occassionaly you just want to capture symbols, and throw an error for other types of input. In that case you can use ensym(). In the next chapter, you’ll learn about enquo() which also captures the environment and is needed for tidy evaluation.) To capture multiple arguments, use enexprs(): f &lt;- function(...) enexprs(...) f(x = 1, y = 10 * z) #&gt; $x #&gt; [1] 1 #&gt; #&gt; $y #&gt; 10 * z Finally, exprs() is useful interactively to make a list of expressions: exprs(x = x ^ 2, y = y ^ 3, z = z ^ 4) #&gt; $x #&gt; x^2 #&gt; #&gt; $y #&gt; y^3 #&gt; #&gt; $z #&gt; z^4 # shorthand for # list(x = expr(x ^ 2), y = expr(y ^ 3), z = expr(z ^ 4)) Use enexpr() and enexprs() inside a function when you want to capture the expressions supplied as arguments by the user of that function. Use expr() and exprs() when you want to capture expressions that you supply. There’s not much you can do with a list of expressions yet, but we’ll see a few techniques later in case studies: using purrr to work with list of expressions turns out to be a surprisingly powerful tool. 19.2.2 With base R The base equivalent of expr() is quote(): quote(x + y) #&gt; x + y quote(1 / 2 / 3) #&gt; 1/2/3 It is identical to expr() except that does not support unquoting, so it a quoting function, not a quasiquoting function. The base function closest to enexpr() is substitute(): f &lt;- function(x) substitute(x) f(x + y + z) #&gt; x + y + z You’ll most often see it used to capture unevaluated arguments; often in concert with deparse() to create labels for output. However, substitute() also does “substitution”: if you give it an expression, rather than a symbol, it will substitute in values of symbols defined in the current environment. f2 &lt;- function(x) substitute(x * 2) f2(x + y + z) #&gt; (x + y + z) * 2 substitute() provides a sort of automatic unquoting for any symbol that is bound to a value. However, making use of this behaviour can make for hard to read code, because for example, taken out of context, you can’t tell if the goal of substitute(x + y) is to replace x, or, y, or both. If you do want to use substitute() in this way, I recomend that you use the 2nd argument to be explicit: substitute(x * y * z, list(x = 10, y = quote(a + b))) #&gt; 10 * (a + b) * z The base equivalent to exprs() is alist(): alist(x = 1, y = x + 2) #&gt; $x #&gt; [1] 1 #&gt; #&gt; $y #&gt; x + 2 There are two other important base quoting functions that we’ll cover in depth elsewhere: bquote() provides a limited form of quasiquotation, and is discussed in unquoting with base R. ~, the formula, is a quoting function that also captures the environment. It’s the inspiration for quosures, and is discussed in [formulas]. 19.2.3 Exercises What happens if you try and use enexpr() with an expression? What happens if you try and use enexpr() with a missing argument? Compare and contrast the following two functions. Can you predict the ouput before running them? f1 &lt;- function(x, y) { exprs(x = x, y = y) } f2 &lt;- function(x, y) { enexprs(x = x, y = y) } f1(a + b, c + d) #&gt; $x #&gt; x #&gt; #&gt; $y #&gt; y f2(a + b, c + d) #&gt; $x #&gt; a + b #&gt; #&gt; $y #&gt; c + d What does the following command return? What information is lost? Why? expr({ x + y # comment }) The documentation for substitute() says: Substitution takes place by examining each component of the parse tree as follows: If it is not a bound symbol in env, it is unchanged. If it is a promise object, i.e., a formal argument to a function or explicitly created using delayedAssign(), the expression slot of the promise replaces the symbol. If it is an ordinary variable, its value is substituted, unless env is .GlobalEnv in which case the symbol is left unchanged. Create four examples that illustrate each of the different cases. Why does as.Date.default() use substitute() and deparse()? Why does pairwise.t.test() use them? Read the source code. pairwise.t.test() assumes that deparse() always returns a length one character vector. Can you construct an input that violates this expectation? What happens? 19.3 Evaluation Typically you have quoted a function argument for one of two reasons: You want to operate on the AST using the techniques described in the previous chapter. You want to run, or evaluate the code in a special context, as described in depth next chapter. Evaluation is a rich topic, so it gets a complete chapter. Here we’ll just illustrate the most important ideas. The most important base R function is base::eval(). Its first argument is the expression to evalute: ru5 &lt;- expr(runif(5)) ru5 #&gt; runif(5) eval(ru5) #&gt; [1] 0.0808 0.8343 0.6008 0.1572 0.0074 eval(ru5) #&gt; [1] 0.466 0.498 0.290 0.733 0.773 Note that every time we evaluate this expression we get a different result. The second argument to eval() is the environment in which the expression is evaluated. Manipulating the functions in this environment give us amazing power to control the execution of R code. This is the basic technique gives dplyr the ability to turn R code into SQL. x &lt;- 9 fx &lt;- expr(f(x)) eval(fx, env(f = function(x) x * 10)) #&gt; [1] 90 eval(fx, env(f = function(x) x ^ 2)) #&gt; [1] 81 19.4 Unquotation Evaluation is a developer tool: in combination with quoting, it allows the author of a function to capture an argument and evaluate it in a special way. Unquoting is related to evaluation, but it’s a user tool: it allows the person calling the function to selectively evaluate parts of the expresion that would otherwise be quoted. Another way of thinking about unquoting is that it provides a code template: you define an AST with some “holes” that get filled in using unquoting. 19.4.1 With rlang All quoting functions in rlang (expr(), enexpr(), and friends) supporting unquoting with !! (called “unquote”, and pronounced bang-bang) and !!! (called “unquote-splice”, and pronounced bang-bang-bang). They both replace nodes in the AST. !! is a one-to-one replacement. It takes a single expression and inlines the AST at the location of the !!. x &lt;- expr(a + b + c) expr(f(!!x, y)) #&gt; f(a + b + c, y) !!! is a one-to-many replacement. It takes a list of expressions and inserts them at them at the location of the !!!: x &lt;- exprs(1, 2, 3, y = 10) expr(f(!!!x, z = z)) #&gt; f(1, 2, 3, y = 10, z = z) 19.4.2 The polite fiction of !! So far we have acted as if !! and !!! are regular prefix operators like + , -, and !, but they’re not. Instead, from R’s perspective, !! and !!! are simply the repeated application of !: !!TRUE #&gt; [1] TRUE !!!TRUE #&gt; [1] FALSE !! and !!! have special behaviour inside all quoting functions powered by rlang, and the unquoting operators are given precedence similar to + and -, not !. We do this because the operator precedence for ! is surprisingly low: it has lower precedence that than of the binary algebraic and logical operators. Most of the time this doesn’t matter as it is unusual to mix ! and binary operators (e.g. you typically would not write !x + y or !x &gt; y). However, expressions like !!x + !!y are not uncommon when unquoting, and requring explicit parentheses, (!!x) + (!!y), feels onerous. For this reason, rlang manipulates the AST to give the unquoting operators a higher, and more natural, precedence. You might wonder why rlang does not use a regular function call. Indeed, early versions of rlang provided UQ() and UQS() as alternatives to !! and !!!. However, these looked like regular function calls, rather than special syntactic operators, and gave people a misleading mental model, which made them harder to use correctly. In particular, function calls only happen (lazily) at evaluation time; unquoting always happens at quotation time. We adopted !! and !!! as the best compromise: they are strong visual symbols, don’t look like existing syntax, and take over a rarely used piece of syntax. (And if for some reason you do need to doubly negate a value in a quasiquoting function, you can just add parentheses !(!x).) One place where the illusion currently breaks down is base::deparse(): x &lt;- quote(!!x + !!y) deparse(x) #&gt; [1] &quot;!(!x + (!(!y)))&quot; Although the R parser can distinguish between !(x) and !x, the deparser currently does not. You are most likely to see this when printing the source for a function in another package, where the source references have been lost. rlang::expr_deparse() works around this problem if you need to manually deparse an expression, but often this does not help because the deparsing occurs outside of your control. expr_deparse(x) #&gt; [1] &quot;!!x + (!!y)&quot; One place you’ll see this is during debugging. 19.4.3 With base R Base R has one function that implements quasiquotation: bquote(). It uses .() for unquoting, but does not support unquote-splicing: xyz &lt;- bquote((x + y + z)) bquote(-.(xyz) / 2) #&gt; -(x + y + z)/2 However, bquote() is not used by any other function in base R. Instead functions that quote an argument must use some other technique to allow indirect specification (none of which are unquoting). There are four basic forms seen in base R: A pair of quoting and non-quoting functions. For example, $ has two arguments, and the second argument is quoted. This is easier to see if you write in prefix form: mtcars$cyl is equivalent to `$`(mtcars, cyl). If you want to refer to a variable indirectly, you use [[, as it takes the name of a variable as a string. x &lt;- list(var = 1, y = 2) var &lt;- &quot;y&quot; x$var #&gt; [1] 1 x[[var]] #&gt; [1] 2 &lt;-/assign() and ::/getExportedValue() work similarly. A pair of quoting and non-quoting arguments. For example, data(), rm(), and save() allow you to provide bare variable names in ..., or a character vector of variable names in list: x &lt;- 1 rm(x) y &lt;- 2 vars &lt;- c(&quot;y&quot;, &quot;vars&quot;) rm(list = vars) An argument that controls whether a different argument is quoting or non-quoting. For example, in library(), the character.only argument controls the quoting behaviour of of the first argument, package: library(MASS) pkg &lt;- &quot;MASS&quot; library(pkg, character.only = TRUE) demo(), detach(), example(), and require() work similarly. Quoting if evaluation fails. For example, the first argument to help() is non-quoting if it evaluates to a string; if evaluation fails, the first argument is quoted. # Shows help for var help(var) var &lt;- &quot;mean&quot; # Shows help for mean help(var) var &lt;- 10 # Shows help for var help(var) ls(), page(), and match.fun() work similarly. Some quoting functions, like subset(), transform(), and with(), don’t have a non-quoting form. This is because they are primarily wrappers around [ and [&lt;-. Another important class of quoting functions are the base modelling and plotting functions, which quote some of their arguments. For example, lm() quotes the weight and subset arguments, and when used with a formula argument, the plotting function quote the aesthetic arguments (col, cex, etc): palette(RColorBrewer::brewer.pal(3, &quot;Set1&quot;)) plot(Sepal.Length ~ Petal.Length, data = iris, col = Species, pch = 20, cex = 2) These functions follow the so-called standard non-standard evaluation rules: http://developer.r-project.org/nonstandard-eval.pdf, and do not provide a standard escaping mechanism. Instead, as we’ll learn in the next chapter, you must use other tools to construct expressions then evaluate them. 19.4.4 Non-standard ASTs Using unquoting, it is easy to create non-standard ASTs, i.e. ASTs that contain components that are not constants, symbols, or calls. (It is also possible to create non-standard ASTs by directly manipulating the underlying objects, but it’s harder to do so accidentally) These are valid, and occasionally useful, but their correct use is beyond the scope of this book. However, it’s important to learn about them because they can be printed in misleading ways. For example, if you inline more complex objects, their attributes are not printed. This can lead to confusing output: x1 &lt;- expr(class(!!data.frame(x = 10))) x1 #&gt; class(list(x = 10)) lobstr::ast(!!x1) #&gt; █─class #&gt; └─&lt;inline data.frame&gt; eval(x1) #&gt; [1] &quot;data.frame&quot; In other cases, R will print parentheses that do not exist in the AST: y2 &lt;- expr(2 + 3) x2 &lt;- expr(1 + !!y2) x2 #&gt; 1 + (2 + 3) lobstr::ast(!!x2) #&gt; █─`+` #&gt; ├─1 #&gt; └─█─`+` #&gt; ├─2 #&gt; └─3 And finally, R will display integer sequences as if they were generated with :. x3 &lt;- expr(f(!!c(1L, 2L, 3L, 4L, 5L))) x3 #&gt; f(1:5) lobstr::ast(!!x3) #&gt; █─f #&gt; └─&lt;inline integer&gt; In general, if you’re ever confused about what is actually in an AST, display the object with lobstr::ast()! 19.4.5 Missing arguments f &lt;- function(x) { x &lt;- enexpr(x) expr(foo(!!maybe_missing(x))) } f() #&gt; foo() f(x) #&gt; foo(x) Generally best reserved for working with existing functions. For new functions, it’s usually preferably to use NULL as a default for optional arguments. See a case study below to generate calls like x[,,1]. 19.4.6 Exercises Given the following components: xy &lt;- expr(x + y) xz &lt;- expr(x + z) yz &lt;- expr(y + z) abc &lt;- exprs(a, b, c) Use quasiquotation to construct the following calls: (x + y) / (y + z) -(x + z) ^ (y + z) (x + y) + (y + z) - (x + y) atan2(x + y, y + z) sum(x + y, x + y, y + z) sum(a, b, c) mean(c(a, b, c), na.rm = TRUE) foo(a = x + y, b = y + z) Explain why both !0 + !0 and !1 + !1 return FALSE while !0 + !1 returns TRUE. Base functions match.fun(), page(), and ls() all try to automatically determine whether you want standard or non-standard evaluation. Each uses a different approach. Figure out the essence of each approach by reading the source code, then compare and contrast the techniques. The following two calls print the same, but are actually different: (a &lt;- expr(mean(1:10))) #&gt; mean(1:10) (b &lt;- expr(mean(!!(1:10)))) #&gt; mean(1:10) identical(a, b) #&gt; [1] FALSE What’s the difference? Which one is more natural? 19.5 Case studies Before we continue to the final section of this chapter, the techniques of quasiquotation translated to ..., we have XYZ case studies showing quasiquotation can be used in practice to solve real problems. We’ll also see a couple of examples combining quasiquotation with purrr to generate complex expressions programmatically. These case studies will use purrr to iterate over lists of expressions to transform them in various ways, and dplyr to motivate some of the problems. We’ll come back to the base quoting functions in the next chapter. Because they don’t support quasiquotation, we’ll need to use techniques of evaluation as well. library(purrr) library(dplyr) 19.5.1 Map-reduce to generate code Quasiquotation gives us powerful tools for generating code, particularly when combined with purrr::map() and purr::reduce(). For example, assume you have a linear model specified by the following coefficients: intercept &lt;- 10 coefs &lt;- c(x1 = 5, x2 = -4) You want to convert it into an expression like 10 + (5 * x1) + (-4 * x2) so you can evaluate it with new data. The first thing we need to turn is turn the character names vector into a list of symbols. rlang::syms() is designed precisely for this case: coef_sym &lt;- syms(names(coefs)) coef_sym #&gt; [[1]] #&gt; x1 #&gt; #&gt; [[2]] #&gt; x2 Next we need to combine each variable name with its coefficient. We can do this by combining expr() with map map2(): summands &lt;- map2(coef_sym, coefs, ~ expr((!!.x * !!.y))) summands #&gt; [[1]] #&gt; (x1 * 5) #&gt; #&gt; [[2]] #&gt; (x2 * -4) In this case, the intercept is also a part of the sum, although it doesn’t involve a multiplication. We can just add it to the start: summands &lt;- c(intercept, summands) summands #&gt; [[1]] #&gt; [1] 10 #&gt; #&gt; [[2]] #&gt; (x1 * 5) #&gt; #&gt; [[3]] #&gt; (x2 * -4) Finally, we need to reduce the individual terms in to a single sum by adding the pieces together: eq &lt;- reduce(summands, ~ expr(!!.x + !!.y)) eq #&gt; 10 + (x1 * 5) + (x2 * -4) As you’ll learn in the next chapter, we than easily evaluate this with any data: df &lt;- data.frame(x1 = runif(5), x2 = runif(5)) eval(eq, df) #&gt; [1] 13.59 9.26 9.92 10.05 8.11 Or alternatively we could turn it into a function: args &lt;- map(coefs, ~ missing_arg()) new_function(args, expr({!!eq})) #&gt; function (x1, x2) #&gt; { #&gt; 10 + (x1 * 5) + (x2 * -4) #&gt; } Very many code generation problems can be solved in a similar way, combining quasiquotation with map functions. 19.5.2 Calling quoting functions One downside of quoting functions is that when you call a lot of them, it is harder to wrap them in another function in order to reduce duplication. For example, if you see the following code: (x - min(x)) / (max(x) - min(x)) (y - min(y)) / (max(y) - min(y)) ((x + y) - min(x + y)) / (max(x + y) - min(x + y)) You can eliminate the reptition with a rescale01 function: rescale01 &lt;- function(x) (x - min(x)) / (max(x) - min(x)) rescale01(x) rescale01(y) rescale01(x + y) Reducing the duplication here is a very good idea, because when you realise that your rescaling technique doesn’t handle missing values gracefully, you only have one place to fix it. It is easy to create rescale01() because min(), max(), and - , are regular functions: they don’t quote any of their arguments. It’s harder to reduce duplication when a function quotes one or more arguments. For example, if you notice this repeated code: df %&gt;% group_by(x1) %&gt;% summmarise(mean = mean(y1)) df %&gt;% group_by(x2) %&gt;% summmarise(mean = mean(y2)) df %&gt;% group_by(x3) %&gt;% summmarise(mean = mean(y3)) This naive function will not work: grouped_mean &lt;- function(df, x, y) { df %&gt;% group_by(x) %&gt;% summmarise(mean = mean(y)) } Because regardless of the input, grouped_mean() will always group by x and compute the mean of y. However, because group_by() and summarise() (like all quoting functions in the tidyverse) use quasiquotation, there’s a standard way to wrap these functions: you quote and then unquote: grouped_mean &lt;- function(df, x, y) { x &lt;- enexpr(x) y &lt;- enexpr(y) df %&gt;% group_by(!!x) %&gt;% summarise(mean = mean(!!y)) } mtcars %&gt;% grouped_mean(cyl, mpg) #&gt; # A tibble: 3 x 2 #&gt; cyl mean #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4.00 26.7 #&gt; 2 6.00 19.7 #&gt; 3 8.00 15.1 (In the next chapter, we’ll learn why enexpr() is not quite general enough and how and why to use quo(), enquo(), and enquos() instead.) This is a powerful pattern that allows you reduce duplication in your code when it uses quasiquoting functions. 19.5.3 Partition It’s often useful to combine these two ideas: quoting input arguments and then mildly manipulating them with purrr. For example, imagine that you want to extend dplyr::select() to return two data frames: one with the variables you selected, and one with the variables that remain. (This problem was inspired by https://stackoverflow.com/questions/46828296/.) There are plenty of ways to attack this problem, but one way is to take advantage of select()'s ability to negate column selection expression in order to remove those columns. We can capture the inputs with quasiquotation, then invert each selection call by negating it. We practice interactively with a list of variables created with exprs(): vars &lt;- exprs(x, y, c(a, b), starts_with(&quot;x&quot;)) vars %&gt;% map(~ expr(-!!.)) #&gt; [[1]] #&gt; -x #&gt; #&gt; [[2]] #&gt; -y #&gt; #&gt; [[3]] #&gt; -c(a, b) #&gt; #&gt; [[4]] #&gt; -starts_with(&quot;x&quot;) Then turn it into a function: # library(rlang) # library(purrr) # library(dplyr) # partition.fun &lt;- function(.data, ...) { # included &lt;- enexprs(...) # excluded &lt;- map(included, ~ expr(-!!.x)) # # list( # incl = select(.data, !!!included), # excl = select(.data, !!!excluded) # ) # } # # df &lt;- data.frame(x1 = 1, x2 = 3, y = &quot;a&quot;, z = &quot;b&quot;) # partition.fun(df, starts_with(&quot;x&quot;)) # $incl # x1 x2 # 1 1 3 # $excl # y z # 1 a b Note the name of the first argument: .data. This is a standard convention through the tidyverse because you don’t need to explicitly name this argument (because it’s always used), and it avoids potential clashes with argument names in .... 19.5.4 Tangling with dots In our grouped_mean() example above, we allow the user to select one grouping variable, and one summary variable. What if we wanted to allow the user to select more than one? One option would be to use .... There are three possible ways we could use ... it: Pass ... onto the mean() function. That would make it easy to set na.rm = TRUE. This is easiest to implement. Allow the user to select multiple groups Allow the user to select multiple variables to summarise. Implementing each one of these is relatively straightforward, but what if we want to be able to group by multiple variables, summarise multiple variables, and pass extra args on to mean(). Generally, I think it is better to avoid this sort of API (instead relying on multiple function that each do one thing) but sometimes it is the lesser of the two evils, so it is useful to have a technique in your backpocket to handle it. grouped_mean &lt;- function(df, groups, vars, args) { var_means &lt;- map(vars, function(var) expr(mean(!!var, !!!args))) names(var_means) &lt;- map_chr(vars, expr_name) df %&gt;% dplyr::group_by(!!!groups) %&gt;% dplyr::summarise(!!!var_means) } grouped_mean(mtcars, exprs(vs, am), exprs(hp, drat, wt), list(na.rm = TRUE)) #&gt; # A tibble: 4 x 5 #&gt; # Groups: vs [?] #&gt; vs am hp drat wt #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0 194. 3.12 4.10 #&gt; 2 0 1 181. 3.94 2.86 #&gt; 3 1 0 102. 3.57 3.19 #&gt; 4 1 1 80.6 4.15 2.03 If you use this design a lot, you may also want to provide an alias to exprs() with a better name. For example, dplyr provides the vars() wrapper to support the scoped verbs (e.g. summarise_if(), mutate_at()). aes() in ggplot2 is similar, although it does a little more: requires all arguments be named, naming the the first arguments (x and y) by default, and automatically renames so you can use the base names for aesthetics (e.g. pch vs shape). grouped_mean(mtcars, vars(vs, am), vars(hp, drat, wt), list(na.rm = TRUE)) 19.5.5 Slicing an array One useful tool missing from base R is the ability to extract a slice from an array given a dimension to slice along and an index to take from the slice. For example, if you want to find the first slice of dimension 2 in a 3d array you’d write x[, 1, ]. How can we write a function to make this simpler? It’s more challenging than usual because most of the arguments are missing. slice &lt;- function(x, along, index) { stopifnot(length(index) == 1) nd &lt;- length(dim(x)) indices &lt;- rep(list(missing_arg()), nd) indices[along] &lt;- index expr(x[!!!indices]) } x &lt;- array(sample(30), c(5, 2, 3)) slice(x, 1, 3) #&gt; x[3, , ] slice(x, 2, 2) #&gt; x[, 2, ] slice(x, 3, 1) #&gt; x[, , 1] A real slice() would evaluate the generated call, but here I think it’s more illimunating to see the code that’s generated. 19.5.6 Exercises Implement filter_or(), a variant of dplyr::filter() that combines multiple arguments using | instead of &amp;. Implement arrange_desc(), a variant of dplyr::arrange() that sorts in descending order by default. Implement the three variants of grouped_mean() described above: # ... passed on to mean grouped_mean &lt;- function(df, group_by, summarise, ...) {} # ... selects variables to summarise grouped_mean &lt;- function(df, group_by, ...) {} # ... selects variables to group by grouped_mean &lt;- function(df, ..., summarise) {} Add error handling to slice(). Give clear error messages if either along or index have invalid values (i.e. not numeric, not length 1, too small, or too big). 19.6 Dot-dot-dot (...) Quasiquotation ensures that every quoted argument has an escape hatch that allows the user to unquote, or evaluated, selected components, if needed. A similar and related needs arises with functions that take arbitrary additional arguments with .... Take the following two motivation problems: What do you do if the elements you want to put in ... are already stored in a list? For example, imagine you have a list of data frames that you want to rbind() together: dfs &lt;- list( a = data.frame(x = 1, y = 2), b = data.frame(x = 3, y = 4) ) You could solve this specific case with rbind(dfs$a, df$b), but how do you generalise that solution to a list of arbitrary length? What do you do if you want to supply the argument name indirectly? For example, imagine you want to create a single column data frame where the name of the column is specified in a variable: var &lt;- &quot;x&quot; val &lt;- c(4, 3, 9) In this case, you could create a data frame and then change names (ie. setNames(data.frame(val), var)), but this feels inelegant. How can we do better? 19.6.1 do.call() Base R provides a swiss-army knife to solve these problems: do.call(). do.call() has two main arguments. The first argument, what, gives a funtion to call. The second argument, args, is a list of arguments to pass to that function, and so do.call(&quot;f&quot;, list(x, y, z)) is equivalent to f(x, y, z). do.call() gives a straightforward solution to rbind()ing together many data frames: do.call(&quot;rbind&quot;, dfs) #&gt; x y #&gt; a 1 2 #&gt; b 3 4 With a little more work, we can use do.call() to solve the second problem. We first create a list of arguments, then name that, then use do.call(): args &lt;- list(val) names(args) &lt;- var do.call(&quot;data.frame&quot;, args) #&gt; x #&gt; 1 4 #&gt; 2 3 #&gt; 3 9 19.6.2 The tidyverse approach An alternative way to solve this problems hinges on the recognition that we can reframe these problems as a need for splicing and unquoting argument names. For example, the tidyverse equivalents of rbind() and data.frame(), dplyr::bind_rows() and tibble::tibble(), do not require an additional helper, instead using the same syntax as unquoting: dplyr::bind_rows(!!!dfs) #&gt; x y #&gt; 1 1 2 #&gt; 2 3 4 tibble::tibble(!!var := val) #&gt; # A tibble: 3 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 4 #&gt; 2 3 #&gt; 3 9 Note the use of := (pronounced colon-equals) rather than =. Unforunately we need this new operation because R’s grammar does not allow expressions as argument names: tibble::tibble(!!var = value) #&gt; Error: unexpected &#39;=&#39; in &quot;tibble::tibble(!!var =&quot; := is like a vestigal organ: it’s recognised by R’s parser, but it doesn’t have any code associated with it. It looks like an = but allows expressions on either side, making it a more flexible alternative to =. It is used in data.table for similar reasons. When used in this context, the behaviour of !!! is sometimes called spatting. It is closed related to star-args (*args) and star-star-kwargs (**kwarg) in Python. 19.6.3 dots_list() Both dplyr::bind_rows() and tibble::tibble() are powered by rlang::dots_list(...). This function is very similar to list(...), but it interprets !!! and !! specially. If you want to take advantage of this behaviour in your own function, all you need to do is use dots_list() in your own code. For example, imagine you want to make a version of structure() that understands !!! and !!. We’ll call it set_attr(): set_attr &lt;- function(.x, ...) { attr &lt;- rlang::dots_list(...) attributes(.x) &lt;- attr .x } attrs &lt;- list(x = 1, y = 2) attr_name &lt;- &quot;z&quot; 1:10 %&gt;% set_attr(w = 0, !!!attrs, !!attr_name := 3) %&gt;% str() #&gt; int [1:10] 1 2 3 4 5 6 7 8 9 10 #&gt; - attr(*, &quot;w&quot;)= num 0 #&gt; - attr(*, &quot;x&quot;)= num 1 #&gt; - attr(*, &quot;y&quot;)= num 2 #&gt; - attr(*, &quot;z&quot;)= num 3 (rlang also provides a set_attr() function with a few extra conveniences, but the essence is the same.) Note that we call the first argument .x: whenever you use ... to take arbitrary data, it’s good practice to give the other argument names a . prefix. This eliminates any ambiguity about argument names. dots_list() provides one other handy feature: by default it will ignore any empty argument at the end. This is useful in functions like tibble::tibble() because it means that you can easily change the order of variables without worrying about the final comma: # Can easily move x to first entry: tibble::tibble( y = 1:5, z = 3:-1, x = 5:1, ) # Need to remove comma from z and add column to x data.frame( y = 1:5, z = 3:-1, x = 5:1 ) Use the .ignore_empty() argument to change this behaviour. 19.6.4 Application: invoke() and lang() One useful application of dots_list() is invoke(): invoke &lt;- function(.f, ...) { do.call(.f, dots_list(...), envir = parent.frame()) } (At time of writing, purrr::invoke() and rlang::invoke() had somewhat different definitions because they were written before we understood how quasiquotation syntax and ... intersected.) As a wrapper around do.call(), invoke() gives powerful ways to call functions with arguments supplied directly (in …) or indirectly (in a list): invoke(&quot;mean&quot;, x = 1:10, na.rm = TRUE) # Equivalent to x &lt;- list(x = 1:10, na.rm = TRUE) invoke(&quot;mean&quot;, !!!x) It also allows us to specify argument names indirectly: arg_name &lt;- &quot;na.rm&quot; arg_val &lt;- TRUE invoke(&quot;mean&quot;, 1:10, !!arg_name := arg_val) Closely related to invoke() is lang(): lang(&quot;mean&quot;, 1:10, !!arg_name := arg_val) #&gt; mean(1:10, na.rm = TRUE) lang() generates an expression and is often equivalent to expr(). The advantage of lang() is that you can use :=: you can’t use it with expr() because it does not take .... 19.6.5 Other approaches Apart from rlang::dots_list() there are several other techniques used to overcome the difficulties described above. One technique is is to take ... but also accept a single unnamed argument that is a list, making f(list(x, y, z)) equivalent to f(x, y, z). The implementation looks something like: f &lt;- function(...) { dots &lt;- list(...) if (length(dots) == 1 &amp;&amp; is.list(dots[[1]])) { dots &lt;- dots[[1]] } # Do something ... } Base functions that use this technique include interaction(), expand.grid(), options(), and par(). Since these functions take either a list, or a ..., but not both, they are slightly less flexible than functions powered by dots_list(). Another related technique is used the RCurl::getURL() function written by Duncan Temple Lang. getURL() take both ... and .opts which are concatenated together. This is useful when writing functions to call web APIs because you often have some options that need to be passed to request. You put these in a common list and pass to .opts, saving ... for the options unique for a given call. I found this technique particular compelling so you can see it used throughout the tidyverse. Now, however, rlang::dots_list() dots solves more problems, more elegantly, by using the ideas from tidy eval so the tidyverse is slowly migrating to that style. 19.6.6 Exercises Carefully read the source code for interaction(), expand.grid(), and par(). Compare and constract the techniques they use for switching between dots and list behaviour. Explain the problem with this defintion of set_attr() set_attr &lt;- function(x, ...) { attr &lt;- rlang::dots_list(...) attributes(x) &lt;- attr x } set_attr(1:10, x = 10) #&gt; Error in attributes(x) &lt;- attr: attempt to use zero-length variable name References "],
["evaluation.html", "20 Evaluation 20.1 Introduction 20.2 Evaluation basics 20.3 Quosures 20.4 Tidy evaluation 20.5 Case study: calling base NSE functions", " 20 Evaluation 20.1 Introduction The user-facing opposite of quotation is unquotation: it gives the user the ability to selectively evaluate parts of an otherwise quoted argument. The developer-facing complement of quotation is evaluation: this gives the developer of the function the ability to evaluate quoted expressions in special ways to create domain specific languages for data analysis like ggplot2 and dplyr. Mention tidy evaluation because it’s one of the principles underlying the tidyverse. Why do I teach here when it’s not base R? Because it gives us the following good things. Tidy evaluation is the combination of four big ideas: Quasiquotation to give the user control of quoting. Quosures to capture arguments expressions and their evaluation environment. Data masks to enable expressions to mingle variables from an environment and from a data source. Pronouns to eliminate ambiguity between environment and data source when needed. library(rlang) 20.1.0.0.1 Outline 20.1.0.0.2 Prerequisites Environments play a big role in evaluation, so make sure you’re familiar with Environments before continuing. 20.2 Evaluation basics In the previous chapter, we briefly mentioned eval(). Here, rather than starting with eval(), we’re going to start with rlang::eval_bare() which is the purest evocation of the idea of evaluation. The first argument, expr is an expression to evaluate. This will usually be either a symbol or expression: x &lt;- 10 eval_bare(expr(x)) #&gt; [1] 10 y &lt;- 2 eval_bare(expr(x + y)) #&gt; [1] 12 The second argument, env, gives the environment in which the expression should be evaluated, i.e. where should the values of x, y, and + be looked for? By default, this is the current environment, i.e. the calling environment of eval_bare(), but you can override it if you want: eval_bare(expr(x + y), env(x = 1000)) #&gt; [1] 1002 Because R looks up functions in the same way as variables, we can also override the meaning of functions. This is a key technique for generating DSLs, as discussed in the next chapter. eval_bare(expr(x + y), env(`+` = function(x, y) paste0(x, &quot; + &quot;, y))) #&gt; [1] &quot;10 + 2&quot; If passed an object other than a symbol or expression, the evaluation functions will simply return the input as is (because it’s already evaluated). This can lead to confusing results if you forget to quote() the input: eval_bare() doesn’t quote expr so it is passed by value. eval_bare(x + y) #&gt; [1] 12 eval_bare(x + y, env = env) #&gt; [1] 12 Now that you’ve seen the basics, let’s explore some applications. We’ll focus primarily on base R functions that you might have used before; now you can learn how they work. To focus on the underlying principles, we’ll extracting their essence and rewrite to use functions from rlang. We’ll then circle back and talk about the base R functions most important for evaluation. 20.2.1 Application: local() Sometimes you want to perform a chunk of calculation that creates a bunch of intermediate variables. The intermediate variables have no long term use and could be quite large, so you’d rather not keep them around. One approach is to clean up after yourself using rm(). Another approach is to wrap the code in a function, and just call it once. A more elegant approach is to use local(): # Clean up variables created earlier rm(x, y) foo &lt;- local({ x &lt;- 10 y &lt;- 200 x + y }) foo #&gt; [1] 210 x #&gt; Error in eval(expr, envir, enclos): object &#39;x&#39; not found y #&gt; Error in eval(expr, envir, enclos): object &#39;y&#39; not found The essence of local() is quite simple. We capture the expression, and create an new environment in which to evaluate it. This environment inherits from the caller environment so it can access the current lexical scope. local2 &lt;- function(expr, env = child_env(caller_env())) { eval_bare(enexpr(expr), env) } foo &lt;- local2({ x &lt;- 10 y &lt;- 200 x + y }) env_has(nms = c(&quot;x&quot;, &quot;y&quot;)) #&gt; x y #&gt; FALSE FALSE It’s a bit harder to understand how base::local() works, as it takes uses eval() and substitute() together in rather complicated ways. Figuring out exactly what’s going on is good practice if you really want to understand the subtleties of substitute() and the base eval() funtions. 20.2.2 Application: source() We can create a simple version of source() by combining expr_text() and eval_tidy(). We read in the file from disk, use parse_expr() to parse the string into an list of expressions, and then use eval_bare() to evaluate each component. This version evaluates the code in the caller environment, and invisibly returns the result of the last expression in the file (like source()). source2 &lt;- function(file, env = caller_env()) { lines &lt;- readLines(file, warn = FALSE) code &lt;- paste(lines, collapse = &quot;\\n&quot;) exprs &lt;- parse_exprs(code) res &lt;- NULL for (i in seq_along(exprs)) { res &lt;- eval_bare(exprs[[i]], env) } invisible(res) } The real source() is considerably more complicated because it can echo input and output, and also has many additional settings to control behaviour. 20.2.3 Gotcha: function() x &lt;- 10 y &lt;- 20 f &lt;- eval_bare(expr(function(x, y) !!x + !!y)) f #&gt; function(x, y) !!x + !!y But it works! f() #&gt; [1] 30 What is going on? srcrefs! Two options: remove srcrefs by setting srcref attr to NULL. Alternatively, use new_function(). new_function( exprs(x = , y = ), expr({!!x + !!y}) ) #&gt; function (x, y) #&gt; { #&gt; 10 + 20 #&gt; } 20.2.4 Base R The base function equivalent to eval_bare() is the two-argument form of eval(): eval(expr, envir): eval(expr(x + y), env(x = 1000, y = 1)) #&gt; [1] 1001 The final argument, enclos provides support for data masks, which you’ll learn about in tidy evaluation. eval() is paired with two helper functions: evalq(x, env) quotes its first argument, and is hence a shortcut for eval(quote(x), env). eval.parent(expr, n) is shortcut for eval(x, env = parent.frame(n)). base::eval() has special behaviour for expression objects, evaluating each component in turn. This makes for a very compact implementation of source2() because base::parse() also returns an expression object: source3 &lt;- function(file, env = parent.frame()) { lines &lt;- parse(file) res &lt;- eval(lines, envir = env) invisible(res) } While source3() is considerably more concise than source2(), this one use case is the strongest argument for expression objects, and overall we don’t believe this one benefit outweighs the cost of introducing a new data structure. 20.2.5 Exercises Carefully read the documentation for source(). What environment does it use by default? What if you supply local = TRUE? How do you provide a custom argument? Predict the results of the following lines of code: eval(quote(eval(quote(eval(quote(2 + 2)))))) eval(eval(quote(eval(quote(eval(quote(2 + 2))))))) quote(eval(quote(eval(quote(eval(quote(2 + 2))))))) Write an equivalent to get() using sym() and eval_bare(). Write an equivalent to assign() using sym(), expr(), and eval_bare(). (Don’t worry about the multiple ways of choosing an environment that get() and assign() support; assume that the user supplies it explicitly.) # name is a string get2 &lt;- function(name, env) {} assign2 &lt;- function(name, value, env) {} Modify source2() so it returns the result of every expression, not just the last one. Can you eliminate the for loop? The code generated by source2() lacks source references. Read the source code for sys.source() and the help for srcfilecopy(), then modify source2() to preserve source references. You can test your code by sourcing a function that contains a comment. If successful, when you look at the function, you’ll see the comment and not just the source code. The third argument in subset() allows you to select variables. It treats variable names as if they were positions. This allows you to do things like subset(mtcars, , -cyl) to drop the cylinder variable, or subset(mtcars, , disp:drat) to select all the variables between disp and drat. How does this work? I’ve made this easier to understand by extracting it out into its own function that uses tidy evaluation. select &lt;- function(df, vars) { vars &lt;- enexpr(vars) var_pos &lt;- set_names(as.list(seq_along(df)), names(df)) cols &lt;- eval_tidy(vars, var_pos) df[, cols, drop = FALSE] } select(mtcars, -cyl) We can make base::local() slightly easier to understand by spreading out over multiple lines: local3 &lt;- function(expr, envir = new.env()) { call &lt;- substitute(eval(quote(expr), envir)) eval(call, envir = parent.frame()) } Explain how local() works in words. (Hint: you might want to print(call) to help understand what substitute() is doing, and read the documentation to remind yourself what environment new.env() will inherit from.) 20.3 Quosures The simplest form of evaluation combines an expression and an environment. This coupling is sufficiently important that we need a data structure that captures both pieces. We call this data structure a quosure, a portmanteau of quoting and closure. You almost always want to capture a quosure rather than an expression because it gives you uniformly more information. Once we’ve discussed its primary use case of tidy evaluation, we’ll come back to the few cases where you should prefer expressions. 20.3.1 Motivation Quosures are needed when expressions to be evaluate mix variables from a data frame and variables in the environment. For example, the following mutate() call creates a new variable called log with a calculation that involves a varible in the dataset x, and a variable in the environment, base: df &lt;- data.frame(z = runif(5)) x &lt;- 10 dplyr::mutate(df, log = log(z, base = x)) #&gt; z log #&gt; 1 0.0808 -1.0929 #&gt; 2 0.8343 -0.0787 #&gt; 3 0.6008 -0.2213 #&gt; 4 0.1572 -0.8035 #&gt; 5 0.0074 -2.1308 Also remember that log() itself is found in the global environment, but there’s no confusion about where functions come from because (without gymnastics) you can’t put a function in a data frame. Worrying about the execution environment of an argument is important when you write quoting functions. Take this simple example: compute_mean &lt;- function(df, x) { x &lt;- enexpr(x) dplyr::summarise(df, mean = mean(!!x)) } It works correctly for simple inputs: compute_mean(df, z) #&gt; mean #&gt; 1 0.336 It contains a subtle bug, which we can illustrate with this slightly forced example: x &lt;- 10 compute_mean(df, log(z, base = x)) #&gt; Error in summarise_impl(.data, dots): Evaluation error: non-numeric argument to mathematical function. We get this error because we have lost the evaluation environment associated with log(z, base = x) so it is evaluated inside compute_mean() where is x an AST. This type of bug is pernicious because it will happen rarely and the error message will be inscrutable. We can avoid the bug by the expression along with its evaluation evniroment. That’s the job of enquo(), which otherwise works identically to enexpr(): compute_mean &lt;- function(df, x) { x &lt;- enquo(x) dplyr::summarise(df, mean = mean(!!x)) } compute_mean(mtcars, log(mpg, base = x)) #&gt; mean #&gt; 1 1.28 20.3.2 Creating and manipulating To create a quosure you will typically use one of the equivalents of the expr() functions that you learned about in the previous chapter: Use quo() and quos() for experimenting interactively and for unquoting with fixed expressions inside a function. Use enquo() and enquos() to capture user-supplied arguments to a function. Alternatively, you can use new_quosure() to create a quosure from its components: an expression and an environment. x &lt;- new_quosure(expr(x + y), env(x = 1, y = 10)) x #&gt; &lt;quosure&gt; #&gt; expr: ^x + y #&gt; env: 0x7fa23a26aee8 Note how quosures are printed. If you unquote a quosure inside another quosure, each quosusre starts with ^ and if you’re in console that supports it, each quosure gets a differnt colour to help remind you that it has a different environment attached to it. q2 &lt;- quo(x + !!x) q2 #&gt; &lt;quosure&gt; #&gt; expr: ^x + (^x + y) #&gt; env: global (Note that because quosures capture the complete environment you need to be a little careful if your function returns quosures. If you have large temporary objects they will not get gc’d until the quosure has been gc’d. See XXXXXXX for more details.) You can evaluated a quosure with eval_tidy(), which we’ll study in depth in the next section: eval_tidy(x) #&gt; [1] 11 You can the extract components with quo_ helpers: quo_get_env(x) #&gt; &lt;environment: 0x7fa23a26aee8&gt; quo_get_expr(x) #&gt; x + y And if you need to turn a quosure into text for output to the console you can use quo_name(), quo_label(), or quo_text(). quo_name() and quo_label() are garanteed to be short; quo_expr() may span multiple lines. # https://github.com/tidyverse/rlang/issues/367 y &lt;- quo(long_function_name( argument_1 = long_argument_value, argument_2 = long_argument_value, argument_3 = long_argument_value, argument_4 = long_argument_value )) quo_name(y) # e.g. for data frames #&gt; [1] &quot;long_function_name(...)&quot; quo_label(y) # e.g. for error messages #&gt; [1] &quot;`long_function_name(...)`&quot; quo_text(y) # for longer messages #&gt; [1] &quot;long_function_name(argument_1 = long_argument_value, argument_2 = long_argument_value, \\n argument_3 = long_argument_value, argument_4 = long_argument_value)&quot; 20.3.3 Implementation Quosures are possible because internally R represents function arguments with a special type of object called a promise. A promise captures the expression needed to compute the value and the environment in which to compute it. You’re not normally aware of promises because the first time you access a promise its code is evaluated in its environment, yielding a value. This is what powers lazy evaluation. However, you cannot manipulate promises with R code: they’re sort of quantum; if you attempt to manipulate with R code, they are immediately evaluated, and the promise nature goes away. To work around this, rlang manipulates the promise in C, reifying it into an R object that you can work with. There is one big difference between promises and quosures. An argument is evaluated implicitly when you access it for the first time. Every time you access it subsequently it will return the same value. A quosure must be evaluated explicitly, and each evaluation is independent of the previous evaluations. # The argument x is evaluated once, then reuses foo &lt;- function(x_arg) { list(x1 = x_arg, x2 = x_arg) } foo(runif(3)) #&gt; $x1 #&gt; [1] 0.466 0.498 0.290 #&gt; #&gt; $x2 #&gt; [1] 0.466 0.498 0.290 # The quosure x is evaluated afresh each time x_quo &lt;- quo(runif(3)) eval_tidy(x_quo) #&gt; [1] 0.733 0.773 0.875 eval_tidy(x_quo) #&gt; [1] 0.1749 0.0342 0.3204 Quosures are inspired by the the formula operator, ~, which also captures both the expression and its environment, and is used extremely heavily in R’s modelling functions: f &lt;- ~runif(3) f #&gt; ~runif(3) str(f) #&gt; Class &#39;formula&#39; language ~runif(3) #&gt; ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; Initial versions of rlang used formulas as quosures: an attractive feature of ~ is that is provides quoting with just a single keystroke. Unfortunately, however, there is no way to add quasiquotation to ~, so we decided to use a new function, quo(), instead. 20.3.4 Multiple environments Quosures are particularly important when used with ... because each argument can potentially have a different environment associated with it: f &lt;- function(...) { x &lt;- 1 g(..., x1 = x) } g &lt;- function(...) { x &lt;- 2 h(..., x2 = x) } h &lt;- function(...) { enquos(...) } x &lt;- 0 qs &lt;- f(x0 = x) qs #&gt; $x0 #&gt; &lt;quosure&gt; #&gt; expr: ^x #&gt; env: global #&gt; #&gt; $x1 #&gt; &lt;quosure&gt; #&gt; expr: ^x #&gt; env: 0x7fa2384e6590 #&gt; #&gt; $x2 #&gt; &lt;quosure&gt; #&gt; expr: ^x #&gt; env: 0x7fa2384e6360 purrr::map_dbl(qs, eval_tidy) #&gt; x0 x1 x2 #&gt; 0 1 2 20.3.5 Embedded quosures make_x &lt;- function(x) quo(x) thirty &lt;- quo(!!make_x(0) + !!make_x(10) + !!make_x(20)) thirty #&gt; &lt;quosure&gt; #&gt; expr: ^(^x) + (^x) + (^x) #&gt; env: global If you’re viewing from the console, you’ll see that each quosure is coloured - the point of the colours is to emphasise that the quosures have different environments associated with them even though the expressions are the same. eval_tidy(thirty) #&gt; [1] 30 This was a lot of work to get right. But means that quosures just work, even when embedded inside other quosures. Note that this code doesn’t make any sense at all if we use expressions instead of quosures equivalents, the environment is never captured so all we have make_x &lt;- function(x) expr(x) thirty &lt;- expr(!!make_x(0) + !!make_x(10) + !!make_x(20)) thirty #&gt; x + x + x eval_tidy(thirty) #&gt; [1] 0 20.3.6 When not to use quosures In code generation. When expression will be evaluated completely in data context To call functions that don’t use tidy eval; fuller example next. Sometimes you can avoid using a quosure by inlining/unquoting values. base &lt;- 2 quo(log(x, base = base)) #&gt; &lt;quosure&gt; #&gt; expr: ^log(x, base = base) #&gt; env: global expr(log(x, base = !!base)) #&gt; log(x, base = 2) 20.3.7 Exercises Predict what evaluating each of the following quosures will return. q1 &lt;- new_quosure(expr(x), env(x = 1)) q2 &lt;- new_quosure(expr(x + !!q1), env(x = 10)) q3 &lt;- new_quosure(expr(x + !!q2), env(x = 100)) Run this code in your head and predict what it will print. Confirm or refute your prediction by running the code in R. f &lt;- function(...) { x &lt;- &quot;f&quot; g(f = x, ...) } g &lt;- function(...) { x &lt;- &quot;g&quot; h(g = x, ...) } h &lt;- function(...) { enquos(...) } x &lt;- &quot;top&quot; out &lt;- f(top = x) out purrr::map_chr(out, eval_tidy) 20.4 Tidy evaluation In the previous section, you learn how to capture quosures and why they are important when calling existing functions that use tidy evaluation. In this section, you’ll learn how to create your own functions that use tidy evaluations. There are two big new concepts both related to evaluating code in the context of a data frame: A data mask is a data frame where the evaluated code will look first for variable definitions. A data mask introduces ambiguity, so to remove that ambiguity when necessary we introduce pronouns. We’ll explore tidy evaluation in the context of base::subset(), because it’s a simple yet powerful function that encapsulates one of the central ideas that makes R so elegant for data analysis. Once we’ve seen the tidy implementation, we’ll return to the base R implementation, learn how it works, and explore the downsides which make subset() suitable only for interactive usage. 20.4.1 eval_tidy() Once you have a quosure, you will need to use eval_tidy() instead of eval_bare(): x &lt;- 2 # These two calls are equivalent eval_bare(expr(x), globalenv()) #&gt; [1] 2 eval_tidy(quo(x)) #&gt; [1] 2 Like eval_bare(), eval_tidy() has a env argument, but generally you will not use it, because the environment is captured by the quosure. Instead, you will typically use the second argument, data. This lets you set up a data mask, where variables in the environment are potentially masked by variables in data frame. This allows you to mingle variables from the environment and variables from a data frame: df &lt;- data.frame(y = 1:10) eval_tidy(quo(x * y), df) #&gt; [1] 2 4 6 8 10 12 14 16 18 20 This is the key idea that powers base R functions like with(), subset() and transform(), and that is used through tidyverse packages like dplyr. 20.4.2 Data masks Unlike environments, data frames don’t have parents. This is what allows data masks to work: eval_tidy() effectively creates a new environment that contains the values of data and has a parent of env: df &lt;- data.frame(y = 1:10) x &lt;- 2 q1 &lt;- quo(x * y) # eval_tidy(q1, mtcars) is equivalent to: df_env &lt;- as_env(df, parent = quo_get_env(q1)) q2 &lt;- quo_set_env(q1, df_env) eval_tidy(q2) #&gt; [1] 2 4 6 8 10 12 14 16 18 20 base::eval() has similar functionality. If the 2nd argument is a data frame it becomes a data mask, and you provide the environment in the 3rd argument: eval(quo_get_expr(q1), df, quo_get_env(q1)) #&gt; [1] 2 4 6 8 10 12 14 16 18 20 20.4.3 Application: subset() To see why the data mask is so important, lets implement our own version of subset(). If you haven’t used it before, subset() (like dplyr::filter()), provides a convenient way of selecting rows of a data frame using an expression that is evaluated in the context of the data frame. It allows you to subset without repeatedly referring to the name of the data frame: sample_df &lt;- data.frame(a = 1:5, b = 5:1, c = c(5, 3, 1, 4, 1)) # Shorthand for sample_df[sample_df$a &gt;= 4, ] subset(sample_df, a &gt;= 4) #&gt; a b c #&gt; 4 4 2 4 #&gt; 5 5 1 1 # Shorthand for sample_df[sample_df$b == sample_df$c, ] subset(sample_df, b == c) #&gt; a b c #&gt; 1 1 5 5 #&gt; 5 5 1 1 The core of subset2() is quite simple. It takes two arguments: a data frame, data, and a quoted expression, rows. We evaluate subset in using data as a data mask, then use the results to subset the data frame with [. I’ve included a very simple check to ensure the result is a logical vector; real code should do more work to create an informative error. subset2 &lt;- function(data, rows) { rows &lt;- enquo(rows) rows_val &lt;- eval_tidy(rows, data) stopifnot(is.logical(rows_val)) data[rows_val, , drop = FALSE] } subset(sample_df, b == c) #&gt; a b c #&gt; 1 1 5 5 #&gt; 5 5 1 1 20.4.4 Application: arrange() A slightly more complicated exercise is to implement a basic version of dplyr::arrange(). The goal of arrange() is to allow you to sort a data frame by multiple variables, each evaluated in the context of the data frame. arrange2 &lt;- function(data, ..., na.last = TRUE) { # Capture all dots args &lt;- enquos(...) # Create a call to order, using `!!!` to splice in the # individual expressions, and `!!` to splice in .na.last order_call &lt;- quo(order(!!!args, na.last = !!na.last)) # Evaluate the call to order with ord &lt;- eval_tidy(order_call, data) data[ord, , drop = FALSE] } df &lt;- data.frame(x = c(2, 3, 1), y = runif(3)) arrange2(df, x) #&gt; x y #&gt; 3 1 0.404 #&gt; 1 2 0.402 #&gt; 2 3 0.196 arrange2(df, -y) #&gt; x y #&gt; 3 1 0.404 #&gt; 1 2 0.402 #&gt; 2 3 0.196 Next we’ll talk a problem introduced by the data mask and how to fix it. Then we’ll come back to base::subset() and discuss why it’s documentation strongly advises against putting it in a function, and show how tidy evaluation overcomes each challenge. 20.4.5 Ambiguity and pronouns One of the downsides of the data mask is that it introduces ambiguity: when you say x, are you refering to a variable in the data or in the environment? This ambiguity is ok when doing interactive data analysis because you are familiar with the variables, and if there are problems, you spot them quickly because you are looking at the data frequently. However, ambiguity becomes a problem when you start programming with functions that use tidy evaluation. For example, take this simple wrapper: threshold_x &lt;- function(df, val) { subset2(df, x &gt;= val) } This function silently return an incorrect result in two ways: If df does not contain a variable called x, threshold_x() will silently return an incorrect result if x exists in the calling environment: x &lt;- 10 no_x &lt;- data.frame(y = 1:3) threshold_x(no_x, 2) #&gt; y #&gt; 1 1 #&gt; 2 2 #&gt; 3 3 If df contains a variable called val, the function will always return an incorrect answer: has_val &lt;- data.frame(x = 1:3, val = 9:11) threshold_x(has_val, 2) #&gt; [1] x val #&gt; &lt;0 rows&gt; (or 0-length row.names) These failure modes arise because tidy evaluation is ambiguous: each variable can be found in either the data mask or the environment. To make this function work we need to remove that ambiguity and ensure that x is always found in the data and val in the environment. To make this possible eval_tidy() provides the .data and .env pronouns: threshold_x &lt;- function(df, val) { subset2(df, .data$x &gt;= .env$val) } x &lt;- 10 threshold_x(no_x, 2) #&gt; Error: Column `x` not found in `.data` threshold_x(has_val, 2) #&gt; x val #&gt; 2 2 10 #&gt; 3 3 11 (NB: unlike indexing an ordinary list or environment with $, if the variable is not found then these pronouns will throw an error) Generally, whenever you use the .env pronoun, you can use unquoting instead: threshold_x &lt;- function(df, val) { subset2(df, .data$x &gt;= !!val) } There are subtle differences in when val is evaluated. If you unquote, val will be evaluated by enquo(); if you use a pronoun, val will be evaluated by eval_tidy(). These differences are usually unimportant, so pick the form that looks most natural. What if we generalise threshold_x() slightly so that the user can pick the variable used for thresholding. There are two basic approaches. Both start by capturing a symbol: threshold_var1 &lt;- function(df, var, val) { var &lt;- ensym(var) subset2(df, `$`(data, !!var) &gt;= !!val) } threshold_var2 &lt;- function(df, var, val) { var &lt;- as.character(ensym(var)) subset2(df, data[[!!var]] &gt;= !!val) } In threshold_var1 we need to use the prefix form of $, because df$!!var is not valid syntax. Alternatively, we can convert the symbol to a string, and use [[. Note that it is not always the responsibility of the function author to avoid ambiguity. Imagine we generalise further to allow thresholding based on any expression: threshold_expr &lt;- function(df, expr, val) { expr &lt;- enquo(expr) subset2(df, !!expr &gt;= !!val) } There’s no way to ensure that expr is only evaluated in the data, and indeed that would not be desirable because data will not include any functions (like + or &lt;). In this case, it is now the users responsibility to avoid ambiguity. As a function author it’s your responsibility to avoid ambiguity with an expressions that you create; it’s the users responsibility to avoid ambiguity in expressions that they create. 20.4.6 Base subset() The documentation of subset() includes the following warning: This is a convenience function intended for use interactively. For programming it is better to use the standard subsetting functions like [, and in particular the non-standard evaluation of argument subset can have unanticipated consequences. Why is subset() dangerous for programming and how does tidy evaluation help us avoid those dangers? First, lets implement the key parts of subset() following the same structure as subset2(). We convert enquo() to substitute() and eval_tidy() to eval(). We also need to supply a backup environment to eval(). There’s no way to access the environment associated with an argument in base R, so we take the best approximation: the caller environment (aka parent frame): subset_base &lt;- function(data, rows) { rows &lt;- substitute(rows) rows_val &lt;- eval(rows, data, caller_env()) stopifnot(is.logical(rows_val)) data[rows_val, , drop = FALSE] } There are three problems with this implementation: subset() doesn’t support unquoting, so wrapping the function is hard. First, you use substitute() to capture the complete expression, then you evaluate it. Because substitute() doesn’t use a syntactic marker for unquoting, it is hard to see exactly what’s happening here. f1a &lt;- function(df, expr) { eval(substitute(subset(df, expr)), caller_env()) } df &lt;- data.frame(x = 1:3, y = 3:1) f1a(df, x == 1) #&gt; x y #&gt; 1 1 3 I think the tidy evaluation equivalent is easier to understand because the quoting and unquoting is explicit: f1b &lt;- function(df, expr) { expr &lt;- enquo(expr) subset2(df, !!expr) } f1b(df, x == 1) #&gt; x y #&gt; 1 1 3 base::subset() always evaluates rows in the parent frame, but if ... has been used, then the expression might need to be evaluated elsewhere: f &lt;- function(df, ...) { xval &lt;- 3 subset(df, ...) } xval &lt;- 1 f(df, x == xval) #&gt; x y #&gt; 3 3 1 Because enquo() captures the environment of the argument as well as its expression, this is not a problem with subset2(): f &lt;- function(df, ...) { xval &lt;- 10 subset2(df, ...) } xval &lt;- 1 f(df, x == xval) #&gt; x y #&gt; 1 1 3 Finally, subset() doesn’t have any pronouns so there’s no way to write a safe version of threshold_x(). You might wonder if all this rigamorale is worth it when you can just use [. Firstly, it seems unappealing to have functions that can only be used safely in an interactive context. Then every interactive function needs to be paired with a programming function which behaves slightly differently. Secondly, even the simple subset() function, provides two useful features: It sets drop = FALSE by default, so it’s garuanteed to return a data frame It drops rows where the conditional evaluates to NA. That means subset(df, x == y) is not equivalent to df[x == y,] as you might naively expect. Instead, it is equivalent to df[x == y &amp; !is.na(x == y), , drop = FALSE]: that’s a lot more typing! 20.4.7 Performance Note that there some performance overhead when evaluating a quosure compared to evaluating an expression: n &lt;- 1000 x1 &lt;- expr(runif(n)) e1 &lt;- globalenv() q1 &lt;- quo(runif(n)) microbenchmark::microbenchmark( runif(n), eval_bare(x1, e1), eval_tidy(q1), eval_tidy(q1, mtcars) ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; runif(n) 21.5 21.9 23.1 22.3 22.7 35.0 100 a #&gt; eval_bare(x1, e1) 22.0 22.4 24.2 22.7 23.1 113.1 100 ab #&gt; eval_tidy(q1) 23.7 24.4 25.6 24.8 25.5 34.1 100 b #&gt; eval_tidy(q1, mtcars) 25.6 26.6 28.1 27.1 27.6 49.9 100 c However, most of the overhead is due to setting up the data mask so if you need to evaluate code repeatedly, it’s a good idea to the data mask once then reuse it: d_mtcars &lt;- as_data_mask(mtcars) microbenchmark::microbenchmark( as_data_mask(mtcars), eval_tidy(q1, mtcars), eval_tidy(q1, d_mtcars) ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; as_data_mask(mtcars) 3.08 3.68 4.36 4.07 4.42 28.0 100 a #&gt; eval_tidy(q1, mtcars) 25.22 26.22 26.74 26.69 27.07 29.6 100 c #&gt; eval_tidy(q1, d_mtcars) 22.44 23.02 23.47 23.32 23.68 37.0 100 b (The amount of savings is surprising because eval_tidy() also calls data_mask_clean(). Currently discussing if that should be the default at https://github.com/tidyverse/rlang/issues/372) 20.4.8 Exercises Improve subset2() to make it more like base::subset(): Drop rows where subset evaluates to NA. Give a clear error message if subset doesn’t yield a logical vector. What happens if subset doesn’t yield a logical vector with length equal to the number of rows in data? What do you think should happen? Here’s an alternative implementation of arrange(): invoke &lt;- function(fun, ...) do.call(fun, dots_list(...)) arrange3 &lt;- function(.data, ..., .na.last = TRUE) { args &lt;- enquos(...) ords &lt;- purrr::map(args, eval_tidy, data = .data) ord &lt;- invoke(order, !!!ords, na.last = .na.last) .data[ord, , drop = FALSE] } Describe the primary difference in approach compared to the function defined in the text. One advantage of this approach is that you could check each element of ... to make sure that input is correct. What property should each element of ords have? Here’s an alternative implementation of subset2(): subset3 &lt;- function(data, rows) { eval_tidy(quo(data[!!enquo(rows), , drop = FALSE])) } Rewrite the function to improve clarity then explain how this approach differs to the approach in the text. Implement a form of arrange() where you can request a variable to sorted in descending order using named arguments: arrange(mtcars, cyl, desc = mpg, vs) (Hint: The descreasing argument to order() will not help you. Instead, look at the definition of dplyr::desc(), and read the help for xtfrm().) Why do you not need to worry about ambiguity in arrange()? What does transform() do? Read the documentation. How does it work? Read the source code for transform.data.frame(). What does substitute(list(...)) do? Use tidy evaluation to implement your own version of transform(). Extend it so that a calculation can refer to variables created by transform, i.e. make this work: df &lt;- data.frame(x = 1:3) transform(df, x1 = x + 1, x2 = x1 + 1) #&gt; Error in x1 + 1: non-numeric argument to binary operator What does with() do? How does it work? Read the source code for with.default(). What does within() do? How does it work? Read the source code for within.data.frame(). Why is the code so much more complex than with()? Implement a version of within.data.frame() that uses tidy evaluation. Read the documentation and make sure that you understand what within() does, then read the source code. 20.5 Case study: calling base NSE functions To finish up this chapter we’re going to show how to wrap base NSE functions. We’ll focus on wrapping models because this is a common need, and illustrates the spectrum of challenges you’ll need to overcome for another base funtion. Unfortunately it’s not possible to use tidy evaluation in our wrappers, because the semantics of NSE functions are not quite rich enough. This means that the wrappers we will create can not in turn be easily wrapped. This makes them useful for reducing duplication in your analysis code, but not suitable for inclusion in a package. 20.5.1 Basics Let’s start with a very simple wrapper around lm(): lm2 &lt;- function(formula, data) { lm(formula, data) } This wrapper works, but is supoptimal because lm() captures its call, and displays it when printing: lm2(mpg ~ disp, mtcars) #&gt; #&gt; Call: #&gt; lm(formula = formula, data = data) #&gt; #&gt; Coefficients: #&gt; (Intercept) disp #&gt; 29.5999 -0.0412 This is important because this call is the chief way that you see the model specification when printing the model. To overcome this problem, we need to capture the arguments, create the call to lm() using unquoting, then evaluate that call: lm3 &lt;- function(formula, data) { formula &lt;- enexpr(formula) data &lt;- enexpr(data) lm_call &lt;- expr(lm(!!formula, data = !!data)) eval_bare(lm_call, caller_env()) } lm3(mpg ~ disp, mtcars)$call #&gt; lm(formula = mpg ~ disp, data = mtcars) Note that we manually supply an evaluation environment, caller_env(). We’ll discuss that in more detail shortly. Note that this technique works for all the arguments, even those that use NSE, like subset(): lm4 &lt;- function(formula, data, subset = NULL) { formula &lt;- enexpr(formula) data &lt;- enexpr(data) subset &lt;- enexpr(subset) lm_call &lt;- expr(lm(!!formula, data = !!data, subset = !!subset)) eval_bare(lm_call, caller_env()) } coef(lm4(mpg ~ disp, mtcars)) #&gt; (Intercept) disp #&gt; 29.5999 -0.0412 coef(lm4(mpg ~ disp, mtcars, subset = cyl == 4)) #&gt; (Intercept) disp #&gt; 40.872 -0.135 Note that I’ve supplied a default argument to subset. I think this is good practice because it clearly indicates that subset is optional: arguments with no default are ususally required. NULL has two nice properties here: lm() already knows how to handle subset = NULL: it treats it the same way as a missing subset. expr(NULL) is NULL; which makes it easier to detect progammatically. However, the current approach has one small downside: subset = NULL is shown in the call. lm4(mpg ~ disp, mtcars)$call #&gt; lm(formula = mpg ~ disp, data = mtcars, subset = NULL) It’s possible, if a little more work, to generate a call where subset is simply absent. There are two tricks needed to do this: We use the %||% helper to replace a NULL subset with missing_arg(). We use maybe_missing() in expr(): if we don’t do that the essential weirdness of the missing argument crops up and generates an error. This leads to lm5(): lm5 &lt;- function(formula, data, subset = NULL) { formula &lt;- enexpr(formula) data &lt;- enexpr(data) subset &lt;- enexpr(subset) %||% missing_arg() lm_call &lt;- expr(lm(!!formula, data = !!data, subset = !!maybe_missing(subset))) eval_bare(lm_call, caller_env()) } lm5(mpg ~ disp, mtcars)$call #&gt; lm(formula = mpg ~ disp, data = mtcars) Note that all these wrappers have one small advantage over lm(): we can use unquoting. f &lt;- mpg ~ disp lm5(!!f, mtcars)$call #&gt; lm(formula = mpg ~ disp, data = mtcars) resp &lt;- expr(mpg) lm5(!!resp ~ disp, mtcars)$call #&gt; lm(formula = mpg ~ disp, data = mtcars) 20.5.2 The evaluation environment What if you want to mingle object supplied by the user with objects that you create in the function? For example, imagine you want to make an auto-boostrapping version of lm(). You might write it like this: boot_lm0 &lt;- function(formula, data) { formula &lt;- enexpr(formula) boot_data &lt;- data[sample(nrow(data), replace = TRUE), , drop = FALSE] lm_call &lt;- expr(lm(!!formula, data = boot_data)) eval_bare(lm_call, caller_env()) } df &lt;- data.frame(x = 1:10, y = 5 + 3 * (1:10) + rnorm(10)) boot_lm0(y ~ x, data = df) #&gt; Error in is.data.frame(data): object &#39;boot_data&#39; not found Why doesn’t this code work? It’s because we’re evaluating lm_call in the caller environment, but boot_data exists in the execution environment. We could instead evaluate in the execution environment of boot_lm0(), but there’s no guarantee that formula could be evaluated in that environment. There are two basic way to overcome this challenge: Unquote the data frame into the call. This means that no look up has to occur, but has all the problems of inlining expressions. For modelling functions this means that captured call is suboptimal: boot_lm1 &lt;- function(formula, data) { formula &lt;- enexpr(formula) boot_data &lt;- data[sample(nrow(data), replace = TRUE), , drop = FALSE] lm_call &lt;- expr(lm(!!formula, data = !!boot_data)) eval_bare(lm_call, caller_env()) } boot_lm1(y ~ x, data = df) #&gt; #&gt; Call: #&gt; lm(formula = y ~ x, data = structure(list(x = c(2L, 2L, 4L, 5L, #&gt; 5L, 2L, 3L, 8L, 7L, 7L), y = c(9.6955738174265, 9.6955738174265, #&gt; 14.7455601602445, 21.1046762836731, 21.1046762836731, 9.6955738174265, #&gt; 11.9277429020827, 29.3929136850427, 26.5771509796652, 26.5771509796652 #&gt; )), row.names = c(&quot;2&quot;, &quot;2.1&quot;, &quot;4&quot;, &quot;5&quot;, &quot;5.1&quot;, &quot;2.2&quot;, &quot;3&quot;, &quot;8&quot;, #&gt; &quot;7&quot;, &quot;7.1&quot;), class = &quot;data.frame&quot;)) #&gt; #&gt; Coefficients: #&gt; (Intercept) x #&gt; 2.68 3.42 Alternatively you can create a new environment that inherits from the caller, and you can bind variables that you’ve created inside the function to that environment. boot_lm2 &lt;- function(formula, data) { formula &lt;- enexpr(formula) boot_data &lt;- data[sample(nrow(data), replace = TRUE), , drop = FALSE] lm_env &lt;- child_env(caller_env(), boot_data = boot_data) lm_call &lt;- expr(lm(!!formula, data = boot_data)) eval_bare(lm_call, lm_env) } boot_lm2(y ~ x, data = df) #&gt; #&gt; Call: #&gt; lm(formula = y ~ x, data = boot_data) #&gt; #&gt; Coefficients: #&gt; (Intercept) x #&gt; 4.19 3.07 20.5.3 Making formulas One final aspect to wrapping modelling functions is generating formulas. You just need to learn about one small wrinkle and then you can use the techniques you learned in Quotation. Formulas they print the same when evaluated and unevaluated: y ~ x #&gt; y ~ x expr(y ~ x) #&gt; y ~ x Instead, check the class to make sure you have an actual formula: class(y ~ x) #&gt; [1] &quot;formula&quot; class(expr(y ~ x)) #&gt; [1] &quot;call&quot; class(eval_bare(expr(y ~ x))) #&gt; [1] &quot;formula&quot; Once you understand this, you can generate formulas with unquoting and reduce(). Just remember to evaluate the result before returning it. Like in another base NSE wrapper, you should use caller_env() as the evaluation environment. Here’s a simple example that generates a formula by combining a response variable with a set of predictors. build_formula &lt;- function(resp, ...) { resp &lt;- enexpr(resp) preds &lt;- enexprs(...) pred_sum &lt;- purrr::reduce(preds, ~ expr(!!.x + !!.y)) eval_bare(expr(!!resp ~ !!pred_sum), caller_env()) } build_formula(y, a, b, c) #&gt; y ~ a + b + c 20.5.4 Exercises When model building, typically the predictor and data are relatively constant while you rapidly experiment with different predictors. Write a small wrapper that allows you to reduce duplication in this situation. pred_mpg &lt;- function(resp, ...) { } pred_mpg(~ disp) pred_mpg(~ I(1 / disp)) pred_mpg(~ disp * cyl) Another way to way to write boot_lm() would be to include the boostrapping expression (data[sample(nrow(data), replace = TRUE), , drop = FALSE]) in to the data argument. Implement that approach. What are the advantages? What are the disadvantages? We could capture quosures, and then extract the environment from them. There are multiple environments associated with a quosore, but eval_bare() can only use one. Write a function that takes a list of quosures and returns the common environment, if they have one, or otherwise throws an error. Write a function that takes a data frame and a list of formulas, fitting a linear model with each formula, generating a useful model call. Create a formula generation function that allows you to optionally supply a transformation function (e.g. log()) to the response or the predictors. "],
["translation.html", "21 Translating R code 21.1 HTML 21.2 LaTeX", " 21 Translating R code The combination of first class environments, lexical scoping, and metaprogramming gives us a powerful toolkit for translating R code in to other languages. One fully-fledged example of this idea is dbplyr. dbplyr powers the database backends for dplyr, allowing to express data maniplation in R and automatically translating it in to SQL. An important part of dbplyr is translate_sql() which turns vector R code in to the equivalent SQL: library(dbplyr) translate_sql(x ^ 2) #&gt; &lt;SQL&gt; POWER(&quot;x&quot;, 2.0) translate_sql(x &lt; 5 &amp; !is.na(x)) #&gt; &lt;SQL&gt; &quot;x&quot; &lt; 5.0 AND NOT(((&quot;x&quot;) IS NULL)) translate_sql(!first %in% c(&quot;John&quot;, &quot;Roger&quot;, &quot;Robert&quot;)) #&gt; &lt;SQL&gt; NOT(&quot;first&quot; IN (&#39;John&#39;, &#39;Roger&#39;, &#39;Robert&#39;)) translate_sql(select == 7) #&gt; &lt;SQL&gt; &quot;select&quot; = 7.0 This chapter will develop two simple, but useful DSLs: one to generate HTML, and the other to turn mathematical expressions from R code into LaTeX. This chapter together pulls together many techniques discussed elsewhere in the book. In particular, you’ll need to understand environments, metaprogramming, and a little functional programming and S3. We’ll use rlang for its metaprogramming tools, and purrr for its mapping functions library(rlang) library(purrr) #&gt; #&gt; Attaching package: &#39;purrr&#39; #&gt; The following objects are masked from &#39;package:rlang&#39;: #&gt; #&gt; %@%, %||%, as_function, flatten, flatten_chr, flatten_dbl, #&gt; flatten_int, flatten_lgl, invoke, list_along, modify, prepend, #&gt; rep_along, splice 21.1 HTML HTML (hypertext markup language) is the language that underlies the majority of the web. It’s a special case of SGML (standard generalised markup language), and it’s similar but not identical to XML (extensible markup language). HTML looks like this: &lt;body&gt; &lt;h1 id=&#39;first&#39;&gt;A heading&lt;/h1&gt; &lt;p&gt;Some text &amp;amp; &lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt; &lt;img src=&#39;myimg.png&#39; width=&#39;100&#39; height=&#39;100&#39; /&gt; &lt;/body&gt; Even if you’ve never looked at HTML before, you can still see that the key component of its coding structure is tags: &lt;tag&gt;&lt;/tag&gt;. Tags can be nested within other tags and intermingled with text. There are over 100 HTML tags, but in this chapter we’ll focus on just a handful: &lt;body&gt; is the top-level tag that contains all content. &lt;h1&gt; defines a top level heading. &lt;p&gt; defines a paragraph. &lt;b&gt; emboldens text. &lt;img&gt; embeds an image. Tags can also have named attributes which look like &lt;tag name1='value1' name2='value2'&gt;&lt;/tag&gt;. Two important attributes used with just about every tag are id and class. These are used in conjunction with CSS (cascading style sheets) in order to control the visual appearance of the page. Void tags, like &lt;img&gt;, don’t have any content, are written &lt;img /&gt;, not &lt;img&gt;&lt;/img&gt;. Since they have no content, attributes are more important, and img has three that are used with almost every image: src (where the image lives), width, and height. Because &lt; and &gt; have special meanings in HTML, you can’t write them directly. Instead you have to use the HTML escapes: &amp;gt; and &amp;lt;. And, since those escapes use &amp;, if you want a literal ampersand you have to escape it with &amp;amp;. 21.1.1 Goal Our goal is to make it easy to generate HTML from R. To give a concrete example, we want to generate the following HTML: &lt;body&gt; &lt;h1 id=&#39;first&#39;&gt;A heading&lt;/h1&gt; &lt;p&gt;Some text &amp;amp; &lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt; &lt;img src=&#39;myimg.png&#39; width=&#39;100&#39; height=&#39;100&#39; /&gt; &lt;/body&gt; And we want the structure of the R code to match the structure of the HTML as closely as possible. To that end, we will work our way up to the following DSL: with_html( body( h1(&quot;A heading&quot;, id = &quot;first&quot;), p(&quot;Some text &amp;&quot;, b(&quot;some bold text.&quot;)), img(src = &quot;myimg.png&quot;, width = 100, height = 100) ) ) This DSL has the following the properties: The nesting of function calls matches the nesting of tags. Unnamed arguments become the content of the tag, and named arguments become their attributes. We can automatically escape &amp; and other special characters because tags and text are clearly distinct. 21.1.2 Escaping Escaping is so fundamental to tranlsation that it’ll be our first topic. There are two related challenges: In user input, we need to automatically escape &amp;, &lt; and &gt;. At the same time we need to make sure that the &amp;, &lt; and &gt; we generate are not double-escaped (i.e. to &amp;amp;amp;, &amp;amp;lt; and `&amp;gt;). The easiest way to do this is to create an S3 class that distinguishes between regular text (that needs escaping) and HTML (that doesn’t). html &lt;- function(x) structure(x, class = &quot;advr_html&quot;) cat_line &lt;- function(...) cat(..., &quot;\\n&quot;, sep = &quot;&quot;) print.advr_html &lt;- function(x, ...) { out &lt;- paste0(&quot;&lt;HTML&gt; &quot;, x) cat_line(paste(strwrap(out), collapse = &quot;\\n&quot;)) } We then write an escape method. It has two important methods: escape.character() takes a regular character vector and returns an HTML vector with special characters (&amp;, &lt;, &gt;) escaped. escape.html() which leaves already escaped HTML as is. escape &lt;- function(x) UseMethod(&quot;escape&quot;) escape.character &lt;- function(x) { x &lt;- gsub(&quot;&amp;&quot;, &quot;&amp;amp;&quot;, x) x &lt;- gsub(&quot;&lt;&quot;, &quot;&amp;lt;&quot;, x) x &lt;- gsub(&quot;&gt;&quot;, &quot;&amp;gt;&quot;, x) html(x) } escape.advr_html &lt;- function(x) x Now we check that it works escape(&quot;This is some text.&quot;) #&gt; &lt;HTML&gt; This is some text. escape(&quot;x &gt; 1 &amp; y &lt; 2&quot;) #&gt; &lt;HTML&gt; x &amp;gt; 1 &amp;amp; y &amp;lt; 2 # Double escaping is not a problem escape(escape(&quot;This is some text. 1 &gt; 2&quot;)) #&gt; &lt;HTML&gt; This is some text. 1 &amp;gt; 2 # And text we know is HTML doesn&#39;t get escaped. escape(html(&quot;&lt;hr /&gt;&quot;)) #&gt; &lt;HTML&gt; &lt;hr /&gt; Conveniently this also gives the user a way to opt-out of our escaping if they know the content is already escaped. 21.1.3 Basic tag functions Next, we’ll write a few simple tag functions then figure out how to generalise this function to cover all possible tags. Let’s start with &lt;p&gt;. HTML tags can have both attributes (e.g., id or class) and children (like &lt;b&gt; or &lt;i&gt;). We need some way of separating these in the function call. Given that attributes are named values and children don’t have names, it seems natural to separate using named arguments from unnamed ones. For example, a call to p() might look like: p(&quot;Some text. &quot;, b(i(&quot;some bold italic text&quot;)), class = &quot;mypara&quot;) We could list all the possible attributes of the &lt;p&gt; tag in the function definition. But that’s hard not only because there are many attributes, but also because it’s possible to use custom attributes. Instead, we’ll just use ... and separate the components based on whether or not they are named. With this in mind, we create a helper function that wraps around rlang::dots_list() (so we can use !! and !!!) and returns named and unnamed components separately: dots_partition &lt;- function(...) { dots &lt;- dots_list(...) is_named &lt;- names(dots) != &quot;&quot; list( named = dots[is_named], unnamed = dots[!is_named] ) } str(dots_partition(a = 1, 2, b = 3, 4)) #&gt; List of 2 #&gt; $ named :List of 2 #&gt; ..$ a: num 1 #&gt; ..$ b: num 3 #&gt; $ unnamed:List of 2 #&gt; ..$ : num 2 #&gt; ..$ : num 4 We can now create our p() function. Notice that there’s one new function here: html_attributes(). It takes a named list and returns the HTML attibute specification as a string. It’s a little complicated (in part, because it deals with some idiosyncracies of HTML that I haven’t mentioned.), but it’s not that important and doesn’t introduce any programming new ideas, so I won’t discuss it here (you can find the source online). source(&quot;dsl-html-attributes.r&quot;, local = TRUE) p &lt;- function(...) { dots &lt;- dots_partition(...) attribs &lt;- html_attributes(dots$named) children &lt;- map_chr(dots$unnamed, escape) html(paste0( &quot;&lt;p&quot;, attribs, &quot;&gt;&quot;, paste(children, collapse = &quot;&quot;), &quot;&lt;/p&gt;&quot; )) } p(&quot;Some text&quot;) #&gt; &lt;HTML&gt; &lt;p&gt;Some text&lt;/p&gt; p(&quot;Some text&quot;, id = &quot;myid&quot;) #&gt; &lt;HTML&gt; &lt;p id=&#39;myid&#39;&gt;Some text&lt;/p&gt; p(&quot;Some text&quot;, class = &quot;important&quot;, `data-value` = 10) #&gt; &lt;HTML&gt; &lt;p class=&#39;important&#39; data-value=&#39;10&#39;&gt;Some text&lt;/p&gt; 21.1.4 Tag functions It’s straightforward to adapt p() to other tags: we just need to replace &quot;p&quot; with the name of the tag. One elegant way to do that is to manually create a function with rlang::new_function(), using unquoting with paste0() to generate the starting and ending tags. tag &lt;- function(tag) { new_function( exprs(... = ), expr({ dots &lt;- dots_partition(...) attribs &lt;- html_attributes(dots$named) children &lt;- map_chr(dots$unnamed, escape) html(paste0( !!paste0(&quot;&lt;&quot;, tag), attribs, &quot;&gt;&quot;, paste(children, collapse = &quot;&quot;), !!paste0(&quot;&lt;/&quot;, tag, &quot;&gt;&quot;) )) }), caller_env() ) } tag(&quot;b&quot;) #&gt; function (...) #&gt; { #&gt; dots &lt;- dots_partition(...) #&gt; attribs &lt;- html_attributes(dots$named) #&gt; children &lt;- map_chr(dots$unnamed, escape) #&gt; html(paste0(&quot;&lt;b&quot;, attribs, &quot;&gt;&quot;, paste(children, collapse = &quot;&quot;), #&gt; &quot;&lt;/b&gt;&quot;)) #&gt; } Now we can run our earlier example: p &lt;- tag(&quot;p&quot;) b &lt;- tag(&quot;b&quot;) i &lt;- tag(&quot;i&quot;) p(&quot;Some text. &quot;, b(i(&quot;some bold italic text&quot;)), class = &quot;mypara&quot;) #&gt; &lt;HTML&gt; &lt;p class=&#39;mypara&#39;&gt;Some text. &lt;b&gt;&lt;i&gt;some bold italic #&gt; text&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; Before we generate functions for every possible HTML tag, we need to create a variant of tag() for void tags. It’s very similar to tag(), but it will throw an error if there are any unnamed tags, and the tag itself looks a little different. void_tag &lt;- function(tag) { new_function( exprs(... = ), expr({ dots &lt;- dots_partition(...) if (length(dots$unnamed) &gt; 0) { stop(!!paste0(&quot;&lt;&quot;, tag, &quot;&gt; must not have unnamed arguments&quot;), call. = FALSE) } attribs &lt;- html_attributes(dots$named) html(paste0(!!paste0(&quot;&lt;&quot;, tag), attribs, &quot; /&gt;&quot;)) }), caller_env() ) } img &lt;- void_tag(&quot;img&quot;) img(src = &quot;myimage.png&quot;, width = 100, height = 100) #&gt; &lt;HTML&gt; &lt;img src=&#39;myimage.png&#39; width=&#39;100&#39; height=&#39;100&#39; /&gt; 21.1.5 Processing all tags Next we need a list of all the HTML tags: tags &lt;- c(&quot;a&quot;, &quot;abbr&quot;, &quot;address&quot;, &quot;article&quot;, &quot;aside&quot;, &quot;audio&quot;, &quot;b&quot;,&quot;bdi&quot;, &quot;bdo&quot;, &quot;blockquote&quot;, &quot;body&quot;, &quot;button&quot;, &quot;canvas&quot;, &quot;caption&quot;,&quot;cite&quot;, &quot;code&quot;, &quot;colgroup&quot;, &quot;data&quot;, &quot;datalist&quot;, &quot;dd&quot;, &quot;del&quot;,&quot;details&quot;, &quot;dfn&quot;, &quot;div&quot;, &quot;dl&quot;, &quot;dt&quot;, &quot;em&quot;, &quot;eventsource&quot;,&quot;fieldset&quot;, &quot;figcaption&quot;, &quot;figure&quot;, &quot;footer&quot;, &quot;form&quot;, &quot;h1&quot;, &quot;h2&quot;, &quot;h3&quot;, &quot;h4&quot;, &quot;h5&quot;, &quot;h6&quot;, &quot;head&quot;, &quot;header&quot;, &quot;hgroup&quot;, &quot;html&quot;, &quot;i&quot;,&quot;iframe&quot;, &quot;ins&quot;, &quot;kbd&quot;, &quot;label&quot;, &quot;legend&quot;, &quot;li&quot;, &quot;mark&quot;, &quot;map&quot;,&quot;menu&quot;, &quot;meter&quot;, &quot;nav&quot;, &quot;noscript&quot;, &quot;object&quot;, &quot;ol&quot;, &quot;optgroup&quot;, &quot;option&quot;, &quot;output&quot;, &quot;p&quot;, &quot;pre&quot;, &quot;progress&quot;, &quot;q&quot;, &quot;ruby&quot;, &quot;rp&quot;,&quot;rt&quot;, &quot;s&quot;, &quot;samp&quot;, &quot;script&quot;, &quot;section&quot;, &quot;select&quot;, &quot;small&quot;, &quot;span&quot;, &quot;strong&quot;, &quot;style&quot;, &quot;sub&quot;, &quot;summary&quot;, &quot;sup&quot;, &quot;table&quot;, &quot;tbody&quot;, &quot;td&quot;, &quot;textarea&quot;, &quot;tfoot&quot;, &quot;th&quot;, &quot;thead&quot;, &quot;time&quot;, &quot;title&quot;, &quot;tr&quot;, &quot;u&quot;, &quot;ul&quot;, &quot;var&quot;, &quot;video&quot;) void_tags &lt;- c(&quot;area&quot;, &quot;base&quot;, &quot;br&quot;, &quot;col&quot;, &quot;command&quot;, &quot;embed&quot;, &quot;hr&quot;, &quot;img&quot;, &quot;input&quot;, &quot;keygen&quot;, &quot;link&quot;, &quot;meta&quot;, &quot;param&quot;, &quot;source&quot;, &quot;track&quot;, &quot;wbr&quot;) If you look at this list carefully, you’ll see there are quite a few tags that have the same name as base R functions (body, col, q, source, sub, summary, table), and others that have the same name as popular packages (e.g., map). This means we don’t want to make all the functions available by default, in either the global environment or in a package. Instead, we’ll put them in a list and then provide a helper to make it easy to use them when desired. First, we make a named list: html_tags &lt;- c( tags %&gt;% set_names() %&gt;% map(tag), void_tags %&gt;% set_names() %&gt;% map(void_tag) ) This gives us an explicit (but verbose) way to call tag functions: html_tags$p( &quot;Some text. &quot;, html_tags$b(html_tags$i(&quot;some bold italic text&quot;)), class = &quot;mypara&quot; ) #&gt; &lt;HTML&gt; &lt;p class=&#39;mypara&#39;&gt;Some text. &lt;b&gt;&lt;i&gt;some bold italic #&gt; text&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; We can then finish off our HTML DSL with a function that allows us to evaluate code in the context of that list. Here we slightly abuse the data mask, passing it a list of functions rather than a data frame. This is quick hack to mingle the execution environment of code with the functions in html_tags. with_html &lt;- function(code) { code &lt;- enquo(code) eval_tidy(code, html_tags) } This gives us a succinct API which allows us to write HTML when we need it but doesn’t clutter up the namespace when we don’t. with_html( body( h1(&quot;A heading&quot;, id = &quot;first&quot;), p(&quot;Some text &amp;&quot;, b(&quot;some bold text.&quot;)), img(src = &quot;myimg.png&quot;, width = 100, height = 100) ) ) #&gt; &lt;HTML&gt; &lt;body&gt;&lt;h1 id=&#39;first&#39;&gt;A heading&lt;/h1&gt;&lt;p&gt;Some text #&gt; &amp;amp;&lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt;&lt;img src=&#39;myimg.png&#39; width=&#39;100&#39; #&gt; height=&#39;100&#39; /&gt;&lt;/body&gt; If you want to access the R function overridden by an HTML tag with the same name inside with_html(), you can use the full package::function specification. 21.1.6 Exercises The escaping rules for &lt;script&gt; and &lt;style&gt; tags are different: you don’t want to escape angle brackets or ampersands, but you do want to escape &lt;/script&gt; or &lt;/style&gt;. Adapt the code above to follow these rules. The use of ... for all functions has some big downsides. There’s no input validation and there will be little information in the documentation or autocomplete about how they are used in the function. Create a new function that, when given a named list of tags and their attribute names (like below), creates functions which address this problem. list( a = c(&quot;href&quot;), img = c(&quot;src&quot;, &quot;width&quot;, &quot;height&quot;) ) All tags should get class and id attributes. Currently the HTML doesn’t look terribly pretty, and it’s hard to see the structure. How could you adapt tag() to do indenting and formatting? Reason about the following code that calls with_html() referening objects from the environment. Will it work or fail? Why? Run the code to verify your predictions. greeting &lt;- &quot;Hello!&quot; with_html(p(greeting)) address &lt;- &quot;123 anywhere street&quot; with_html(p(address)) 21.2 LaTeX The next DSL will convert R expressions into their LaTeX math equivalents. (This is a bit like ?plotmath, but for text instead of plots.) LaTeX is the lingua franca of mathematicians and statisticians: it’s common to use LaTeX notation whenever you want to expression an equation in text (e.g., in an email). Since many reports are produced using both R and LaTeX, it might be useful to be able to automatically convert mathematical expressions from one language to the other. Because we need to convert both functions and names, this mathematical DSL will be more complicated than the HTML DSL. We’ll also need to create a “default” conversion, so that functions we don’t know about get a standard conversion. Like the HTML DSL, we’ll also use metaprogramming to make it easier to generate the translators. Can no longer just use eval: we also need to walk the tree. Ideally this would not be necessary. ObjectTables (see objectable package) almost make it possible to eliminate the tree walking but: They have currently have a big performance penalty There’s no way to distinguish symbols used in for function calls vs. other symbols. Before we begin, let’s quickly cover how formulas are expressed in LaTeX. 21.2.1 LaTeX mathematics The full spectrum of LaTeX mathematical notation is complex. Fortunately, they are well documented, and the most common commands have a fairly simple structure: Most simple mathematical equations are written in the same way you’d type them in R: x * y, z ^ 5. Subscripts are written using _ (e.g., x_1). Special characters start with a \\: \\pi = π, \\pm = ±, and so on. There are a huge number of symbols available in LaTeX. Googling for latex math symbols will return many lists. There’s even a service that will look up the symbol you sketch in the browser. More complicated functions look like \\name{arg1}{arg2}. For example, to write a fraction you’d use \\frac{a}{b}. To write a square root, you’d use \\sqrt{a}. To group elements together use {}: i.e., x ^ a + b vs. x ^ {a + b}. In good math typesetting, a distinction is made between variables and functions. But without extra information, LaTeX doesn’t know whether f(a * b) represents calling the function f with input a * b, or is shorthand for f * (a * b). If f is a function, you can tell LaTeX to typeset it using an upright font with \\textrm{f}(a * b). 21.2.2 Goal Our goal is to use these rules to automatically convert an R expression to its appropriate LaTeX representation. We’ll tackle this in four stages: Convert known symbols: pi -&gt; \\pi Leave other symbols unchanged: x -&gt; x, y -&gt; y Convert known functions to their special forms: sqrt(frac(a, b)) -&gt; \\sqrt{\\frac{a, b}} Wrap unknown functions with \\textrm: f(a) -&gt; \\textrm{f}(a) We’ll code this translation in the opposite direction of what we did with the HTML DSL. We’ll start with infrastructure, because that makes it easy to experiment with our DSL, and then work our way back down to generate the desired output. 21.2.3 to_math To begin, we need a wrapper function that will convert R expressions into LaTeX math expressions. This will work similarly to to_html(): capture the unevaluated expression and evaluate it in a special environment. Two main differences: Environment is no longer constant It will vary depending on the expression. We do this in order to be specially handle unknown symbols and functions Don’t use quosure. to_math &lt;- function(x) { expr &lt;- enexpr(x) out &lt;- eval_bare(expr, latex_env(expr)) latex(out) } latex &lt;- function(x) structure(x, class = &quot;advr_latex&quot;) print.advr_latex &lt;- function(x) { cat_line(&quot;&lt;LATEX&gt; &quot;, x) } 21.2.4 Known symbols Our first step is to create an environment that will convert the special LaTeX symbols used for Greek, e.g., pi to \\pi. We’ll use the same basic trick as used by subset to make it possible to select column ranges by name (subset(mtcars, , cyl:wt)): bind a name to a string in a special environment. We create that environment by naming a vector, converting the vector into a list, and converting the list into an environment. greek &lt;- c( &quot;alpha&quot;, &quot;theta&quot;, &quot;tau&quot;, &quot;beta&quot;, &quot;vartheta&quot;, &quot;pi&quot;, &quot;upsilon&quot;, &quot;gamma&quot;, &quot;varpi&quot;, &quot;phi&quot;, &quot;delta&quot;, &quot;kappa&quot;, &quot;rho&quot;, &quot;varphi&quot;, &quot;epsilon&quot;, &quot;lambda&quot;, &quot;varrho&quot;, &quot;chi&quot;, &quot;varepsilon&quot;, &quot;mu&quot;, &quot;sigma&quot;, &quot;psi&quot;, &quot;zeta&quot;, &quot;nu&quot;, &quot;varsigma&quot;, &quot;omega&quot;, &quot;eta&quot;, &quot;xi&quot;, &quot;Gamma&quot;, &quot;Lambda&quot;, &quot;Sigma&quot;, &quot;Psi&quot;, &quot;Delta&quot;, &quot;Xi&quot;, &quot;Upsilon&quot;, &quot;Omega&quot;, &quot;Theta&quot;, &quot;Pi&quot;, &quot;Phi&quot;) greek_list &lt;- set_names(paste0(&quot;\\\\&quot;, greek), greek) greek_env &lt;- as_env(greek_list) We can then check it: latex_env &lt;- function(expr) { greek_env } to_math(pi) #&gt; &lt;LATEX&gt; \\pi to_math(beta) #&gt; &lt;LATEX&gt; \\beta Looks good so far! 21.2.5 Unknown symbols If a symbol isn’t Greek, we want to leave it as is. This is tricky because we don’t know in advance what symbols will be used, and we can’t possibly generate them all. So we’ll use the approach described in walking the tree. The all_names function takes an expression and does the following: if it’s a name, it converts it to a string; if it’s a call, it recurses down through its arguments. all_names_rec &lt;- function(x) { switch_expr(x, constant = character(), symbol = as.character(x), pairlist = , call = flat_map_chr(as.list(x[-1]), all_names) ) } all_names &lt;- function(x) { unique(all_names_rec(x)) } all_names(expr(x + y + f(a, b, c, 10))) #&gt; [1] &quot;x&quot; &quot;y&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; We now want to take that list of symbols, and convert it to an environment so that each symbol is mapped to its corresponding string representation (e.g., so eval(quote(x), env) yields &quot;x&quot;). We again use the pattern of converting a named character vector to a list, then converting the list to an environment. latex_env &lt;- function(expr) { names &lt;- all_names(expr) symbol_env &lt;- as_env(set_names(names)) symbol_env } to_math(x) #&gt; &lt;LATEX&gt; x to_math(longvariablename) #&gt; &lt;LATEX&gt; longvariablename to_math(pi) #&gt; &lt;LATEX&gt; pi This works, but we need to combine it with the Greek symbols environment. Since we want to give preference to Greek over defaults (e.g., to_math(pi) should give &quot;\\\\pi&quot;, not &quot;pi&quot;), symbol_env needs to be the parent of greek_env. To do that, we need to make a copy of greek_env with a new parent. This gives us a function that can convert both known (Greek) and unknown symbols. latex_env &lt;- function(expr) { # Unknown symbols names &lt;- all_names(expr) symbol_env &lt;- as_env(set_names(names)) # Known symbols env_clone(greek_env, parent = symbol_env) } to_math(x) #&gt; &lt;LATEX&gt; x to_math(longvariablename) #&gt; &lt;LATEX&gt; longvariablename to_math(pi) #&gt; &lt;LATEX&gt; \\pi 21.2.6 Known functions Next we’ll add functions to our DSL. We’ll start with a couple of helper closures that make it easy to add new unary and binary operators. These functions are very simple: they only assemble strings. (Again we use force() to make sure the arguments are evaluated at the right time.) unary_op &lt;- function(left, right) { new_function( exprs(e1 = ), expr( paste0(!!left, e1, !!right) ), caller_env() ) } binary_op &lt;- function(sep) { new_function( exprs(e1 = , e2 = ), expr( paste0(e1, !!sep, e2) ), caller_env() ) } unary_op(&quot;\\\\sqrt{&quot;, &quot;}&quot;) #&gt; function (e1) #&gt; paste0(&quot;\\\\sqrt{&quot;, e1, &quot;}&quot;) binary_op(&quot;+&quot;) #&gt; function (e1, e2) #&gt; paste0(e1, &quot;+&quot;, e2) Using these helpers, we can map a few illustrative examples of converting R to LaTeX. Note that with R’s lexical scoping rules helping us, we can easily provide new meanings for standard functions like +, -, and *, and even ( and {. # Binary operators f_env &lt;- child_env( .parent = empty_env(), `+` = binary_op(&quot; + &quot;), `-` = binary_op(&quot; - &quot;), `*` = binary_op(&quot; * &quot;), `/` = binary_op(&quot; / &quot;), `^` = binary_op(&quot;^&quot;), `[` = binary_op(&quot;_&quot;), # Grouping `{` = unary_op(&quot;\\\\left{ &quot;, &quot; \\\\right}&quot;), `(` = unary_op(&quot;\\\\left( &quot;, &quot; \\\\right)&quot;), paste = paste, # Other math functions sqrt = unary_op(&quot;\\\\sqrt{&quot;, &quot;}&quot;), sin = unary_op(&quot;\\\\sin(&quot;, &quot;)&quot;), log = unary_op(&quot;\\\\log(&quot;, &quot;)&quot;), abs = unary_op(&quot;\\\\left| &quot;, &quot;\\\\right| &quot;), frac = function(a, b) { paste0(&quot;\\\\frac{&quot;, a, &quot;}{&quot;, b, &quot;}&quot;) }, # Labelling hat = unary_op(&quot;\\\\hat{&quot;, &quot;}&quot;), tilde = unary_op(&quot;\\\\tilde{&quot;, &quot;}&quot;) ) We again modify latex_env() to include this environment. It should be the last environment R looks for names in: in other words, sin(sin) should work. latex_env &lt;- function(expr) { # Known functions f_env # Default symbols names &lt;- all_names(expr) symbol_env &lt;- as_env(set_names(names), parent = f_env) # Known symbols greek_env &lt;- env_clone(greek_env, parent = symbol_env) } to_math(sin(x + pi)) #&gt; &lt;LATEX&gt; \\sin(x + \\pi) to_math(log(x_i ^ 2)) #&gt; &lt;LATEX&gt; \\log(x_i^2) to_math(sin(sin)) #&gt; &lt;LATEX&gt; \\sin(sin) 21.2.7 Unknown functions Finally, we’ll add a default for functions that we don’t yet know about. Like the unknown names, we can’t know in advance what these will be, so we again use a little metaprogramming to figure them out: all_calls_rec &lt;- function(x) { switch_expr(x, constant = , symbol = character(), call = { fname &lt;- as.character(x[[1]]) children &lt;- flat_map_chr(as.list(x[-1]), all_calls) c(fname, children) }, pairlist = flat_map_chr(as.list(x[1]), all_calls) ) } all_calls &lt;- function(x) { unique(all_calls_rec(x)) } all_calls(expr(f(g + b, c, d(a)))) #&gt; [1] &quot;f&quot; &quot;+&quot; &quot;d&quot; And we need a closure that will generate the functions for each unknown call. unknown_op &lt;- function(op) { new_function( exprs(... = ), expr({ contents &lt;- paste(..., collapse = &quot;, &quot;) paste0(!!paste0(&quot;\\\\mathrm{&quot;, op, &quot;}(&quot;), contents, &quot;)&quot;) }) ) } unknown_op(&quot;foo&quot;) #&gt; function (...) #&gt; { #&gt; contents &lt;- paste(..., collapse = &quot;, &quot;) #&gt; paste0(&quot;\\\\mathrm{foo}(&quot;, contents, &quot;)&quot;) #&gt; } #&gt; &lt;environment: 0x7fd0df367e28&gt; And again we update latex_env(): latex_env &lt;- function(expr) { calls &lt;- all_calls(expr) call_list &lt;- map(set_names(calls), unknown_op) call_env &lt;- as_environment(call_list) # Known functions f_env &lt;- env_clone(f_env, call_env) # Default symbols names &lt;- all_names(expr) symbol_env &lt;- as_env(set_names(names), parent = f_env) # Known symbols greek_env &lt;- env_clone(greek_env, parent = symbol_env) } to_math(f(a * b)) #&gt; &lt;LATEX&gt; \\mathrm{f}(a * b) 21.2.8 Exercises Add escaping. The special symbols that should be escaped by adding a backslash in front of them are \\, $, and %. Just as with HTML, you’ll need to make sure you don’t end up double-escaping. So you’ll need to create a small S3 class and then use that in function operators. That will also allow you to embed arbitrary LaTeX if needed. Complete the DSL to support all the functions that plotmath supports. "],
["performance.html", "22 Performance 22.1 Why is R slow? 22.2 Microbenchmarking 22.3 Language performance 22.4 Implementation performance 22.5 Alternative R implementations", " 22 Performance R is not a fast language. This is not an accident. R was purposely designed to make data analysis and statistics easier for you to do. It was not designed to make life easier for your computer. While R is slow compared to other programming languages, for most purposes, it’s fast enough. The goal of this part of the book is to give you a deeper understanding of R’s performance characteristics. In this chapter, you’ll learn about some of the trade-offs that R has made, valuing flexibility over performance. The following four chapters will give you the skills to improve the speed of your code when you need to: In Profiling, you’ll learn how to systematically make your code faster. First you figure what’s slow, and then you apply some general techniques to make the slow parts faster. In Memory, you’ll learn about how R uses memory, and how garbage collection and copy-on-modify affect performance and memory usage. For really high-performance code, you can move outside of R and use another programming language. Rcpp will teach you the absolute minimum you need to know about C++ so you can write fast code using the Rcpp package. To really understand the performance of built-in base functions, you’ll need to learn a little bit about R’s C API. In R’s C interface, you’ll learn a little about R’s C internals. Let’s get started by learning more about why R is slow. 22.1 Why is R slow? To understand R’s performance, it helps to think about R as both a language and as an implementation of that language. The R-language is abstract: it defines what R code means and how it should work. The implementation is concrete: it reads R code and computes a result. The most popular implementation is the one from r-project.org. I’ll call that implementation GNU-R to distinguish it from R-language, and from the other implementations I’ll discuss later in the chapter. The distinction between R-language and GNU-R is a bit murky because the R-language is not formally defined. While there is the R language definition, it is informal and incomplete. The R-language is mostly defined in terms of how GNU-R works. This is in contrast to other languages, like C++ and javascript, that make a clear distinction between language and implementation by laying out formal specifications that describe in minute detail how every aspect of the language should work. Nevertheless, the distinction between R-language and GNU-R is still useful: poor performance due to the language is hard to fix without breaking existing code; fixing poor performance due to the implementation is easier. In Language performance, I discuss some of the ways in which the design of the R-language imposes fundamental constraints on R’s speed. In Implementation performance, I discuss why GNU-R is currently far from the theoretical maximum, and why improvements in performance happen so slowly. While it’s hard to know exactly how much faster a better implementation could be, a &gt;10x improvement in speed seems achievable. In alternative implementations, I discuss some of the promising new implementations of R, and describe one important technique they use to make R code run faster. Beyond performance limitations due to design and implementation, it has to be said that a lot of R code is slow simply because it’s poorly written. Few R users have any formal training in programming or software development. Fewer still write R code for a living. Most people use R to understand data: it’s more important to get an answer quickly than to develop a system that will work in a wide variety of situations. This means that it’s relatively easy to make most R code much faster, as we’ll see in the following chapters. Before we examine some of the slower parts of the R-language and GNU-R, we need to learn a little about benchmarking so that we can give our intuitions about performance a concrete foundation. 22.2 Microbenchmarking A microbenchmark is a measurement of the performance of a very small piece of code, something that might take microseconds (µs) or nanoseconds (ns) to run. I’m going to use microbenchmarks to demonstrate the performance of very low-level pieces of R code, which help develop your intuition for how R works. This intuition, by-and-large, is not useful for increasing the speed of real code. The observed differences in microbenchmarks will typically be dominated by higher-order effects in real code; a deep understanding of subatomic physics is not very helpful when baking. Don’t change the way you code because of these microbenchmarks. Instead wait until you’ve read the practical advice in the following chapters. The best tool for microbenchmarking in R is the microbenchmark package. It provides very precise timings, making it possible to compare operations that only take a tiny amount of time. For example, the following code compares the speed of two ways of computing a square root. library(microbenchmark) x &lt;- runif(100) microbenchmark( sqrt(x), x ^ 0.5 ) #&gt; Unit: nanoseconds #&gt; expr min lq mean median uq max neval cld #&gt; sqrt(x) 461 478 551 491 557 2,070 100 a #&gt; x^0.5 2,210 2,240 2656 2,280 2,400 14,300 100 b By default, microbenchmark() runs each expression 100 times (controlled by the times parameter). In the process, it also randomises the order of the expressions. It summarises the results with a minimum (min), lower quartile (lq), median, upper quartile (uq), and maximum (max). Focus on the median, and use the upper and lower quartiles (lq and uq) to get a feel for the variability. In this example, you can see that using the special purpose sqrt() function is faster than the general exponentiation operator. As with all microbenchmarks, pay careful attention to the units: here, each computation takes about 600 ns, 600 billionths of a second. To help calibrate the impact of a microbenchmark on run time, it’s useful to think about how many times a function needs to run before it takes a second. If a microbenchmark takes: 1 ms, then one thousand calls takes a second 1 µs, then one million calls takes a second 1 ns, then one billion calls takes a second The sqrt() function takes about 600 ns, or 0.6 µs, to compute the square root of 100 numbers. That means if you repeated the operation a million times, it would take 0.6 s. So changing the way you compute the square root is unlikely to significantly affect real code. 22.2.1 Exercises Instead of using microbenchmark(), you could use the built-in function system.time(). But system.time() is much less precise, so you’ll need to repeat each operation many times with a loop, and then divide to find the average time of each operation, as in the code below. n &lt;- 1e6 system.time(for (i in 1:n) sqrt(x)) / n system.time(for (i in 1:n) x ^ 0.5) / n How do the estimates from system.time() compare to those from microbenchmark()? Why are they different? Here are two other ways to compute the square root of a vector. Which do you think will be fastest? Which will be slowest? Use microbenchmarking to test your answers. x ^ (1 / 2) exp(log(x) / 2) Use microbenchmarking to rank the basic arithmetic operators (+, -, *, /, and ^) in terms of their speed. Visualise the results. Compare the speed of arithmetic on integers vs. doubles. You can change the units in which the microbenchmark results are expressed with the unit parameter. Use unit = &quot;eps&quot; to show the number of evaluations needed to take 1 second. Repeat the benchmarks above with the eps unit. How does this change your intuition for performance? 22.3 Language performance In this section, I’ll explore three trade-offs that limit the performance of the R-language: extreme dynamism, name lookup with mutable environments, and lazy evaluation of function arguments. I’ll illustrate each trade-off with a microbenchmark, showing how it slows GNU-R down. I benchmark GNU-R because you can’t benchmark the R-language (it can’t run code). This means that the results are only suggestive of the cost of these design decisions, but are nevertheless useful. I’ve picked these three examples to illustrate some of the trade-offs that are key to language design: the designer must balance speed, flexibility, and ease of implementation. If you’d like to learn more about the performance characteristics of the R-language and how they affect real code, I highly recommend “Evaluating the Design of the R Language” by Floreal Morandat, Brandon Hill, Leo Osvald, and Jan Vitek. It uses a powerful methodology that combines a modified R interpreter and a wide set of code found in the wild. 22.3.1 Extreme dynamism R is an extremely dynamic programming language. Almost anything can be modified after it is created. To give just a few examples, you can: Change the body, arguments, and environment of functions. Change the S4 methods for a generic. Add new fields to an S3 object, or even change its class. Modify objects outside of the local environment with &lt;&lt;-. Pretty much the only things you can’t change are objects in sealed namespaces, which are created when you load a package. The advantage of dynamism is that you need minimal upfront planning. You can change your mind at any time, iterating your way to a solution without having to start afresh. The disadvantage of dynamism is that it’s difficult to predict exactly what will happen with a given function call. This is a problem because the easier it is to predict what’s going to happen, the easier it is for an interpreter or compiler to make an optimisation. (If you’d like more details, Charles Nutter expands on this idea at On Languages, VMs, Optimization, and the Way of the World.) If an interpreter can’t predict what’s going to happen, it has to consider many options before it finds the right one. For example, the following loop is slow in R, because R doesn’t know that x is always an integer. That means R has to look for the right + method (i.e., is it adding doubles, or integers?) in every iteration of the loop. x &lt;- 0L for (i in 1:1e6) { x &lt;- x + 1 } The cost of finding the right method is higher for non-primitive functions. The following microbenchmark illustrates the cost of method dispatch for S3, S4, and RC. I create a generic and a method for each OO system, then call the generic and see how long it takes to find and call the method. I also time how long it takes to call the bare function for comparison. f &lt;- function(x) NULL s3 &lt;- function(x) UseMethod(&quot;s3&quot;) s3.integer &lt;- f A &lt;- setClass(&quot;A&quot;, representation(a = &quot;list&quot;)) setGeneric(&quot;s4&quot;, function(x) standardGeneric(&quot;s4&quot;)) setMethod(s4, &quot;A&quot;, f) B &lt;- setRefClass(&quot;B&quot;, methods = list(rc = f)) a &lt;- A() b &lt;- B$new() microbenchmark( fun = f(), S3 = s3(1L), S4 = s4(a), RC = b$rc() ) #&gt; Unit: nanoseconds #&gt; expr min lq mean median uq max neval cld #&gt; fun 141 160 289 172 206 9,800 100 a #&gt; S3 705 860 6660 954 1,120 537,000 100 a #&gt; S4 8,180 10,100 21966 10,900 11,900 747,000 100 a #&gt; RC 5,570 6,200 30659 6,730 7,260 2,370,000 100 a The bare function takes about 200 ns. S3 method dispatch takes an additional 800 ns; S4 dispatch, 10,000 ns; and 7,000 dispatch, 10,000 ns. S3 and S4 method dispatch are expensive because R must search for the right method every time the generic is called; it might have changed between this call and the last. R could do better by caching methods between calls, but caching is hard to do correctly and a notorious source of bugs. 22.3.2 Name lookup with mutable environments It’s surprisingly difficult to find the value associated with a name in the R-language. This is due to combination of lexical scoping and extreme dynamism. Take the following example. Each time we print a it comes from a different environment: a &lt;- 1 f &lt;- function() { g &lt;- function() { print(a) assign(&quot;a&quot;, 2, envir = parent.frame()) print(a) a &lt;- 3 print(a) } g() } f() #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 This means that you can’t do name lookup just once: you have to start from scratch each time. This problem is exacerbated by the fact that almost every operation is a lexically scoped function call. You might think the following simple function calls two functions: + and ^. In fact, it calls four because { and ( are regular functions in R. f &lt;- function(x, y) { (x + y) ^ 2 } Since these functions are in the global environment, R has to look through every environment in the search path, which could easily be 10 or 20 environments. The following microbenchmark hints at the performance costs. We create four versions of f(), each with one more environment (containing 26 bindings) between the environment of f() and the base environment where +, ^, (, and { are defined. random_env &lt;- function(parent = globalenv()) { letter_list &lt;- setNames(as.list(runif(26)), LETTERS) list2env(letter_list, envir = new.env(parent = parent)) } set_env &lt;- function(f, e) { environment(f) &lt;- e f } f2 &lt;- set_env(f, random_env()) f3 &lt;- set_env(f, random_env(environment(f2))) f4 &lt;- set_env(f, random_env(environment(f3))) microbenchmark( f(1, 2), f2(1, 2), f3(1, 2), f4(1, 2), times = 10000 ) #&gt; Unit: nanoseconds #&gt; expr min lq mean median uq max neval cld #&gt; f(1, 2) 240 254 449 259 285 1,500,000 10000 a #&gt; f2(1, 2) 432 458 527 473 522 19,900 10000 a #&gt; f3(1, 2) 460 489 563 507 556 14,400 10000 a #&gt; f4(1, 2) 495 521 598 538 589 16,500 10000 a Each additional environment between f() and the base environment makes the function slower by about 30 ns. It might be possible to implement a caching system so that R only needs to look up the value of each name once. This is hard because there are so many ways to change the value associated with a name: &lt;&lt;-, assign(), eval(), and so on. Any caching system would have to know about these functions to make sure the cache was correctly invalidated and you didn’t get an out-of-date value. Another simple fix would be to add more built-in constants that you can’t override. This, for example, would mean that R always knew exactly what +, -, {, and ( meant, and you wouldn’t have to repeatedly look up their definitions. That would make the interpreter more complicated (because there are more special cases) and hence harder to maintain, and the language less flexible. This would change the R-language, but it would be unlikely to affect much existing code because it’s such a bad idea to override functions like { and (. 22.3.3 Lazy evaluation overhead In R, function arguments are evaluated lazily (as discussed in lazy evaluation and capturing expressions). To implement lazy evaluation, R uses a promise object that contains the expression needed to compute the result and the environment in which to perform the computation. Creating these objects has some overhead, so each additional argument to a function decreases its speed a little. The following microbenchmark compares the runtime of a very simple function. Each version of the function has one additional argument. This suggests that adding an additional argument slows the function down by ~20 ns. f0 &lt;- function() NULL f1 &lt;- function(a = 1) NULL f2 &lt;- function(a = 1, b = 1) NULL f3 &lt;- function(a = 1, b = 2, c = 3) NULL f4 &lt;- function(a = 1, b = 2, c = 4, d = 4) NULL f5 &lt;- function(a = 1, b = 2, c = 4, d = 4, e = 5) NULL microbenchmark(f0(), f1(), f2(), f3(), f4(), f5(), times = 10000) #&gt; Unit: nanoseconds #&gt; expr min lq mean median uq max neval cld #&gt; f0() 97 113 160 116 119 312,000 10000 a #&gt; f1() 111 129 203 131 135 529,000 10000 a #&gt; f2() 125 144 213 148 154 347,000 10000 a #&gt; f3() 141 161 244 166 181 350,000 10000 a #&gt; f4() 150 170 266 179 222 352,000 10000 a #&gt; f5() 168 185 293 193 258 374,000 10000 a In most other programming languages there is little overhead for adding extra arguments. Many compiled languages will even warn you if arguments are never used (like in the above example), and automatically remove them from the function. 22.3.4 Exercises scan() has the most arguments (21) of any base function. About how much time does it take to make 21 promises each time scan is called? Given a simple input (e.g., scan(text = &quot;1 2 3&quot;, quiet = T)) what proportion of the total run time is due to creating those promises? Read “Evaluating the Design of the R Language”. What other aspects of the R-language slow it down? Construct microbenchmarks to illustrate. How does the performance of S3 method dispatch change with the length of the class vector? How does performance of S4 method dispatch change with number of superclasses? How about RC? What is the cost of multiple inheritance and multiple dispatch on S4 method dispatch? Why is the cost of name lookup less for functions in the base package? 22.4 Implementation performance The design of the R language limits its maximum theoretical performance, but GNU-R is currently nowhere near that maximum. There are many things that can (and will) be done to improve performance. This section discusses some aspects of GNU-R that are slow not because of their definition, but because of their implementation. R is over 20 years old. It contains nearly 800,000 lines of code (about 45% C, 19% R, and 17% Fortran). Changes to base R can only be made by members of the R Core Team (or R-core for short). Currently R-core has twenty members, but only six are active in day-to-day development. No one on R-core works full time on R. Most are statistics professors who can only spend a relatively small amount of their time on R. Because of the care that must be taken to avoid breaking existing code, R-core tends to be very conservative about accepting new code. It can be frustrating to see R-core reject proposals that would improve performance. However, the overriding concern for R-core is not to make R fast, but to build a stable platform for data analysis and statistics. Below, I’ll show two small, but illustrative, examples of parts of R that are currently slow but could, with some effort, be made faster. They are not critical parts of base R, but they have been sources of frustration for me in the past. As with all microbenchmarks, these won’t affect the performance of most code, but can be important for special cases. 22.4.1 Extracting a single value from a data frame The following microbenchmark shows five ways to access a single value (the number in the bottom-right corner) from the built-in mtcars dataset. The variation in performance is startling: the slowest method takes 30x longer than the fastest. There’s no reason that there has to be such a huge difference in performance. It’s simply that no one has had the time to fix it. microbenchmark( &quot;[32, 11]&quot; = mtcars[32, 11], &quot;$carb[32]&quot; = mtcars$carb[32], &quot;[[c(11, 32)]]&quot; = mtcars[[c(11, 32)]], &quot;[[11]][32]&quot; = mtcars[[11]][32], &quot;.subset2&quot; = .subset2(mtcars, 11)[32] ) #&gt; Unit: nanoseconds #&gt; expr min lq mean median uq max neval cld #&gt; [32, 11] 7,080 7,570 8151 7,810 8,100 33,700 100 b #&gt; $carb[32] 3,430 4,250 7685 4,620 5,000 314,000 100 b #&gt; [[c(11, 32)]] 2,840 3,310 3801 3,770 4,110 7,400 100 ab #&gt; [[11]][32] 2,690 3,130 3538 3,520 3,930 4,590 100 ab #&gt; .subset2 187 230 303 246 308 2,500 100 a 22.4.2 ifelse(), pmin(), and pmax() Some base functions are known to be slow. For example, take the following three implementations of squish(), a function that ensures that the smallest value in a vector is at least a and its largest value is at most b. The first implementation, squish_ife(), uses ifelse(). ifelse() is known to be slow because it is relatively general and must evaluate all arguments fully. The second implementation, squish_p(), uses pmin() and pmax(). Because these two functions are so specialised, one might expect that they would be fast. However, they’re actually rather slow. This is because they can take any number of arguments and they have to do some relatively complicated checks to determine which method to use. The final implementation uses basic subassignment. squish_ife &lt;- function(x, a, b) { ifelse(x &lt;= a, a, ifelse(x &gt;= b, b, x)) } squish_p &lt;- function(x, a, b) { pmax(pmin(x, b), a) } squish_in_place &lt;- function(x, a, b) { x[x &lt;= a] &lt;- a x[x &gt;= b] &lt;- b x } x &lt;- runif(100, -1.5, 1.5) microbenchmark( squish_ife = squish_ife(x, -1, 1), squish_p = squish_p(x, -1, 1), squish_in_place = squish_in_place(x, -1, 1), unit = &quot;us&quot; ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; squish_ife 17.40 30.10 50.9 31.90 33.80 1,970 100 a #&gt; squish_p 8.09 11.00 28.2 12.20 13.90 1,100 100 a #&gt; squish_in_place 1.87 2.31 23.2 2.66 4.19 1,900 100 a Using pmin() and pmax() is about 3x faster than ifelse(), and using subsetting directly is about 5x as fast again. We can often do even better by using C++. The following example compares the best R implementation to a relatively simple, if verbose, implementation in C++. Even if you’ve never used C++, you should still be able to follow the basic strategy: loop over every element in the vector and perform a different action depending on whether or not the value is less than a and/or greater than b. #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector squish_cpp(NumericVector x, double a, double b) { int n = x.length(); NumericVector out(n); for (int i = 0; i &lt; n; ++i) { double xi = x[i]; if (xi &lt; a) { out[i] = a; } else if (xi &gt; b) { out[i] = b; } else { out[i] = xi; } } return out; } (You’ll learn how to access this C++ code from R in Rcpp.) microbenchmark( squish_in_place = squish_in_place(x, -1, 1), squish_cpp = squish_cpp(x, -1, 1), unit = &quot;us&quot; ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; squish_in_place 1.74 1.95 2.27 2.12 2.34 7.63 100 a #&gt; squish_cpp 1.50 1.72 13.19 1.88 2.14 1,100.00 100 a The C++ implementation is around 1x faster than the best pure R implementation. 22.4.3 Exercises The performance characteristics of squish_ife(), squish_p(), and squish_in_place() vary considerably with the size of x. Explore the differences. Which sizes lead to the biggest and smallest differences? Compare the performance costs of extracting an element from a list, a column from a matrix, and a column from a data frame. Do the same for rows. 22.5 Alternative R implementations There are some exciting new implementations of R. While they all try to stick as closely as possible to the existing language definition, they improve speed by using ideas from modern interpreter design. The four most mature open-source projects are: pqR (pretty quick R) by Radford Neal. Built on top of R 2.15.0, it fixes many obvious performance issues, and provides better memory management and some support for automatic multithreading. Renjin by BeDataDriven. Renjin uses the Java virtual machine, and has an extensive test suite. FastR by a team from Purdue. FastR is similar to Renjin, but it makes more ambitious optimisations and is somewhat less mature. Riposte by Justin Talbot and Zachary DeVito. Riposte is experimental and ambitious. For the parts of R it implements, it is extremely fast. Riposte is described in more detail in Riposte: A Trace-Driven Compiler and Parallel VM for Vector Code in R. These are roughly ordered from most practical to most ambitious. Another project, CXXR by Andrew Runnalls, does not provide any performance improvements. Instead, it aims to refactor R’s internal C code in order to build a stronger foundation for future development, to keep behaviour identical to GNU-R, and to create better, more extensible documentation of its internals. R is a huge language and it’s not clear whether any of these approaches will ever become mainstream. It’s a hard task to make an alternative implementation run all R code in the same way as GNU-R. Can you imagine having to reimplement every function in base R to be not only faster, but also to have exactly the same documented bugs? However, even if these implementations never make a dent in the use of GNU-R, they still provide benefits: Simpler implementations make it easy to validate new approaches before porting to GNU-R. Knowing which aspects of the language can be changed with minimal impact on existing code and maximal impact on performance can help to guide us to where we should direct our attention. Alternative implementations put pressure on the R-core to incorporate performance improvements. One of the most important approaches that pqR, Renjin, FastR, and Riposte are exploring is the idea of deferred evaluation. As Justin Talbot, the author of Riposte, points out: “for long vectors, R’s execution is completely memory bound. It spends almost all of its time reading and writing vector intermediates to memory”. If we could eliminate these intermediate vectors, we could improve performance and reduce memory usage. The following example shows a very simple example of how deferred evaluation can help. We have three vectors, x, y, z, each containing 1 million elements, and we want to find the sum of x + y where z is TRUE. (This represents a simplification of a pretty common sort of data analysis question.) x &lt;- runif(1e6) y &lt;- runif(1e6) z &lt;- sample(c(T, F), 1e6, rep = TRUE) sum((x + y)[z]) In R, this creates two big temporary vectors: x + y, 1 million elements long, and (x + y)[z], about 500,000 elements long. This means you need to have extra memory available for the intermediate calculation, and you have to shuttle the data back and forth between the CPU and memory. This slows computation down because the CPU can’t work at maximum efficiency if it’s always waiting for more data to come in. However, if we rewrote the function using a loop in a language like C++, we only need one intermediate value: the sum of all the values we’ve seen: #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] double cond_sum_cpp(NumericVector x, NumericVector y, LogicalVector z) { double sum = 0; int n = x.length(); for(int i = 0; i &lt; n; i++) { if (!z[i]) continue; sum += x[i] + y[i]; } return sum; } cond_sum_r &lt;- function(x, y, z) { sum((x + y)[z]) } microbenchmark( cond_sum_cpp = cond_sum_cpp(x, y, z), cond_sum_r = cond_sum_r(x, y, z), unit = &quot;ms&quot; ) #&gt; Unit: milliseconds #&gt; expr min lq mean median uq max neval cld #&gt; cond_sum_cpp 3.08 3.15 3.38 3.23 3.4 9.9 100 a #&gt; cond_sum_r 6.90 7.24 12.56 8.19 11.6 221.0 100 b On my computer, this approach is about 3x faster than the vectorised R equivalent, which is already pretty fast. The goal of deferred evaluation is to perform this transformation automatically, so you can write concise R code and have it automatically translated into efficient machine code. Sophisticated translators can also figure out how to make the most of multiple cores. In the above example, if you have four cores, you could split x, y, and z into four pieces performing the conditional sum on each core, then adding together the four individual results. Deferred evaluation can also work with for loops, automatically discovering operations that can be vectorised. This chapter has discussed some of the fundamental reasons that R is slow. The following chapters will give you the tools to do something about it when it impacts your code. "],
["profiling.html", "23 Optimising code 23.1 Measuring performance 23.2 Improving performance 23.3 Code organisation 23.4 Has someone already solved the problem? 23.5 Do as little as possible 23.6 Vectorise 23.7 Avoid copies 23.8 Byte code compilation 23.9 Case study: t-test 23.10 Parallelise 23.11 Other techniques", " 23 Optimising code “Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered.” — Donald Knuth. Optimising code to make it run faster is an iterative process: Find the biggest bottleneck (the slowest part of your code). Try to eliminate it (you may not succeed but that’s ok). Repeat until your code is “fast enough.” This sounds easy, but it’s not. Even experienced programmers have a hard time identifying bottlenecks in their code. Instead of relying on your intuition, you should profile your code: use realistic inputs and measure the run-time of each individual operation. Only once you’ve identified the most important bottlenecks can you attempt to eliminate them. It’s difficult to provide general advice on improving performance, but I try my best with six techniques that can be applied in many situations. I’ll also suggest a general strategy for performance optimisation that helps ensure that your faster code will still be correct code. It’s easy to get caught up in trying to remove all bottlenecks. Don’t! Your time is valuable and is better spent analysing your data, not eliminating possible inefficiencies in your code. Be pragmatic: don’t spend hours of your time to save seconds of computer time. To enforce this advice, you should set a goal time for your code and optimise only up to that goal. This means you will not eliminate all bottlenecks. Some you will not get to because you’ve met your goal. Others you may need to pass over and accept either because there is no quick and easy solution or because the code is already well optimised and no significant improvement is possible. Accept these possibilities and move on to the next candidate. 23.0.0.0.1 Outline Measuring performance describes how to find the bottlenecks in your code using line profiling. Improving performance outlines seven general strategies for improving the performance of your code. Code organisation teaches you how to organise your code to make optimisation as easy, and bug free, as possible. Already solved reminds you to look for existing solutions. Do as little as possible emphasises the importance of being lazy: often the easiest way to make a function faster is to let it to do less work. Vectorise concisely defines vectorisation, and shows you how to make the most of built-in functions. Avoid copies discusses the performance perils of copying data. Byte code compilation shows you how to take advantage of R’s byte code compiler. Case study: t-test pulls all the pieces together into a case study showing how to speed up repeated t-tests by ~1000x. Parallelise teaches you how to use parallelisation to spread computation across all the cores in your computer. Other techniques finishes the chapter with pointers to more resources that will help you write fast code. 23.0.0.0.2 Prerequisites In this chapter we’ll be using the lineprof package to understand the performance of R code. Get it with: devtools::install_github(&quot;hadley/lineprof&quot;) 23.1 Measuring performance To understand performance, you use a profiler. There are a number of different types of profilers. R uses a fairly simple type called a sampling or statistical profiler. A sampling profiler stops the execution of code every few milliseconds and records which function is currently executing (along with which function called that function, and so on). For example, consider f(), below: library(lineprof) f &lt;- function() { pause(0.1) g() h() } g &lt;- function() { pause(0.1) h() } h &lt;- function() { pause(0.1) } (I use lineprof::pause() instead of Sys.sleep() because Sys.sleep() does not appear in profiling outputs because as far as R can tell, it doesn’t use up any computing time.) If we profiled the execution of f(), stopping the execution of code every 0.1 s, we’d see a profile like below. Each line represents one “tick” of the profiler (0.1 s in this case), and function calls are nested with &gt;. It shows that the code spends 0.1 s running f(), then 0.2 s running g(), then 0.1 s running h(). f() f() &gt; g() f() &gt; g() &gt; h() f() &gt; h() If we actually profile f(), using the code below, we’re unlikely to get such a clear result. tmp &lt;- tempfile() Rprof(tmp, interval = 0.1) f() Rprof(NULL) That’s because profiling is hard to do accurately without slowing your code down by many orders of magnitude. The compromise that RProf() makes, sampling, only has minimal impact on the overall performance, but is fundamentally stochastic. There’s some variability in both the accuracy of the timer and in the time taken by each operation, so each time you profile you’ll get a slightly different answer. Fortunately, pinpoint accuracy is not needed to identify the slowest parts of your code. Rather than focussing on individual calls, we’ll visualise aggregates using the lineprof package. There are a number of other options, like summaryRprof(), the proftools package, and the profr package, but these tools are beyond the scope of this book. I wrote the lineprof package as a simpler way to visualise profiling data. As the name suggests, the fundamental unit of analysis in lineprof() is a line of code. This makes lineprof less precise than the alternatives (because a line of code can contain multiple function calls), but it’s easier to understand the context. To use lineprof, we first save the code in a file and source() it. Here profiling-example.R contains the definition of f(), g(), and h(). Note that you must use source() to load the code. This is because lineprof uses srcrefs to match up the code to the profile, and the needed srcrefs are only created when you load code from disk. We then use lineprof() to run our function and capture the timing output. Printing this object shows some basic information. For now, we’ll just focus on the time column which estimates how long each line took to run and the ref column which tells us which line of code was run. The estimates aren’t perfect, but the ratios look about right. library(lineprof) source(&quot;profiling-example.R&quot;) l &lt;- lineprof(f()) l #&gt; time alloc release dups ref src #&gt; 1 0.074 0.001 0 0 profiling.R#2 f/pause #&gt; 2 0.143 0.002 0 0 profiling.R#3 f/g #&gt; 3 0.071 0.000 0 0 profiling.R#4 f/h lineprof provides some functions to navigate through this data structure, but they’re a bit clumsy. Instead, we’ll start an interactive explorer using the shiny package. shine(l) will open a new web page (or if you’re using RStudio, a new pane) that shows your source code annotated with information about how long each line took to run. shine() starts a shiny app which “blocks” your R session. To exit, you’ll need to stop the process using escape or ctrl + c. The t column visualises how much time is spent on each line. (You’ll learn about the other columns in memory profiling.) While not precise, it allows you to spot bottlenecks, and you can get precise numbers by hovering over each bar. This shows that twice as much time is spent on g() as on h(), so it would make sense to drill down into g() for more details. To do so, click g(): Then h(): This technique should allow you to quickly identify the major bottlenecks in your code. 23.1.1 Limitations There are some other limitations to profiling: Profiling does not extend to C code. You can see if your R code calls C/C++ code but not what functions are called inside of your C/C++ code. Unfortunately, tools for profiling compiled code are beyond the scope of this book (i.e., I have no idea how to do it). Similarly, you can’t see what’s going on inside primitive functions or byte code compiled code. If you’re doing a lot of functional programming with anonymous functions, it can be hard to figure out exactly which function is being called. The easiest way to work around this is to name your functions. Lazy evaluation means that arguments are often evaluated inside another function. For example, in the following code, profiling would make it seem like i() was called by j() because the argument isn’t evaluated until it’s needed by j(). i &lt;- function() { pause(0.1) 10 } j &lt;- function(x) { x + 10 } j(i()) If this is confusing, you can create temporary variables to force computation to happen earlier. 23.2 Improving performance “We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.” — Donald Knuth. Once you’ve used profiling to identify a bottleneck, you need to make it faster. The following sections introduce you to a number of techniques that I’ve found broadly useful: Look for existing solutions. Do less work. Vectorise. Parallelise. Avoid copies. Byte-code compile. A final technique is to rewrite in a faster language, like C++. That’s a big topic and is covered in Rcpp. Before we get into specific techniques, I’ll first describe a general strategy and organisational style that’s useful when working on performance. 23.3 Code organisation There are two traps that are easy to fall into when trying to make your code faster: Writing faster but incorrect code. Writing code that you think is faster, but is actually no better. The strategy outlined below will help you avoid these pitfalls. When tackling a bottleneck, you’re likely to come up with multiple approaches. Write a function for each approach, encapsulating all relevant behaviour. This makes it easier to check that each approach returns the correct result and to time how long it takes to run. To demonstrate the strategy, I’ll compare two approaches for computing the mean: mean1 &lt;- function(x) mean(x) mean2 &lt;- function(x) sum(x) / length(x) I recommend that you keep a record of everything you try, even the failures. If a similar problem occurs in the future, it’ll be useful to see everything you’ve tried. To do this I often use R Markdown, which makes it easy to intermingle code with detailed comments and notes. Next, generate a representative test case. The case should be big enough to capture the essence of your problem but small enough that it takes only a few seconds to run. You don’t want it to take too long because you’ll need to run the test case many times to compare approaches. On the other hand, you don’t want the case to be too small because then results might not scale up to the real problem. Use this test case to quickly check that all variants return the same result. An easy way to do so is with stopifnot() and all.equal(). For real problems with fewer possible outputs, you may need more tests to make sure that an approach doesn’t accidentally return the correct answer. That’s unlikely for the mean. x &lt;- runif(100) stopifnot(all.equal(mean1(x), mean2(x))) Finally, use the microbenchmark package to compare how long each variation takes to run. For bigger problems, reduce the times parameter so that it only takes a couple of seconds to run. Focus on the median time, and use the upper and lower quartiles to gauge the variability of the measurement. microbenchmark( mean1(x), mean2(x) ) #&gt; Unit: nanoseconds #&gt; expr min lq mean median uq max neval cld #&gt; mean1(x) 2,180 2,240 11112 2,330 2,460 872,000 100 a #&gt; mean2(x) 486 517 26157 553 609 2,550,000 100 a (You might be surprised by the results: mean(x) is considerably slower than sum(x) / length(x). This is because, among other reasons, mean(x) makes two passes over the vector to be more numerically accurate.) Before you start experimenting, you should have a target speed that defines when the bottleneck is no longer a problem. Setting such a goal is important because you don’t want to spend valuable time over-optimising your code. If you’d like to see this strategy in action, I’ve used it a few times on stackoverflow: http://stackoverflow.com/questions/22515525#22518603 http://stackoverflow.com/questions/22515175#22515856 http://stackoverflow.com/questions/3476015#22511936 23.4 Has someone already solved the problem? Once you’ve organised your code and captured all the variations you can think of, it’s natural to see what others have done. You are part of a large community, and it’s quite possible that someone has already tackled the same problem. If your bottleneck is a function in a package, it’s worth looking at other packages that do the same thing. Two good places to start are: CRAN task views. If there’s a CRAN task view related to your problem domain, it’s worth looking at the packages listed there. Reverse dependencies of Rcpp, as listed on its CRAN page. Since these packages use C++, it’s possible to find a solution to your bottleneck written in a higher performance language. Otherwise, the challenge is describing your bottleneck in a way that helps you find related problems and solutions. Knowing the name of the problem or its synonyms will make this search much easier. But because you don’t know what it’s called, it’s hard to search for it! By reading broadly about statistics and algorithms, you can build up your own knowledge base over time. Alternatively, ask others. Talk to your colleagues and brainstorm some possible names, then search on Google and stackoverflow. It’s often helpful to restrict your search to R related pages. For Google, try rseek. For stackoverflow, restrict your search by including the R tag, [R], in your search. As discussed above, record all solutions that you find, not just those that immediately appear to be faster. Some solutions might be initially slower, but because they are easier to optimise they end up being faster. You may also be able to combine the fastest parts from different approaches. If you’ve found a solution that’s fast enough, congratulations! If appropriate, you may want to share your solution with the R community. Otherwise, read on. 23.4.1 Exercises What are faster alternatives to lm? Which are specifically designed to work with larger datasets? What package implements a version of match() that’s faster for repeated lookups? How much faster is it? List four functions (not just those in base R) that convert a string into a date time object. What are their strengths and weaknesses? How many different ways can you compute a 1d density estimate in R? Which packages provide the ability to compute a rolling mean? What are the alternatives to optim()? 23.5 Do as little as possible The easiest way to make a function faster is to let it do less work. One way to do that is use a function tailored to a more specific type of input or output, or a more specific problem. For example: rowSums(), colSums(), rowMeans(), and colMeans() are faster than equivalent invocations that use apply() because they are vectorised (the topic of the next section). vapply() is faster than sapply() because it pre-specifies the output type. If you want to see if a vector contains a single value, any(x == 10) is much faster than 10 %in% x. This is because testing equality is simpler than testing inclusion in a set. Having this knowledge at your fingertips requires knowing that alternative functions exist: you need to have a good vocabulary. Start with the basics, and expand your vocab by regularly reading R code. Good places to read code are the R-help mailing list and stackoverflow. Some functions coerce their inputs into a specific type. If your input is not the right type, the function has to do extra work. Instead, look for a function that works with your data as it is, or consider changing the way you store your data. The most common example of this problem is using apply() on a data frame. apply() always turns its input into a matrix. Not only is this error prone (because a data frame is more general than a matrix), it is also slower. Other functions will do less work if you give them more information about the problem. It’s always worthwhile to carefully read the documentation and experiment with different arguments. Some examples that I’ve discovered in the past include: read.csv(): specify known column types with colClasses. factor(): specify known levels with levels. cut(): don’t generate labels with labels = FALSE if you don’t need them, or, even better, use findInterval() as mentioned in the “see also” section of the documentation. unlist(x, use.names = FALSE) is much faster than unlist(x). interaction(): if you only need combinations that exist in the data, use drop = TRUE. Sometimes you can make a function faster by avoiding method dispatch. As we saw in (Extreme dynamism), method dispatch in R can be costly. If you’re calling a method in a tight loop, you can avoid some of the costs by doing the method lookup only once: For S3, you can do this by calling generic.class() instead of generic(). For S4, you can do this by using getMethod() to find the method, saving it to a variable, and then calling that function. For example, calling mean.default() quite a bit faster than calling mean() for small vectors: x &lt;- runif(1e2) microbenchmark( mean(x), mean.default(x) ) #&gt; Unit: nanoseconds #&gt; expr min lq mean median uq max neval cld #&gt; mean(x) 1,750 1,800 2107 1,870 1,940 21,500 100 b #&gt; mean.default(x) 799 836 946 872 942 3,240 100 a This optimisation is a little risky. While mean.default() is almost twice as fast, it’ll fail in surprising ways if x is not a numeric vector. You should only use it if you know for sure what x is. Knowing that you’re dealing with a specific type of input can be another way to write faster code. For example, as.data.frame() is quite slow because it coerces each element into a data frame and then rbind()s them together. If you have a named list with vectors of equal length, you can directly transform it into a data frame. In this case, if you’re able to make strong assumptions about your input, you can write a method that’s about 20x faster than the default. quickdf &lt;- function(l) { class(l) &lt;- &quot;data.frame&quot; attr(l, &quot;row.names&quot;) &lt;- .set_row_names(length(l[[1]])) l } l &lt;- lapply(1:26, function(i) runif(1e3)) names(l) &lt;- letters microbenchmark( quick_df = quickdf(l), as.data.frame = as.data.frame(l) ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; quick_df 5.08 7.07 38.3 11.3 12.3 2,800 100 a #&gt; as.data.frame 907.00 955.00 1030.4 972.0 1,000.0 4,280 100 b Again, note the trade-off. This method is fast because it’s dangerous. If you give it bad inputs, you’ll get a corrupt data frame: quickdf(list(x = 1, y = 1:2)) #&gt; Warning in format.data.frame(x, digits = digits, na.encode = FALSE): #&gt; corrupt data frame: columns will be truncated or padded with NAs #&gt; x y #&gt; 1 1 1 To come up with this minimal method, I carefully read through and then rewrote the source code for as.data.frame.list() and data.frame(). I made many small changes, each time checking that I hadn’t broken existing behaviour. After several hours work, I was able to isolate the minimal code shown above. This is a very useful technique. Most base R functions are written for flexibility and functionality, not performance. Thus, rewriting for your specific need can often yield substantial improvements. To do this, you’ll need to read the source code. It can be complex and confusing, but don’t give up! The following example shows a progressive simplification of the diff() function if you only want computing differences between adjacent values. At each step, I replace one argument with a specific case, and then check to see that the function still works. The initial function is long and complicated, but by restricting the arguments I not only make it around twice as fast, I also make it easier to understand. First, I take the code of diff() and reformat it to my style: diff1 &lt;- function (x, lag = 1L, differences = 1L) { ismat &lt;- is.matrix(x) xlen &lt;- if (ismat) dim(x)[1L] else length(x) if (length(lag) &gt; 1L || length(differences) &gt; 1L || lag &lt; 1L || differences &lt; 1L) stop(&quot;&#39;lag&#39; and &#39;differences&#39; must be integers &gt;= 1&quot;) if (lag * differences &gt;= xlen) { return(x[0L]) } r &lt;- unclass(x) i1 &lt;- -seq_len(lag) if (ismat) { for (i in seq_len(differences)) { r &lt;- r[i1, , drop = FALSE] - r[-nrow(r):-(nrow(r) - lag + 1L), , drop = FALSE] } } else { for (i in seq_len(differences)) { r &lt;- r[i1] - r[-length(r):-(length(r) - lag + 1L)] } } class(r) &lt;- oldClass(x) r } Next, I assume vector input. This allows me to remove the is.matrix() test and the method that uses matrix subsetting. diff2 &lt;- function (x, lag = 1L, differences = 1L) { xlen &lt;- length(x) if (length(lag) &gt; 1L || length(differences) &gt; 1L || lag &lt; 1L || differences &lt; 1L) stop(&quot;&#39;lag&#39; and &#39;differences&#39; must be integers &gt;= 1&quot;) if (lag * differences &gt;= xlen) { return(x[0L]) } i1 &lt;- -seq_len(lag) for (i in seq_len(differences)) { x &lt;- x[i1] - x[-length(x):-(length(x) - lag + 1L)] } x } diff2(cumsum(0:10)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 I now assume that difference = 1L. This simplifies input checking and eliminates the for loop: diff3 &lt;- function (x, lag = 1L) { xlen &lt;- length(x) if (length(lag) &gt; 1L || lag &lt; 1L) stop(&quot;&#39;lag&#39; must be integer &gt;= 1&quot;) if (lag &gt;= xlen) { return(x[0L]) } i1 &lt;- -seq_len(lag) x[i1] - x[-length(x):-(length(x) - lag + 1L)] } diff3(cumsum(0:10)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Finally I assume lag = 1L. This eliminates input checking and simplifies subsetting. diff4 &lt;- function (x) { xlen &lt;- length(x) if (xlen &lt;= 1) return(x[0L]) x[-1] - x[-xlen] } diff4(cumsum(0:10)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Now diff4() is both considerably simpler and considerably faster than diff1(): x &lt;- runif(100) microbenchmark( diff1(x), diff2(x), diff3(x), diff4(x) ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; diff1(x) 2.95 4.98 159.85 5.44 5.97 15,400.0 100 a #&gt; diff2(x) 2.69 4.44 5.24 4.79 5.14 30.2 100 a #&gt; diff3(x) 2.12 4.15 66.98 4.42 4.68 6,250.0 100 a #&gt; diff4(x) 1.60 3.46 32.50 3.60 4.07 2,870.0 100 a You’ll be able to make diff() even faster for this special case once you’ve read Rcpp. A final example of doing less work is to use simpler data structures. For example, when working with rows from a data frame, it’s often faster to work with row indices than data frames. For instance, if you wanted to compute a bootstrap estimate of the correlation between two columns in a data frame, there are two basic approaches: you can either work with the whole data frame or with the individual vectors. The following example shows that working with vectors is about twice as fast. sample_rows &lt;- function(df, i) sample.int(nrow(df), i, replace = TRUE) # Generate a new data frame containing randomly selected rows boot_cor1 &lt;- function(df, i) { sub &lt;- df[sample_rows(df, i), , drop = FALSE] cor(sub$x, sub$y) } # Generate new vectors from random rows boot_cor2 &lt;- function(df, i ) { idx &lt;- sample_rows(df, i) cor(df$x[idx], df$y[idx]) } df &lt;- data.frame(x = runif(100), y = runif(100)) microbenchmark( boot_cor1(df, 10), boot_cor2(df, 10) ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; boot_cor1(df, 10) 86.4 89.1 128.6 91.0 94.9 2,260 100 a #&gt; boot_cor2(df, 10) 56.0 59.2 87.6 60.3 61.7 2,350 100 a 23.5.1 Exercises How do the results change if you compare mean() and mean.default() on 10,000 observations, rather than on 100? The following code provides an alternative implementation of rowSums(). Why is it faster for this input? rowSums2 &lt;- function(df) { out &lt;- df[[1L]] if (ncol(df) == 1) return(out) for (i in 2:ncol(df)) { out &lt;- out + df[[i]] } out } df &lt;- as.data.frame( replicate(1e3, sample(100, 1e4, replace = TRUE)) ) system.time(rowSums(df)) #&gt; user system elapsed #&gt; 0.049 0.002 0.052 system.time(rowSums2(df)) #&gt; user system elapsed #&gt; 0.036 0.004 0.039 What’s the difference between rowSums() and .rowSums()? Make a faster version of chisq.test() that only computes the chi-square test statistic when the input is two numeric vectors with no missing values. You can try simplifying chisq.test() or by coding from the mathematical definition. Can you make a faster version of table() for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test? Imagine you want to compute the bootstrap distribution of a sample correlation using cor_df() and the data in the example below. Given that you want to run this many times, how can you make this code faster? (Hint: the function has three components that you can speed up.) n &lt;- 1e6 df &lt;- data.frame(a = rnorm(n), b = rnorm(n)) cor_df &lt;- function(df, n) { i &lt;- sample(seq(n), n, replace = TRUE) cor(df[i, , drop = FALSE])[2,1] } Is there a way to vectorise this procedure? 23.6 Vectorise If you’ve used R for any length of time, you’ve probably heard the admonishment to “vectorise your code”. But what does that actually mean? Vectorising your code is not just about avoiding for loops, although that’s often a step. Vectorising is about taking a “whole object” approach to a problem, thinking about vectors, not scalars. There are two key attributes of a vectorised function: It makes many problems simpler. Instead of having to think about the components of a vector, you only think about entire vectors. The loops in a vectorised function are written in C instead of R. Loops in C are much faster because they have much less overhead. Functionals stressed the importance of vectorised code as a higher level abstraction. Vectorisation is also important for writing fast R code. This doesn’t mean simply using apply() or lapply(), or even Vectorise(). Those functions improve the interface of a function, but don’t fundamentally change performance. Using vectorisation for performance means finding the existing R function that is implemented in C and most closely applies to your problem. Vectorised functions that apply to many common performance bottlenecks include: rowSums(), colSums(), rowMeans(), and colMeans(). These vectorised matrix functions will always be faster than using apply(). You can sometimes use these functions to build other vectorised functions. rowAny &lt;- function(x) rowSums(x) &gt; 0 rowAll &lt;- function(x) rowSums(x) == ncol(x) Vectorised subsetting can lead to big improvements in speed. Remember the techniques behind lookup tables (lookup tables) and matching and merging by hand (matching and merging by hand). Also remember that you can use subsetting assignment to replace multiple values in a single step. If x is a vector, matrix or data frame then x[is.na(x)] &lt;- 0 will replace all missing values with 0. If you’re extracting or replacing values in scattered locations in a matrix or data frame, subset with an integer matrix. See matrix subsetting for more details. If you’re converting continuous values to categorical make sure you know how to use cut() and findInterval(). Be aware of vectorised functions like cumsum() and diff(). Matrix algebra is a general example of vectorisation. There loops are executed by highly tuned external libraries like BLAS. If you can figure out a way to use matrix algebra to solve your problem, you’ll often get a very fast solution. The ability to solve problems with matrix algebra is a product of experience. While this skill is something you’ll develop over time, a good place to start is to ask people with experience in your domain. The downside of vectorisation is that it makes it harder to predict how operations will scale. The following example measures how long it takes to use character subsetting to lookup 1, 10, and 100 elements from a list. You might expect that looking up 10 elements would take 10x as long as looking up 1, and that looking up 100 elements would take 10x longer again. In fact, the following example shows that it only takes about 9 times longer to look up 100 elements than it does to look up 1. lookup &lt;- setNames(as.list(sample(100, 26)), letters) x1 &lt;- &quot;j&quot; x10 &lt;- sample(letters, 10) x100 &lt;- sample(letters, 100, replace = TRUE) microbenchmark( lookup[x1], lookup[x10], lookup[x100] ) #&gt; Unit: nanoseconds #&gt; expr min lq mean median uq max neval cld #&gt; lookup[x1] 371 385 532 400 496 7,350 100 a #&gt; lookup[x10] 1,150 1,180 1259 1,200 1,260 2,760 100 b #&gt; lookup[x100] 3,360 3,460 3756 3,590 3,790 12,300 100 c Vectorisation won’t solve every problem, and rather than torturing an existing algorithm into one that uses a vectorised approach, you’re often better off writing your own vectorised function in C++. You’ll learn how to do so in Rcpp. 23.6.1 Exercises The density functions, e.g., dnorm(), have a common interface. Which arguments are vectorised over? What does rnorm(10, mean = 10:1) do? Compare the speed of apply(x, 1, sum) with rowSums(x) for varying sizes of x. How can you use crossprod() to compute a weighted sum? How much faster is it than the naive sum(x * w)? 23.7 Avoid copies A pernicious source of slow R code is growing an object with a loop. Whenever you use c(), append(), cbind(), rbind(), or paste() to create a bigger object, R must first allocate space for the new object and then copy the old object to its new home. If you’re repeating this many times, like in a for loop, this can be quite expensive. You’ve entered Circle 2 of the “R inferno”. Here’s a little example that shows the problem. We first generate some random strings, and then combine them either iteratively with a loop using collapse(), or in a single pass using paste(). Note that the performance of collapse() gets relatively worse as the number of strings grows: combining 100 strings takes almost 30 times longer than combining 10 strings. random_string &lt;- function() { paste(sample(letters, 50, replace = TRUE), collapse = &quot;&quot;) } strings10 &lt;- replicate(10, random_string()) strings100 &lt;- replicate(100, random_string()) collapse &lt;- function(xs) { out &lt;- &quot;&quot; for (x in xs) { out &lt;- paste0(out, x) } out } microbenchmark( loop10 = collapse(strings10), loop100 = collapse(strings100), vec10 = paste(strings10, collapse = &quot;&quot;), vec100 = paste(strings100, collapse = &quot;&quot;) ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; loop10 13.80 14.80 41.43 15.30 16.20 2,480.0 100 a #&gt; loop100 463.00 466.00 496.51 469.00 512.00 695.0 100 b #&gt; vec10 3.22 3.56 4.08 3.77 4.03 20.5 100 a #&gt; vec100 25.30 25.90 28.88 26.30 28.60 74.0 100 a Modifying an object in a loop, e.g., x[i] &lt;- y, can also create a copy, depending on the class of x. Modification in place discusses this issue in more depth and gives you some tools to determine when you’re making copies. 23.8 Byte code compilation R 2.13.0 introduced a byte code compiler which can increase the speed of some code. Using the compiler is an easy way to get improvements in speed. Even if it doesn’t work well for your function, you won’t have invested a lot of time in the effort. The following example shows the pure R version of lapply() from functionals. Compiling it gives a considerable speedup, although it’s still not quite as fast as the C version provided by base R. lapply2 &lt;- function(x, f, ...) { out &lt;- vector(&quot;list&quot;, length(x)) for (i in seq_along(x)) { out[[i]] &lt;- f(x[[i]], ...) } out } lapply2_c &lt;- compiler::cmpfun(lapply2) x &lt;- list(1:10, letters, c(F, T), NULL) microbenchmark( lapply2(x, is.null), lapply2_c(x, is.null), lapply(x, is.null) ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; lapply2(x, is.null) 1.41 1.44 35.66 1.46 1.56 3,420.0 100 a #&gt; lapply2_c(x, is.null) 1.41 1.43 1.63 1.46 1.54 10.3 100 a #&gt; lapply(x, is.null) 1.68 1.76 2.08 1.82 1.90 18.4 100 a Byte code compilation really helps here, but in most cases you’re more likely to get a 5-10% improvement. All base R functions are byte code compiled by default. 23.9 Case study: t-test The following case study shows how to make t-tests faster using some of the techniques described above. It’s based on an example in “Computing thousands of test statistics simultaneously in R” by Holger Schwender and Tina Müller. I thoroughly recommend reading the paper in full to see the same idea applied to other tests. Imagine we have run 1000 experiments (rows), each of which collects data on 50 individuals (columns). The first 25 individuals in each experiment are assigned to group 1 and the rest to group 2. We’ll first generate some random data to represent this problem: m &lt;- 1000 n &lt;- 50 X &lt;- matrix(rnorm(m * n, mean = 10, sd = 3), nrow = m) grp &lt;- rep(1:2, each = n / 2) For data in this form, there are two ways to use t.test(). We can either use the formula interface or provide two vectors, one for each group. Timing reveals that the formula interface is considerably slower. system.time(for(i in 1:m) t.test(X[i, ] ~ grp)$statistic) #&gt; user system elapsed #&gt; 0.612 0.004 0.619 system.time( for(i in 1:m) t.test(X[i, grp == 1], X[i, grp == 2])$statistic ) #&gt; user system elapsed #&gt; 0.190 0.003 0.198 Of course, a for loop computes, but doesn’t save the values. We’ll use apply() to do that. This adds a little overhead: compT &lt;- function(x, grp){ t.test(x[grp == 1], x[grp == 2])$statistic } system.time(t1 &lt;- apply(X, 1, compT, grp = grp)) #&gt; user system elapsed #&gt; 0.177 0.002 0.180 How can we make this faster? First, we could try doing less work. If you look at the source code of stats:::t.test.default(), you’ll see that it does a lot more than just compute the t-statistic. It also computes the p-value and formats the output for printing. We can try to make our code faster by stripping out those pieces. my_t &lt;- function(x, grp) { t_stat &lt;- function(x) { m &lt;- mean(x) n &lt;- length(x) var &lt;- sum((x - m) ^ 2) / (n - 1) list(m = m, n = n, var = var) } g1 &lt;- t_stat(x[grp == 1]) g2 &lt;- t_stat(x[grp == 2]) se_total &lt;- sqrt(g1$var / g1$n + g2$var / g2$n) (g1$m - g2$m) / se_total } system.time(t2 &lt;- apply(X, 1, my_t, grp = grp)) #&gt; user system elapsed #&gt; 0.021 0.000 0.022 stopifnot(all.equal(t1, t2)) This gives us about a 6x speed improvement. Now that we have a fairly simple function, we can make it faster still by vectorising it. Instead of looping over the array outside the function, we will modify t_stat() to work with a matrix of values. Thus, mean() becomes rowMeans(), length() becomes ncol(), and sum() becomes rowSums(). The rest of the code stays the same. rowtstat &lt;- function(X, grp){ t_stat &lt;- function(X) { m &lt;- rowMeans(X) n &lt;- ncol(X) var &lt;- rowSums((X - m) ^ 2) / (n - 1) list(m = m, n = n, var = var) } g1 &lt;- t_stat(X[, grp == 1]) g2 &lt;- t_stat(X[, grp == 2]) se_total &lt;- sqrt(g1$var / g1$n + g2$var / g2$n) (g1$m - g2$m) / se_total } system.time(t3 &lt;- rowtstat(X, grp)) #&gt; user system elapsed #&gt; 0.012 0.000 0.013 stopifnot(all.equal(t1, t3)) That’s much faster! It’s at least 40x faster than our previous effort, and around 1000x faster than where we started. Finally, we could try byte code compilation. Here we’ll need to use microbenchmark() instead of system.time() in order to get enough accuracy to see a difference: rowtstat_bc &lt;- compiler::cmpfun(rowtstat) microbenchmark( rowtstat(X, grp), rowtstat_bc(X, grp), unit = &quot;ms&quot; ) #&gt; Unit: milliseconds #&gt; expr min lq mean median uq max neval cld #&gt; rowtstat(X, grp) 0.633 0.664 0.718 0.686 0.733 1.26 100 a #&gt; rowtstat_bc(X, grp) 0.344 0.657 0.894 0.677 0.728 20.50 100 a In this example, byte code compilation doesn’t help at all. 23.10 Parallelise Parallelisation uses multiple cores to work simultaneously on different parts of a problem. It doesn’t reduce the computing time, but it saves your time because you’re using more of your computer’s resources. Parallel computing is a complex topic, and there’s no way to cover it in depth here. Some resources I recommend are: Parallel R by Q. Ethan McCallum and Stephen Weston. Parallel Computing for Data Science by Norm Matloff. What I want to show is a simple application of parallel computing to what are called “embarrassingly parallel problems”. An embarrassingly parallel problem is one that’s made up of many simple problems that can be solved independently. A great example of this is lapply() because it operates on each element independently of the others. It’s very easy to parallelise lapply() on Linux and the Mac because you simply substitute mclapply() for lapply(). The following code snippet runs a trivial (but slow) function on all cores of your computer. library(parallel) cores &lt;- detectCores() cores #&gt; [1] 8 pause &lt;- function(i) { function(x) Sys.sleep(i) } system.time(lapply(1:10, pause(0.25))) #&gt; user system elapsed #&gt; 0.00 0.00 2.53 system.time(mclapply(1:10, pause(0.25), mc.cores = cores)) #&gt; user system elapsed #&gt; 0.011 0.018 0.509 Life is a bit harder in Windows. You need to first set up a local cluster and then use parLapply(): cluster &lt;- makePSOCKcluster(cores) system.time(parLapply(cluster, 1:10, function(i) Sys.sleep(i))) #&gt; user system elapsed #&gt; 0.003 0.000 19.025 The main difference between mclapply() and makePSOCKcluster() is that the individual processes generated by mclapply() inherit from the current process, while those generated by makePSOCKcluster() start with a fresh session. This means that most real code will need some setup. Use clusterEvalQ() to run arbitrary code on each cluster and load needed packages, and clusterExport() to copy objects in the current session to the remote sessions. x &lt;- 10 psock &lt;- parallel::makePSOCKcluster(1L) clusterEvalQ(psock, x) #&gt; Error: one node produced an error: object &#39;x&#39; not found clusterExport(psock, &quot;x&quot;) clusterEvalQ(psock, x) #&gt; [[1]] #&gt; [1] 10 There is some communication overhead with parallel computing. If the subproblems are very small, then parallelisation might hurt rather than help. It’s also possible to distribute computation over a network of computers (not just the cores on your local computer) but that’s beyond the scope of this book, because it gets increasingly complicated to balance computation and communication costs. A good place to start for more information is the high performance computing CRAN task view. 23.11 Other techniques Being able to write fast R code is part of being a good R programmer. Beyond the specific hints in this chapter, if you want to write fast R code, you’ll need to improve your general programming skills. Some ways to do this are to: Read R blogs to see what performance problems other people have struggled with, and how they have made their code faster. Read other R programming books, like Norm Matloff’s The Art of R Programming or Patrick Burns’ R Inferno to learn about common traps. Take an algorithms and data structure course to learn some well known ways of tackling certain classes of problems. I have heard good things about Princeton’s Algorithms course offered on Coursera. Read general books about optimisation like Mature optimisation by Carlos Bueno, or the Pragmatic Programmer by Andrew Hunt and David Thomas. You can also reach out to the community for help. Stackoverflow can be a useful resource. You’ll need to put some effort into creating an easily digestible example that also captures the salient features of your problem. If your example is too complex, few people will have the time and motivation to attempt a solution. If it’s too simple, you’ll get answers that solve the toy problem but not the real problem. If you also try to answer questions on stackoverflow, you’ll quickly get a feel for what makes a good question. "],
["rcpp.html", "24 High performance functions with Rcpp 24.1 Getting started with C++ 24.2 Attributes and other classes 24.3 Missing values 24.4 Rcpp sugar 24.5 The STL 24.6 Case studies 24.7 Using Rcpp in a package 24.8 Learning more 24.9 Acknowledgments", " 24 High performance functions with Rcpp Sometimes R code just isn’t fast enough. You’ve used profiling to figure out where your bottlenecks are, and you’ve done everything you can in R, but your code still isn’t fast enough. In this chapter you’ll learn how to improve performance by rewriting key functions in C++. This magic comes by way of the Rcpp package, a fantastic tool written by Dirk Eddelbuettel and Romain Francois (with key contributions by Doug Bates, John Chambers, and JJ Allaire). Rcpp makes it very simple to connect C++ to R. While it is possible to write C or Fortran code for use in R, it will be painful by comparison. Rcpp provides a clean, approachable API that lets you write high-performance code, insulated from R’s arcane C API. Typical bottlenecks that C++ can address include: Loops that can’t be easily vectorised because subsequent iterations depend on previous ones. Recursive functions, or problems which involve calling functions millions of times. The overhead of calling a function in C++ is much lower than that in R. Problems that require advanced data structures and algorithms that R doesn’t provide. Through the standard template library (STL), C++ has efficient implementations of many important data structures, from ordered maps to double-ended queues. The aim of this chapter is to discuss only those aspects of C++ and Rcpp that are absolutely necessary to help you eliminate bottlenecks in your code. We won’t spend much time on advanced features like object oriented programming or templates because the focus is on writing small, self-contained functions, not big programs. A working knowledge of C++ is helpful, but not essential. Many good tutorials and references are freely available, including http://www.learncpp.com/ and http://www.cplusplus.com/. For more advanced topics, the Effective C++ series by Scott Meyers is a popular choice. You may also enjoy Dirk Eddelbuettel’s Seamless R and C++ integration with Rcpp, which goes into much greater detail into all aspects of Rcpp. 24.0.0.0.1 Outline Getting started with C++ teaches you how to write C++ by converting simple R functions to their C++ equivalents. You’ll learn how C++ differs from R, and what the key scalar, vector, and matrix classes are called. Using sourceCpp shows you how to use sourceCpp() to load a C++ file from disk in the same way you use source() to load a file of R code. Attributes &amp; other classes discusses how to modify attributes from Rcpp, and mentions some of the other important classes. Missing values teaches you how to work with R’s missing values in C++. Rcpp sugar discusses Rcpp “sugar”, which allows you to avoid loops in C++ and write code that looks very similar to vectorised R code. The STL shows you how to use some of the most important data structures and algorithms from the standard template library, or STL, built-in to C++. Case studies shows two real case studies where Rcpp was used to get considerable performance improvements. Putting Rcpp in a package teaches you how to add C++ code to a package. Learning more concludes the chapter with pointers to more resources to help you learn Rcpp and C++. 24.0.0.0.2 Prerequistes All examples in this chapter need version 0.10.1 or above of the Rcpp package. This version includes cppFunction() and sourceCpp(), which makes it very easy to connect C++ to R. Install the latest version of Rcpp from CRAN with install.packages(&quot;Rcpp&quot;). You’ll also need a working C++ compiler. To get it: On Windows, install Rtools. On Mac, install Xcode from the app store. On Linux, sudo apt-get install r-base-dev or similar. 24.1 Getting started with C++ cppFunction() allows you to write C++ functions in R: library(Rcpp) cppFunction(&#39;int add(int x, int y, int z) { int sum = x + y + z; return sum; }&#39;) # add works like a regular R function add #&gt; function (x, y, z) #&gt; .Call(&lt;pointer: 0x10f3e7220&gt;, x, y, z) add(1, 2, 3) #&gt; [1] 6 When you run this code, Rcpp will compile the C++ code and construct an R function that connects to the compiled C++ function. We’re going to use this simple interface to learn how to write C++. C++ is a large language, and there’s no way to cover it all in just one chapter. Instead, you’ll get the basics so that you can start writing useful functions to address bottlenecks in your R code. The following sections will teach you the basics by translating simple R functions to their C++ equivalents. We’ll start simple with a function that has no inputs and a scalar output, and then get progressively more complicated: Scalar input and scalar output Vector input and scalar output Vector input and vector output Matrix input and vector output 24.1.1 No inputs, scalar output Let’s start with a very simple function. It has no arguments and always returns the integer 1: one &lt;- function() 1L The equivalent C++ function is: int one() { return 1; } We can compile and use this from R with cppFunction cppFunction(&#39;int one() { return 1; }&#39;) This small function illustrates a number of important differences between R and C++: The syntax to create a function looks like the syntax to call a function; you don’t use assignment to create functions as you do in R. You must declare the type of output the function returns. This function returns an int (a scalar integer). The classes for the most common types of R vectors are: NumericVector, IntegerVector, CharacterVector, and LogicalVector. Scalars and vectors are different. The scalar equivalents of numeric, integer, character, and logical vectors are: double, int, String, and bool. You must use an explicit return statement to return a value from a function. Every statement is terminated by a ;. 24.1.2 Scalar input, scalar output The next example function implements a scalar version of the sign() function which returns 1 if the input is positive, and -1 if it’s negative: signR &lt;- function(x) { if (x &gt; 0) { 1 } else if (x == 0) { 0 } else { -1 } } cppFunction(&#39;int signC(int x) { if (x &gt; 0) { return 1; } else if (x == 0) { return 0; } else { return -1; } }&#39;) In the C++ version: We declare the type of each input in the same way we declare the type of the output. While this makes the code a little more verbose, it also makes it very obvious what type of input the function needs. The if syntax is identical — while there are some big differences between R and C++, there are also lots of similarities! C++ also has a while statement that works the same way as R’s. As in R you can use break to exit the loop, but to skip one iteration you need to use continue instead of next. 24.1.3 Vector input, scalar output One big difference between R and C++ is that the cost of loops is much lower in C++. For example, we could implement the sum function in R using a loop. If you’ve been programming in R a while, you’ll probably have a visceral reaction to this function! sumR &lt;- function(x) { total &lt;- 0 for (i in seq_along(x)) { total &lt;- total + x[i] } total } In C++, loops have very little overhead, so it’s fine to use them. In STL, you’ll see alternatives to for loops that more clearly express your intent; they’re not faster, but they can make your code easier to understand. cppFunction(&#39;double sumC(NumericVector x) { int n = x.size(); double total = 0; for(int i = 0; i &lt; n; ++i) { total += x[i]; } return total; }&#39;) The C++ version is similar, but: To find the length of the vector, we use the .size() method, which returns an integer. C++ methods are called with . (i.e., a full stop). The for statement has a different syntax: for(init; check; increment). This loop is initialised by creating a new variable called i with value 0. Before each iteration we check that i &lt; n, and terminate the loop if it’s not. After each iteration, we increment the value of i by one, using the special prefix operator ++ which increases the value of i by 1. In C++, vector indices start at 0. I’ll say this again because it’s so important: IN C++, VECTOR INDICES START AT 0! This is a very common source of bugs when converting R functions to C++. Use = for assignment, not &lt;-. C++ provides operators that modify in-place: total += x[i] is equivalent to total = total + x[i]. Similar in-place operators are -=, *=, and /=. This is a good example of where C++ is much more efficient than R. As shown by the following microbenchmark, sumC() is competitive with the built-in (and highly optimised) sum(), while sumR() is several orders of magnitude slower. x &lt;- runif(1e3) microbenchmark( sum(x), sumC(x), sumR(x) ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; sum(x) 0.924 1.06 1.65 1.27 1.62 17.4 100 a #&gt; sumC(x) 1.780 2.34 15.35 2.96 3.99 1,170.0 100 ab #&gt; sumR(x) 38.700 40.00 88.93 41.50 49.30 4,230.0 100 b 24.1.4 Vector input, vector output Next we’ll create a function that computes the Euclidean distance between a value and a vector of values: pdistR &lt;- function(x, ys) { sqrt((x - ys) ^ 2) } It’s not obvious that we want x to be a scalar from the function definition. We’d need to make that clear in the documentation. That’s not a problem in the C++ version because we have to be explicit about types: cppFunction(&#39;NumericVector pdistC(double x, NumericVector ys) { int n = ys.size(); NumericVector out(n); for(int i = 0; i &lt; n; ++i) { out[i] = sqrt(pow(ys[i] - x, 2.0)); } return out; }&#39;) This function introduces only a few new concepts: We create a new numeric vector of length n with a constructor: NumericVector out(n). Another useful way of making a vector is to copy an existing one: NumericVector zs = clone(ys). C++ uses pow(), not ^, for exponentiation. Note that because the R version is fully vectorised, it’s already going to be fast. On my computer, it takes around 8 ms with a 1 million element y vector. The C++ function is twice as fast, ~4 ms, but assuming it took you 10 minutes to write the C++ function, you’d need to run it ~150,000 times to make rewriting worthwhile. The reason why the C++ function is faster is subtle, and relates to memory management. The R version needs to create an intermediate vector the same length as y (x - ys), and allocating memory is an expensive operation. The C++ function avoids this overhead because it uses an intermediate scalar. In the sugar section, you’ll see how to rewrite this function to take advantage of Rcpp’s vectorised operations so that the C++ code is almost as concise as R code. 24.1.5 Matrix input, vector output Each vector type has a matrix equivalent: NumericMatrix, IntegerMatrix, CharacterMatrix, and LogicalMatrix. Using them is straightforward. For example, we could create a function that reproduces rowSums(): cppFunction(&#39;NumericVector rowSumsC(NumericMatrix x) { int nrow = x.nrow(), ncol = x.ncol(); NumericVector out(nrow); for (int i = 0; i &lt; nrow; i++) { double total = 0; for (int j = 0; j &lt; ncol; j++) { total += x(i, j); } out[i] = total; } return out; }&#39;) set.seed(1014) x &lt;- matrix(sample(100), 10) rowSums(x) #&gt; [1] 458 558 488 458 536 537 488 491 508 528 rowSumsC(x) #&gt; [1] 458 558 488 458 536 537 488 491 508 528 The main differences: In C++, you subset a matrix with (), not []. Use .nrow() and .ncol() methods to get the dimensions of a matrix. 24.1.6 Using sourceCpp So far, we’ve used inline C++ with cppFunction(). This makes presentation simpler, but for real problems, it’s usually easier to use stand-alone C++ files and then source them into R using sourceCpp(). This lets you take advantage of text editor support for C++ files (e.g., syntax highlighting) as well as making it easier to identify the line numbers in compilation errors. Your stand-alone C++ file should have extension .cpp, and needs to start with: #include &lt;Rcpp.h&gt; using namespace Rcpp; And for each function that you want available within R, you need to prefix it with: // [[Rcpp::export]] Note that the space is mandatory. If you’re familiar with roxygen2, you might wonder how this relates to @export. Rcpp::export controls whether a function is exported from C++ to R; @export controls whether a function is exported from a package and made available to the user. You can embed R code in special C++ comment blocks. This is really convenient if you want to run some test code: /*** R # This is R code */ The R code is run with source(echo = TRUE) so you don’t need to explicitly print output. To compile the C++ code, use sourceCpp(&quot;path/to/file.cpp&quot;). This will create the matching R functions and add them to your current session. Note that these functions can not be saved in a .Rdata file and reloaded in a later session; they must be recreated each time you restart R. For example, running sourceCpp() on the following file implements mean in C++ and then compares it to the built-in mean(): #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] double meanC(NumericVector x) { int n = x.size(); double total = 0; for(int i = 0; i &lt; n; ++i) { total += x[i]; } return total / n; } /*** R library(microbenchmark) x &lt;- runif(1e5) microbenchmark( mean(x), meanC(x) ) */ NB: if you run this code yourself, you’ll notice that meanC() is much faster than the built-in mean(). This is because it trades numerical accuracy for speed. For the remainder of this chapter C++ code will be presented stand-alone rather than wrapped in a call to cppFunction. If you want to try compiling and/or modifying the examples you should paste them into a C++ source file that includes the elements described above. 24.1.7 Exercises With the basics of C++ in hand, it’s now a great time to practice by reading and writing some simple C++ functions. For each of the following functions, read the code and figure out what the corresponding base R function is. You might not understand every part of the code yet, but you should be able to figure out the basics of what the function does. double f1(NumericVector x) { int n = x.size(); double y = 0; for(int i = 0; i &lt; n; ++i) { y += x[i] / n; } return y; } NumericVector f2(NumericVector x) { int n = x.size(); NumericVector out(n); out[0] = x[0]; for(int i = 1; i &lt; n; ++i) { out[i] = out[i - 1] + x[i]; } return out; } bool f3(LogicalVector x) { int n = x.size(); for(int i = 0; i &lt; n; ++i) { if (x[i]) return true; } return false; } int f4(Function pred, List x) { int n = x.size(); for(int i = 0; i &lt; n; ++i) { LogicalVector res = pred(x[i]); if (res[0]) return i + 1; } return 0; } NumericVector f5(NumericVector x, NumericVector y) { int n = std::max(x.size(), y.size()); NumericVector x1 = rep_len(x, n); NumericVector y1 = rep_len(y, n); NumericVector out(n); for (int i = 0; i &lt; n; ++i) { out[i] = std::min(x1[i], y1[i]); } return out; } To practice your function writing skills, convert the following functions into C++. For now, assume the inputs have no missing values. all() cumprod(), cummin(), cummax(). diff(). Start by assuming lag 1, and then generalise for lag n. range. var. Read about the approaches you can take on wikipedia. Whenever implementing a numerical algorithm, it’s always good to check what is already known about the problem. 24.2 Attributes and other classes You’ve already seen the basic vector classes (IntegerVector, NumericVector, LogicalVector, CharacterVector) and their scalar (int, double, bool, String) and matrix (IntegerMatrix, NumericMatrix, LogicalMatrix, CharacterMatrix) equivalents. All R objects have attributes, which can be queried and modified with .attr(). Rcpp also provides .names() as an alias for the name attribute. The following code snippet illustrates these methods. Note the use of ::create(), a class method. This allows you to create an R vector from C++ scalar values: #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector attribs() { NumericVector out = NumericVector::create(1, 2, 3); out.names() = CharacterVector::create(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;); out.attr(&quot;my-attr&quot;) = &quot;my-value&quot;; out.attr(&quot;class&quot;) = &quot;my-class&quot;; return out; } For S4 objects, .slot() plays a similar role to .attr(). 24.2.1 Lists and data frames Rcpp also provides classes List and DataFrame, but they are more useful for output than input. This is because lists and data frames can contain arbitrary classes but C++ needs to know their classes in advance. If the list has known structure (e.g., it’s an S3 object), you can extract the components and manually convert them to their C++ equivalents with as(). For example, the object created by lm(), the function that fits a linear model, is a list whose components are always of the same type. The following code illustrates how you might extract the mean percentage error (mpe()) of a linear model. This isn’t a good example of when to use C++, because it’s so easily implemented in R, but it shows how to work with an important S3 class. Note the use of .inherits() and the stop() to check that the object really is a linear model. #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] double mpe(List mod) { if (!mod.inherits(&quot;lm&quot;)) stop(&quot;Input must be a linear model&quot;); NumericVector resid = as&lt;NumericVector&gt;(mod[&quot;residuals&quot;]); NumericVector fitted = as&lt;NumericVector&gt;(mod[&quot;fitted.values&quot;]); int n = resid.size(); double err = 0; for(int i = 0; i &lt; n; ++i) { err += resid[i] / (fitted[i] + resid[i]); } return err / n; } mod &lt;- lm(mpg ~ wt, data = mtcars) mpe(mod) #&gt; [1] -0.0154 24.2.2 Functions You can put R functions in an object of type Function. This makes calling an R function from C++ straightforward. We first define our C++ function: #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] RObject callWithOne(Function f) { return f(1); } Then call it from R: callWithOne(function(x) x + 1) #&gt; [1] 2 callWithOne(paste) #&gt; [1] &quot;1&quot; What type of object does an R function return? We don’t know, so we use the catchall type RObject. An alternative is to return a List. For example, the following code is a basic implementation of lapply in C++: #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] List lapply1(List input, Function f) { int n = input.size(); List out(n); for(int i = 0; i &lt; n; i++) { out[i] = f(input[i]); } return out; } Calling R functions with positional arguments is obvious: f(&quot;y&quot;, 1); But to use named arguments, you need a special syntax: f(_[&quot;x&quot;] = &quot;y&quot;, _[&quot;value&quot;] = 1); 24.2.3 Other types There are also classes for many more specialised language objects: Environment, ComplexVector, RawVector, DottedPair, Language, Promise, Symbol, WeakReference, and so on. These are beyond the scope of this chapter and won’t be discussed further. 24.3 Missing values If you’re working with missing values, you need to know two things: how R’s missing values behave in C++’s scalars (e.g., double). how to get and set missing values in vectors (e.g., NumericVector). 24.3.1 Scalars The following code explores what happens when you take one of R’s missing values, coerce it into a scalar, and then coerce back to an R vector. Note that this kind of experimentation is a useful way to figure out what any operation does. #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] List scalar_missings() { int int_s = NA_INTEGER; String chr_s = NA_STRING; bool lgl_s = NA_LOGICAL; double num_s = NA_REAL; return List::create(int_s, chr_s, lgl_s, num_s); } str(scalar_missings()) #&gt; List of 4 #&gt; $ : int NA #&gt; $ : chr NA #&gt; $ : logi TRUE #&gt; $ : num NA With the exception of bool, things look pretty good here: all of the missing values have been preserved. However, as we’ll see in the following sections, things are not quite as straightforward as they seem. 24.3.1.1 Integers With integers, missing values are stored as the smallest integer. If you don’t do anything to them, they’ll be preserved. But, since C++ doesn’t know that the smallest integer has this special behaviour, if you do anything to it you’re likely to get an incorrect value: for example, evalCpp('NA_INTEGER + 1') gives -2147483647. So if you want to work with missing values in integers, either use a length one IntegerVector or be very careful with your code. 24.3.1.2 Doubles With doubles, you may be able to get away with ignoring missing values and working with NaNs (not a number). This is because R’s NA is a special type of IEEE 754 floating point number NaN. So any logical expression that involves a NaN (or in C++, NAN) always evaluates as FALSE: evalCpp(&quot;NAN == 1&quot;) #&gt; [1] FALSE evalCpp(&quot;NAN &lt; 1&quot;) #&gt; [1] FALSE evalCpp(&quot;NAN &gt; 1&quot;) #&gt; [1] FALSE evalCpp(&quot;NAN == NAN&quot;) #&gt; [1] FALSE But be careful when combining them with boolean values: evalCpp(&quot;NAN &amp;&amp; TRUE&quot;) #&gt; [1] TRUE evalCpp(&quot;NAN || FALSE&quot;) #&gt; [1] TRUE However, in numeric contexts NaNs will propagate NAs: evalCpp(&quot;NAN + 1&quot;) #&gt; [1] NaN evalCpp(&quot;NAN - 1&quot;) #&gt; [1] NaN evalCpp(&quot;NAN / 1&quot;) #&gt; [1] NaN evalCpp(&quot;NAN * 1&quot;) #&gt; [1] NaN 24.3.2 Strings String is a scalar string class introduced by Rcpp, so it knows how to deal with missing values. 24.3.3 Boolean While C++’s bool has two possible values (true or false), a logical vector in R has three (TRUE, FALSE, and NA). If you coerce a length 1 logical vector, make sure it doesn’t contain any missing values otherwise they will be converted to TRUE. 24.3.4 Vectors With vectors, you need to use a missing value specific to the type of vector, NA_REAL, NA_INTEGER, NA_LOGICAL, NA_STRING: #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] List missing_sampler() { return List::create( NumericVector::create(NA_REAL), IntegerVector::create(NA_INTEGER), LogicalVector::create(NA_LOGICAL), CharacterVector::create(NA_STRING)); } str(missing_sampler()) #&gt; List of 4 #&gt; $ : num NA #&gt; $ : int NA #&gt; $ : logi NA #&gt; $ : chr NA To check if a value in a vector is missing, use the class method ::is_na(): #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] LogicalVector is_naC(NumericVector x) { int n = x.size(); LogicalVector out(n); for (int i = 0; i &lt; n; ++i) { out[i] = NumericVector::is_na(x[i]); } return out; } is_naC(c(NA, 5.4, 3.2, NA)) #&gt; [1] TRUE FALSE FALSE TRUE Another alternative is the sugar function is_na(), which takes a vector and returns a logical vector. #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] LogicalVector is_naC2(NumericVector x) { return is_na(x); } is_naC2(c(NA, 5.4, 3.2, NA)) #&gt; [1] TRUE FALSE FALSE TRUE 24.3.5 Exercises Rewrite any of the functions from the first exercise to deal with missing values. If na.rm is true, ignore the missing values. If na.rm is false, return a missing value if the input contains any missing values. Some good functions to practice with are min(), max(), range(), mean(), and var(). Rewrite cumsum() and diff() so they can handle missing values. Note that these functions have slightly more complicated behaviour. 24.4 Rcpp sugar Rcpp provides a lot of syntactic “sugar” to ensure that C++ functions work very similarly to their R equivalents. In fact, Rcpp sugar makes it possible to write efficient C++ code that looks almost identical to its R equivalent. If there’s a sugar version of the function you’re interested in, you should use it: it’ll be both expressive and well tested. Sugar functions aren’t always faster than a handwritten equivalent, but they will get faster in the future as more time is spent on optimising Rcpp. Sugar functions can be roughly broken down into arithmetic and logical operators logical summary functions vector views other useful functions 24.4.1 Arithmetic and logical operators All the basic arithmetic and logical operators are vectorised: +, *, -, /, pow, &lt;, &lt;=, &gt;, &gt;=, ==, !=, !. For example, we could use sugar to considerably simplify the implementation of pdistC(). pdistR &lt;- function(x, ys) { sqrt((x - ys) ^ 2) } #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector pdistC2(double x, NumericVector ys) { return sqrt(pow((x - ys), 2)); } 24.4.2 Logical summary functions The sugar function any() and all() are fully lazy so that any(x == 0), for example, might only need to evaluate one element of a vector, and return a special type that can be converted into a bool using .is_true(), .is_false(), or .is_na(). We could also use this sugar to write an efficient function to determine whether or not a numeric vector contains any missing values. To do this in R, we could use any(is.na(x)): any_naR &lt;- function(x) any(is.na(x)) However, this will do the same amount of work regardless of the location of the missing value. Here’s the C++ implementation: #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] bool any_naC(NumericVector x) { return is_true(any(is_na(x))); } x0 &lt;- runif(1e5) x1 &lt;- c(x0, NA) x2 &lt;- c(NA, x0) microbenchmark( any_naR(x0), any_naC(x0), any_naR(x1), any_naC(x1), any_naR(x2), any_naC(x2) ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; any_naR(x0) 143.00 283.00 275.03 290.00 299.00 398.0 100 bc #&gt; any_naC(x0) 161.00 161.00 165.70 162.00 163.00 242.0 100 b #&gt; any_naR(x1) 143.00 286.00 335.34 291.00 300.00 5,330.0 100 c #&gt; any_naC(x1) 161.00 162.00 172.27 162.00 163.00 930.0 100 b #&gt; any_naR(x2) 64.00 201.00 247.71 212.00 225.00 4,330.0 100 bc #&gt; any_naC(x2) 1.05 1.38 1.82 1.81 2.21 3.5 100 a 24.4.3 Vector views A number of helpful functions provide a “view” of a vector: head(), tail(), rep_each(), rep_len(), rev(), seq_along(), and seq_len(). In R these would all produce copies of the vector, but in Rcpp they simply point to the existing vector and override the subsetting operator ([) to implement special behaviour. This makes them very efficient: for instance, rep_len(x, 1e6) does not have to make a million copies of x. 24.4.4 Other useful functions Finally, there’s a grab bag of sugar functions that mimic frequently used R functions: Math functions: abs(), acos(), asin(), atan(), beta(), ceil(), ceiling(), choose(), cos(), cosh(), digamma(), exp(), expm1(), factorial(), floor(), gamma(), lbeta(), lchoose(), lfactorial(), lgamma(), log(), log10(), log1p(), pentagamma(), psigamma(), round(), signif(), sin(), sinh(), sqrt(), tan(), tanh(), tetragamma(), trigamma(), trunc(). Scalar summaries: mean(), min(), max(), sum(), sd(), and (for vectors) var(). Vector summaries: cumsum(), diff(), pmin(), and pmax(). Finding values: match(), self_match(), which_max(), which_min(). Dealing with duplicates: duplicated(), unique(). d/q/p/r for all standard distributions. Finally, noNA(x) asserts that the vector x does not contain any missing values, and allows optimisation of some mathematical operations. For example, when computing the mean of a vector with no missing values, Rcpp doesn’t need to check each value is not missing when computing the sum and the length. 24.5 The STL The real strength of C++ shows itself when you need to implement more complex algorithms. The standard template library (STL) provides a set of extremely useful data structures and algorithms. This section will explain some of the most important algorithms and data structures and point you in the right direction to learn more. I can’t teach you everything you need to know about the STL, but hopefully the examples will show you the power of the STL, and persuade you that it’s useful to learn more. If you need an algorithm or data structure that isn’t implemented in STL, a good place to look is boost. Installing boost on your computer is beyond the scope of this chapter, but once you have it installed, you can use boost data structures and algorithms by including the appropriate header file with (e.g.) #include &lt;boost/array.hpp&gt;. 24.5.1 Using iterators Iterators are used extensively in the STL: many functions either accept or return iterators. They are the next step up from basic loops, abstracting away the details of the underlying data structure. Iterators have three main operators: Advance with ++. Get the value they refer to, or dereference, with *. Compare with ==. For example we could re-write our sum function using iterators: #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] double sum3(NumericVector x) { double total = 0; NumericVector::iterator it; for(it = x.begin(); it != x.end(); ++it) { total += *it; } return total; } The main changes are in the for loop: We start at x.begin() and loop until we get to x.end(). A small optimization is to store the value of the end iterator so we don’t need to look it up each time. This only saves about 2 ns per iteration, so it’s only important when the calculations in the loop are very simple. Instead of indexing into x, we use the dereference operator to get its current value: *it. Notice the type of the iterator: NumericVector::iterator. Each vector type has its own iterator type: LogicalVector::iterator, CharacterVector::iterator, etc. Iterators also allow us to use the C++ equivalents of the apply family of functions. For example, we could again rewrite sum() to use the accumulate() function, which takes a starting and an ending iterator, and adds up all the values in the vector. The third argument to accumulate gives the initial value: it’s particularly important because this also determines the data type that accumulate uses (so we use 0.0 and not 0 so that accumulate uses a double, not an int.). To use accumulate() we need to include the &lt;numeric&gt; header. #include &lt;numeric&gt; #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] double sum4(NumericVector x) { return std::accumulate(x.begin(), x.end(), 0.0); } accumulate() (along with the other functions in &lt;numeric&gt;, like adjacent_difference(), inner_product(), and partial_sum()) is not that important in Rcpp because Rcpp sugar provides equivalents. 24.5.2 Algorithms The &lt;algorithm&gt; header provides a large number of algorithms that work with iterators. A good reference is available at http://www.cplusplus.com/reference/algorithm/. For example, we could write a basic Rcpp version of findInterval() that takes two arguments a vector of values and a vector of breaks, and locates the bin that each x falls into. This shows off a few more advanced iterator features. Read the code below and see if you can figure out how it works. #include &lt;algorithm&gt; #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] IntegerVector findInterval2(NumericVector x, NumericVector breaks) { IntegerVector out(x.size()); NumericVector::iterator it, pos; IntegerVector::iterator out_it; for(it = x.begin(), out_it = out.begin(); it != x.end(); ++it, ++out_it) { pos = std::upper_bound(breaks.begin(), breaks.end(), *it); *out_it = std::distance(breaks.begin(), pos); } return out; } The key points are: We step through two iterators (input and output) simultaneously. We can assign into an dereferenced iterator (out_it) to change the values in out. upper_bound() returns an iterator. If we wanted the value of the upper_bound() we could dereference it; to figure out its location, we use the distance() function. Small note: if we want this function to be as fast as findInterval() in R (which uses handwritten C code), we need to compute the calls to .begin() and .end() once and save the results. This is easy, but it distracts from this example so it has been omitted. Making this change yields a function that’s slightly faster than R’s findInterval() function, but is about 1/10 of the code. It’s generally better to use algorithms from the STL than hand rolled loops. In Effective STL, Scott Meyers gives three reasons: efficiency, correctness, and maintainability. Algorithms from the STL are written by C++ experts to be extremely efficient, and they have been around for a long time so they are well tested. Using standard algorithms also makes the intent of your code more clear, helping to make it more readable and more maintainable. 24.5.3 Data structures The STL provides a large set of data structures: array, bitset, list, forward_list, map, multimap, multiset, priority_queue, queue, deque, set, stack, unordered_map, unordered_set, unordered_multimap, unordered_multiset, and vector. The most important of these data structures are the vector, the unordered_set, and the unordered_map. We’ll focus on these three in this section, but using the others is similar: they just have different performance trade-offs. For example, the deque (pronounced “deck”) has a very similar interface to vectors but a different underlying implementation that has different performance trade-offs. You may want to try them for your problem. A good reference for STL data structures is http://www.cplusplus.com/reference/stl/ — I recommend you keep it open while working with the STL. Rcpp knows how to convert from many STL data structures to their R equivalents, so you can return them from your functions without explicitly converting to R data structures. 24.5.4 Vectors An STL vector is very similar to an R vector, except that it grows efficiently. This makes vectors appropriate to use when you don’t know in advance how big the output will be. Vectors are templated, which means that you need to specify the type of object the vector will contain when you create it: vector&lt;int&gt;, vector&lt;bool&gt;, vector&lt;double&gt;, vector&lt;String&gt;. You can access individual elements of a vector using the standard [] notation, and you can add a new element to the end of the vector using .push_back(). If you have some idea in advance how big the vector will be, you can use .reserve() to allocate sufficient storage. The following code implements run length encoding (rle()). It produces two vectors of output: a vector of values, and a vector lengths giving how many times each element is repeated. It works by looping through the input vector x comparing each value to the previous: if it’s the same, then it increments the last value in lengths; if it’s different, it adds the value to the end of values, and sets the corresponding length to 1. #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] List rleC(NumericVector x) { std::vector&lt;int&gt; lengths; std::vector&lt;double&gt; values; // Initialise first value int i = 0; double prev = x[0]; values.push_back(prev); lengths.push_back(1); NumericVector::iterator it; for(it = x.begin() + 1; it != x.end(); ++it) { if (prev == *it) { lengths[i]++; } else { values.push_back(*it); lengths.push_back(1); i++; prev = *it; } } return List::create( _[&quot;lengths&quot;] = lengths, _[&quot;values&quot;] = values ); } (An alternative implementation would be to replace i with the iterator lengths.rbegin() which always points to the last element of the vector. You might want to try implementing that yourself.) Other methods of a vector are described at http://www.cplusplus.com/reference/vector/vector/. 24.5.5 Sets Sets maintain a unique set of values, and can efficiently tell if you’ve seen a value before. They are useful for problems that involve duplicates or unique values (like unique, duplicated, or in). C++ provides both ordered (std::set) and unordered sets (std::unordered_set), depending on whether or not order matters for you. Unordered sets tend to be much faster (because they use a hash table internally rather than a tree), so even if you need an ordered set, you should consider using an unordered set and then sorting the output. Like vectors, sets are templated, so you need to request the appropriate type of set for your purpose: unordered_set&lt;int&gt;, unordered_set&lt;bool&gt;, etc. More details are available at http://www.cplusplus.com/reference/set/set/ and http://www.cplusplus.com/reference/unordered_set/unordered_set/. The following function uses an unordered set to implement an equivalent to duplicated() for integer vectors. Note the use of seen.insert(x[i]).second. insert() returns a pair, the .first value is an iterator that points to element and the .second value is a boolean that’s true if the value was a new addition to the set. // [[Rcpp::plugins(cpp11)]] #include &lt;Rcpp.h&gt; #include &lt;unordered_set&gt; using namespace Rcpp; // [[Rcpp::export]] LogicalVector duplicatedC(IntegerVector x) { std::unordered_set&lt;int&gt; seen; int n = x.size(); LogicalVector out(n); for (int i = 0; i &lt; n; ++i) { out[i] = !seen.insert(x[i]).second; } return out; } Note that unordered sets are only available in C++ 11, which means we need to use the cpp11 plugin, [[Rcpp::plugins(cpp11)]]. 24.5.6 Map A map is similar to a set, but instead of storing presence or absence, it can store additional data. It’s useful for functions like table() or match() that need to look up a value. As with sets, there are ordered (std::map) and unordered (std::unordered_map) versions. Since maps have a value and a key, you need to specify both types when initialising a map: map&lt;double, int&gt;, unordered_map&lt;int, double&gt;, and so on. The following example shows how you could use a map to implement table() for numeric vectors: #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] std::map&lt;double, int&gt; tableC(NumericVector x) { std::map&lt;double, int&gt; counts; int n = x.size(); for (int i = 0; i &lt; n; i++) { counts[x[i]]++; } return counts; } Note that unordered maps are only available in C++ 11, so to use them, you’ll again need [[Rcpp::plugins(cpp11)]]. 24.5.7 Exercises To practice using the STL algorithms and data structures, implement the following using R functions in C++, using the hints provided: median.default() using partial_sort. %in% using unordered_set and the find() or count() methods. unique() using an unordered_set (challenge: do it in one line!). min() using std::min(), or max() using std::max(). which.min() using min_element, or which.max() using max_element. setdiff(), union(), and intersect() for integers using sorted ranges and set_union, set_intersection and set_difference. 24.6 Case studies The following case studies illustrate some real life uses of C++ to replace slow R code. 24.6.1 Gibbs sampler The following case study updates an example blogged about by Dirk Eddelbuettel, illustrating the conversion of a Gibbs sampler in R to C++. The R and C++ code shown below is very similar (it only took a few minutes to convert the R version to the C++ version), but runs about 20 times faster on my computer. Dirk’s blog post also shows another way to make it even faster: using the faster random number generator functions in GSL (easily accessible from R through the RcppGSL package) can make it another 2–3x faster. The R code is as follows: gibbs_r &lt;- function(N, thin) { mat &lt;- matrix(nrow = N, ncol = 2) x &lt;- y &lt;- 0 for (i in 1:N) { for (j in 1:thin) { x &lt;- rgamma(1, 3, y * y + 4) y &lt;- rnorm(1, 1 / (x + 1), 1 / sqrt(2 * (x + 1))) } mat[i, ] &lt;- c(x, y) } mat } This is straightforward to convert to C++. We: add type declarations to all variables use ( instead of [ to index into the matrix subscript the results of rgamma and rnorm to convert from a vector into a scalar #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] NumericMatrix gibbs_cpp(int N, int thin) { NumericMatrix mat(N, 2); double x = 0, y = 0; for(int i = 0; i &lt; N; i++) { for(int j = 0; j &lt; thin; j++) { x = rgamma(1, 3, 1 / (y * y + 4))[0]; y = rnorm(1, 1 / (x + 1), 1 / sqrt(2 * (x + 1)))[0]; } mat(i, 0) = x; mat(i, 1) = y; } return(mat); } Benchmarking the two implementations yields: microbenchmark( gibbs_r(100, 10), gibbs_cpp(100, 10) ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; gibbs_r(100, 10) 3,130 3,360 5086 3,790 4,070 18,400 100 b #&gt; gibbs_cpp(100, 10) 205 217 231 221 226 1,190 100 a 24.6.2 R vectorisation vs. C++ vectorisation This example is adapted from “Rcpp is smoking fast for agent-based models in data frames”. The challenge is to predict a model response from three inputs. The basic R version of the predictor looks like: vacc1a &lt;- function(age, female, ily) { p &lt;- 0.25 + 0.3 * 1 / (1 - exp(0.04 * age)) + 0.1 * ily p &lt;- p * if (female) 1.25 else 0.75 p &lt;- max(0, p) p &lt;- min(1, p) p } We want to be able to apply this function to many inputs, so we might write a vector-input version using a for loop. vacc1 &lt;- function(age, female, ily) { n &lt;- length(age) out &lt;- numeric(n) for (i in seq_len(n)) { out[i] &lt;- vacc1a(age[i], female[i], ily[i]) } out } If you’re familiar with R, you’ll have a gut feeling that this will be slow, and indeed it is. There are two ways we could attack this problem. If you have a good R vocabulary, you might immediately see how to vectorise the function (using ifelse(), pmin(), and pmax()). Alternatively, we could rewrite vacc1a() and vacc1() in C++, using our knowledge that loops and function calls have much lower overhead in C++. Either approach is fairly straightforward. In R: vacc2 &lt;- function(age, female, ily) { p &lt;- 0.25 + 0.3 * 1 / (1 - exp(0.04 * age)) + 0.1 * ily p &lt;- p * ifelse(female, 1.25, 0.75) p &lt;- pmax(0, p) p &lt;- pmin(1, p) p } (If you’ve worked R a lot you might recognise some potential bottlenecks in this code: ifelse, pmin, and pmax are known to be slow, and could be replaced with p * 0.75 + p * 0.5 * female, p[p &lt; 0] &lt;- 0, p[p &gt; 1] &lt;- 1. You might want to try timing those variations yourself.) Or in C++: #include &lt;Rcpp.h&gt; using namespace Rcpp; double vacc3a(double age, bool female, bool ily){ double p = 0.25 + 0.3 * 1 / (1 - exp(0.04 * age)) + 0.1 * ily; p = p * (female ? 1.25 : 0.75); p = std::max(p, 0.0); p = std::min(p, 1.0); return p; } // [[Rcpp::export]] NumericVector vacc3(NumericVector age, LogicalVector female, LogicalVector ily) { int n = age.size(); NumericVector out(n); for(int i = 0; i &lt; n; ++i) { out[i] = vacc3a(age[i], female[i], ily[i]); } return out; } We next generate some sample data, and check that all three versions return the same values: n &lt;- 1000 age &lt;- rnorm(n, mean = 50, sd = 10) female &lt;- sample(c(T, F), n, rep = TRUE) ily &lt;- sample(c(T, F), n, prob = c(0.8, 0.2), rep = TRUE) stopifnot( all.equal(vacc1(age, female, ily), vacc2(age, female, ily)), all.equal(vacc1(age, female, ily), vacc3(age, female, ily)) ) The original blog post forgot to do this, and introduced a bug in the C++ version: it used 0.004 instead of 0.04. Finally, we can benchmark our three approaches: microbenchmark( vacc1 = vacc1(age, female, ily), vacc2 = vacc2(age, female, ily), vacc3 = vacc3(age, female, ily) ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval cld #&gt; vacc1 1,180.0 1,220.0 1351.9 1,250.0 1,350.0 4,810 100 b #&gt; vacc2 70.2 74.4 134.8 77.6 83.1 5,420 100 a #&gt; vacc3 10.9 11.9 22.5 12.9 13.4 963 100 a Not surprisingly, our original approach with loops is very slow. Vectorising in R gives a huge speedup, and we can eke out even more performance (~10x) with the C++ loop. I was a little surprised that the C++ was so much faster, but it is because the R version has to create 11 vectors to store intermediate results, where the C++ code only needs to create 1. 24.7 Using Rcpp in a package The same C++ code that is used with sourceCpp() can also be bundled into a package. There are several benefits of moving code from a stand-alone C++ source file to a package: Your code can be made available to users without C++ development tools. Multiple source files and their dependencies are handled automatically by the R package build system. Packages provide additional infrastructure for testing, documentation, and consistency. To add Rcpp to an existing package, you put your C++ files in the src/ directory and modify/create the following configuration files: In DESCRIPTION add LinkingTo: Rcpp Imports: Rcpp Make sure your NAMESPACE includes: useDynLib(mypackage) importFrom(Rcpp, sourceCpp) We need to import something (anything) from Rcpp so that internal Rcpp code is properly loaded. This is a bug in R and hopefully will be fixed in the future. To generate a new Rcpp package that includes a simple “hello world” function you can use Rcpp.package.skeleton(): Rcpp.package.skeleton(&quot;NewPackage&quot;, attributes = TRUE) To generate a package based on C++ files that you’ve been using with sourceCpp(), use the cpp_files parameter: Rcpp.package.skeleton(&quot;NewPackage&quot;, example_code = FALSE, cpp_files = c(&quot;convolve.cpp&quot;)) Before building the package, you’ll need to run Rcpp::compileAttributes(). This function scans the C++ files for Rcpp::export attributes and generates the code required to make the functions available in R. Re-run compileAttributes() whenever functions are added, removed, or have their signatures changed. This is done automatically by the devtools package and by Rstudio. For more details see the Rcpp package vignette, vignette(&quot;Rcpp-package&quot;). 24.8 Learning more This chapter has only touched on a small part of Rcpp, giving you the basic tools to rewrite poorly performing R code in C++. The Rcpp book is the best reference to learn more about Rcpp. As noted, Rcpp has many other capabilities that make it easy to interface R to existing C++ code, including: Additional features of attributes including specifying default arguments, linking in external C++ dependencies, and exporting C++ interfaces from packages. These features and more are covered in the Rcpp attributes vignette, vignette(&quot;Rcpp-attributes&quot;). Automatically creating wrappers between C++ data structures and R data structures, including mapping C++ classes to reference classes. A good introduction to this topic is Rcpp modules vignette, vignette(&quot;Rcpp-modules&quot;) The Rcpp quick reference guide, vignette(&quot;Rcpp-quickref&quot;), contains a useful summary of Rcpp classes and common programming idioms. I strongly recommend keeping an eye on the Rcpp homepage and Dirk’s Rcpp page as well as signing up for the Rcpp mailing list. Rcpp is still under active development, and is getting better with every release. Other resources I’ve found helpful in learning C++ are: Effective C++ and Effective STL by Scott Meyers. C++ Annotations, aimed at “knowledgeable users of C (or any other language using a C-like grammar, like Perl or Java) who would like to know more about, or make the transition to, C++”. Algorithm Libraries, which provides a more technical, but still concise, description of important STL concepts. (Follow the links under notes). Writing performance code may also require you to rethink your basic approach: a solid understanding of basic data structures and algorithms is very helpful here. That’s beyond the scope of this book, but I’d suggest the Algorithm Design Manual, MIT’s Introduction to Algorithms, Algorithms by Robert Sedgewick and Kevin Wayne which has a free online textbook and a matching coursera course. 24.9 Acknowledgments I’d like to thank the Rcpp-mailing list for many helpful conversations, particularly Romain Francois and Dirk Eddelbuettel who have not only provided detailed answers to many of my questions, but have been incredibly responsive at improving Rcpp. This chapter would not have been possible without JJ Allaire; he encouraged me to learn C++ and then answered many of my dumb questions along the way. "]
]
